import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from torch.nn.utils import spectral_norm
import shutil
import os
import webbrowser
from loss import smooth_mask_generate
import matplotlib.pyplot as plt
from torch.nn import Parameter


class EarlyAttentionModule(nn.Module):
    """é€‚ç”¨äºè¾“å…¥å±‚çš„æ³¨æ„åŠ›æ¨¡å—"""

    def __init__(self, in_channels):
        super(EarlyAttentionModule, self).__init__()
        self.conv = nn.Conv2d(in_channels + 1, in_channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, mask):
        # ç¡®ä¿æ©ç å°ºå¯¸ä¸è¾“å…¥ç‰¹å¾åŒ¹é…
        mask_resized = F.interpolate(
            mask, size=x.shape[2:], mode="bilinear", align_corners=True
        )
        # è¿æ¥ç‰¹å¾å’Œæ©ç 
        combined = torch.cat([x, mask_resized], dim=1)
        # ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        attention = self.sigmoid(self.conv(combined))
        # åº”ç”¨æ³¨æ„åŠ›
        return x * attention


class MaskAttentionModule(nn.Module):
    def __init__(self, in_channels):
        super(MaskAttentionModule, self).__init__()
        self.conv = nn.Conv2d(in_channels + 1, in_channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, mask):
        # å°†æ©ç ä¸Šé‡‡æ ·åˆ°ç‰¹å¾å›¾å°ºå¯¸
        mask_resized = F.interpolate(
            mask, size=x.shape[2:], mode="bilinear", align_corners=True
        )
        # è¿æ¥ç‰¹å¾å’Œæ©ç 
        combined = torch.cat([x, mask_resized], dim=1)
        # ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        attention = self.sigmoid(self.conv(combined))
        # åº”ç”¨æ³¨æ„åŠ›
        return x * attention


# æ·»åŠ è¿™ä¸ªæ–°æ¨¡å—åˆ°CompletionNetworkç±»å®šä¹‰ä¹‹å‰
class BoundaryAwareModule(nn.Module):
    """è¾¹ç•Œæ„ŸçŸ¥æ¨¡å—ï¼Œä¸“æ³¨äºæ”¹å–„è¾¹ç•Œè¿ç»­æ€§"""

    def __init__(self, in_channels):
        super(BoundaryAwareModule, self).__init__()

        # è¾¹ç•Œç‰¹å¾æå–
        self.boundary_conv = nn.Sequential(
            nn.Conv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=2, dilation=2),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
        )

        # ç‰¹å¾èåˆ
        self.fusion = nn.Conv2d(in_channels * 2, in_channels, kernel_size=1)

    def forward(self, features, mask):
        # è·å–è¾¹ç•Œæ©ç 
        boundary_mask = self._get_boundary_mask(mask)

        # å°†ç‰¹å¾ä¸è¾¹ç•Œæ©ç è¿æ¥
        boundary_input = torch.cat([features, boundary_mask], dim=1)

        # æå–è¾¹ç•Œç‰¹å¾
        boundary_features = self.boundary_conv(boundary_input)

        # èåˆåŸå§‹ç‰¹å¾å’Œè¾¹ç•Œç‰¹å¾
        combined = torch.cat([features, boundary_features], dim=1)
        enhanced_features = self.fusion(combined)

        return enhanced_features

    def _get_boundary_mask(self, mask):
        """ç®€åŒ–çš„è¾¹ç•Œæ©ç ç”Ÿæˆ"""
        # ä½¿ç”¨æœ€å¤§æ± åŒ–å’Œæœ€å°æ± åŒ–æ‰¾å‡ºè¾¹ç•Œ
        dilated = F.max_pool2d(mask, kernel_size=3, stride=1, padding=1)
        eroded = -F.max_pool2d(-mask, kernel_size=3, stride=1, padding=1)

        # è¾¹ç•Œæ˜¯è†¨èƒ€å’Œè…èš€çš„å·®å¼‚
        boundary = torch.abs(dilated - eroded)

        return boundary


'''class GatedConv2d(nn.Module):
    """
    å¸¦é—¨æ§æœºåˆ¶çš„å·ç§¯å±‚ï¼Œå¯ä»¥æ›´å¥½åœ°å¤„ç†æœ‰æ©ç çš„è¾“å…¥
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):
        super(GatedConv2d, self).__init__()
        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)
        self.mask_conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)
        
    def forward(self, x, mask=None):
        # ç‰¹å¾é€šé“
        features = self.conv2d(x)
            
        # ç”Ÿæˆé—¨æ§é€šé“
        gates = torch.sigmoid(self.mask_conv2d(x))
        
        # å¦‚æœæä¾›äº†å¤–éƒ¨æ©ç ï¼Œå°†å®ƒä¸é—¨æ§æœºåˆ¶ç»“åˆ
        if mask is not None:
            if mask.shape[2:] != gates.shape[2:]:
                mask = F.interpolate(mask, size=gates.shape[2:], mode='nearest')
            gates = gates * mask
            
        # åº”ç”¨é—¨æ§æœºåˆ¶
        return features * gates'''


def l2normalize(v, eps=1e-12):
    return v / (v.norm() + eps)


class SpectralNorm(nn.Module):
    def __init__(self, module, name="weight", power_iterations=1):
        super(SpectralNorm, self).__init__()
        self.module = module
        self.name = name
        self.power_iterations = power_iterations
        if not self._made_params():
            self._make_params()

    def _update_u_v(self):
        u = getattr(self.module, self.name + "_u")
        v = getattr(self.module, self.name + "_v")
        w = getattr(self.module, self.name + "_bar")

        height = w.data.shape[0]
        for _ in range(self.power_iterations):
            v.data = l2normalize(torch.mv(torch.t(w.view(height, -1).data), u.data))
            u.data = l2normalize(torch.mv(w.view(height, -1).data, v.data))

        sigma = u.dot(w.view(height, -1).mv(v))
        setattr(self.module, self.name, w / sigma.expand_as(w))

    def _made_params(self):
        try:
            u = getattr(self.module, self.name + "_u")
            v = getattr(self.module, self.name + "_v")
            w = getattr(self.module, self.name + "_bar")
            return True
        except AttributeError:
            return False

    def _make_params(self):
        w = getattr(self.module, self.name)

        height = w.data.shape[0]
        width = w.view(height, -1).data.shape[1]

        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)
        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)
        u.data = l2normalize(u.data)
        v.data = l2normalize(v.data)
        w_bar = Parameter(w.data)

        del self.module._parameters[self.name]

        self.module.register_parameter(self.name + "_u", u)
        self.module.register_parameter(self.name + "_v", v)
        self.module.register_parameter(self.name + "_bar", w_bar)

    def forward(self, *args):
        self._update_u_v()
        return self.module.forward(*args)


class LayerNorm(nn.Module):
    def __init__(self, num_features, eps=1e-8, affine=True):
        super(LayerNorm, self).__init__()
        self.num_features = num_features
        self.affine = affine
        self.eps = eps

        if self.affine:
            self.gamma = Parameter(torch.Tensor(num_features).uniform_())
            self.beta = Parameter(torch.zeros(num_features))

    def forward(self, x):
        shape = [-1] + [1] * (x.dim() - 1)
        if x.size(0) == 1:
            mean = x.view(-1).mean().view(*shape)
            std = x.view(-1).std().view(*shape)
        else:
            mean = x.view(x.size(0), -1).mean(1).view(*shape)
            std = x.view(x.size(0), -1).std(1).view(*shape)
        x = (x - mean) / (std + self.eps)
        if self.affine:
            shape = [1, -1] + [1] * (x.dim() - 2)
            x = x * self.gamma.view(*shape) + self.beta.view(*shape)
        return x


class GatedConv2d(nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        stride=1,
        padding=0,
        dilation=1,
        pad_type="reflect",
        activation="lrelu",
        norm="none",
        sn=False,
    ):
        super(GatedConv2d, self).__init__()
        if pad_type == "reflect":
            self.pad = nn.ReflectionPad2d(padding)
        elif pad_type == "replicate":
            self.pad = nn.ReplicationPad2d(padding)
        elif pad_type == "zero":
            self.pad = nn.ZeroPad2d(padding)
        else:
            raise ValueError(f"Unsupported padding type: {pad_type}")

        if norm == "bn":
            self.norm = nn.BatchNorm2d(out_channels)
        elif norm == "in":
            self.norm = nn.InstanceNorm2d(out_channels)
        elif norm == "ln":
            self.norm = LayerNorm(out_channels)
        elif norm == "none":
            self.norm = None
        else:
            raise ValueError(f"Unsupported normalization: {norm}")

        if activation == "relu":
            self.activation = nn.ReLU(inplace=True)
        elif activation == "lrelu":
            self.activation = nn.LeakyReLU(0.2, inplace=True)
        elif activation == "prelu":
            self.activation = nn.PReLU()
        elif activation == "selu":
            self.activation = nn.SELU(inplace=True)
        elif activation == "tanh":
            self.activation = nn.Tanh()
        elif activation == "sigmoid":
            self.activation = nn.Sigmoid()
        elif activation == "none":
            self.activation = None
        else:
            raise ValueError(f"Unsupported activation: {activation}")

        if sn:
            self.conv2d = SpectralNorm(
                nn.Conv2d(
                    in_channels,
                    out_channels,
                    kernel_size,
                    stride,
                    padding=0,
                    dilation=dilation,
                )
            )
            self.mask_conv2d = SpectralNorm(
                nn.Conv2d(
                    in_channels,
                    out_channels,
                    kernel_size,
                    stride,
                    padding=0,
                    dilation=dilation,
                )
            )
        else:
            self.conv2d = nn.Conv2d(
                in_channels,
                out_channels,
                kernel_size,
                stride,
                padding=0,
                dilation=dilation,
            )
            self.mask_conv2d = nn.Conv2d(
                in_channels,
                out_channels,
                kernel_size,
                stride,
                padding=0,
                dilation=dilation,
            )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.pad(x)
        conv = self.conv2d(x)
        mask_conv = self.mask_conv2d(x)
        gated_mask = self.sigmoid(mask_conv)
        x = conv * gated_mask
        if self.norm:
            x = self.norm(x)
        if self.activation:
            x = self.activation(x)
        return x


class CompletionNetwork(nn.Module):
    def __init__(self, input_channels=1):
        super(CompletionNetwork, self).__init__()

        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯çš„æ ‡å¿—
        self.debug = False

        # ä¸ºå±€éƒ¨å’Œå…¨å±€è¾“å…¥æ·»åŠ æ—©æœŸæ³¨æ„åŠ›æ¨¡å—
        self.early_local_attention = EarlyAttentionModule(input_channels)
        self.early_global_attention = EarlyAttentionModule(input_channels)

        # Local encoder branch with intermediate outputs saved for skip connections
        # ç®€åŒ–ç¼–ç å™¨ç»“æ„ï¼Œä¿æŒåŸæœ‰çš„è·³è·ƒè¿æ¥è®¾è®¡
        self.local_enc1 = nn.Sequential(
            nn.Conv2d(input_channels * 2, 64, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )

        self.local_enc2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
        )

        self.local_enc3 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )

        # Global encoder branch - ä½¿ç”¨GatedConv2dæ›¿æ¢æ ‡å‡†Conv2d
        self.global_enc1 = nn.Sequential(
            GatedConv2d(input_channels * 2, 64, kernel_size=5, stride=2, padding=2),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.global_enc2 = nn.Sequential(
            GatedConv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.global_enc3 = nn.Sequential(
            GatedConv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            GatedConv2d(256, 256, kernel_size=3, stride=2, padding=0),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )

        # é«˜çº§ç‰¹å¾çš„å…¨å±€æ³¨æ„åŠ›æ¨¡å—
        self.global_attention = MaskAttentionModule(256)

        # ç‰¹å¾èåˆçš„æ³¨æ„åŠ›æ¨¡å—
        self.fusion_attention = MaskAttentionModule(256)

        # Feature fusion - æ›´å¤æ‚çš„èåˆæ¨¡å—
        self.fusion = nn.Sequential(
            nn.Conv2d(512, 384, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(384),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )

        self.boundary_aware = BoundaryAwareModule(256)  # åœ¨æ·±å±‚ç‰¹å¾å¤„ä½¿ç”¨

        # è§£ç å™¨å¢åŠ æ›´å¤šå±‚æ¬¡å’Œè·³è·ƒè¿æ¥
        # è§£ç å™¨ç¬¬ä¸€å±‚
        self.decoder1 = nn.Sequential(
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
        )

        # è§£ç å™¨ç¬¬ä¸€å±‚è·³è·ƒè¿æ¥èåˆ
        self.skip_fusion1 = nn.Sequential(
            nn.Conv2d(128 + 128, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
        )

        # è§£ç å™¨ç¬¬äºŒå±‚
        self.decoder2 = nn.Sequential(
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )

        # è§£ç å™¨ç¬¬äºŒå±‚è·³è·ƒè¿æ¥èåˆ
        self.skip_fusion2 = nn.Sequential(
            nn.Conv2d(64 + 64, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )

        # æ–°å¢ä¸€ä¸ªè·³è·ƒè¿æ¥åˆ°ç¼–ç å™¨çš„ç¬¬ä¸€ä¸ªè¾“å‡º
        self.decoder3 = nn.Sequential(
            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, input_channels, kernel_size=3, stride=1, padding=1),
        )

        # å¹³æ»‘è¿‡æ¸¡å±‚
        self.transition_conv = nn.Conv2d(
            input_channels + 1, input_channels, kernel_size=3, stride=1, padding=1
        )
        # æ·»åŠ åç§»é¢„æµ‹å±‚
        # ä¼˜åŒ–å…¨å±€åç§»é¢„æµ‹æ¨¡å—
        self.offset_module = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),  # å…¨å±€å¹³å‡æ± åŒ–
            nn.Flatten(),
            nn.Linear(input_channels * 2, 64),  # å¢åŠ ä¸­é—´å±‚
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(64, 1),  # æœ€ç»ˆè¾“å‡ºä¸€ä¸ªåç§»å€¼
            nn.Tanh(),  # ä½¿ç”¨Tanhé™åˆ¶åç§»èŒƒå›´åœ¨[-1, 1]
        )

        # æ·»åŠ å…¨å±€åç§»å¹…åº¦å› å­ - æ§åˆ¶åç§»çš„å¼ºåº¦
        self.offset_scale = nn.Parameter(torch.tensor(20.0))

        # è¾¹ç•Œæ„ŸçŸ¥çš„åç§»è¿‡æ¸¡æ¨¡å—
        self.offset_transition = nn.Sequential(
            nn.Conv2d(input_channels + 1, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, input_channels, kernel_size=3, padding=1),
        )
        """self.transition_conv = nn.Sequential(
        nn.Conv2d(input_channels + 1, 32, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm2d(32),
        nn.ReLU(inplace=True),
        nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=2, dilation=2),  # æ‰©å¼ å·ç§¯æ‰©å¤§æ„Ÿå—é‡
        nn.BatchNorm2d(16),
        nn.ReLU(inplace=True),
        nn.Conv2d(16, input_channels, kernel_size=3, stride=1, padding=1)
    )"""

    def print_sizes(self, tensor, name):
        """æ‰“å°å¼ é‡å°ºå¯¸ï¼Œç”¨äºè°ƒè¯•"""
        if self.debug:
            print(f"{name} size: {tensor.size()}")

    def forward(self, local_input, local_mask, global_input, global_mask):

        local_mask = local_mask.float()
        global_mask = global_mask.float()

        # å¯¹è¾“å…¥åº”ç”¨æ—©æœŸæ³¨æ„åŠ›
        local_input_attended = self.early_local_attention(local_input, local_mask)
        global_input_attended = self.early_global_attention(global_input, global_mask)

        # å°†è¾“å…¥å›¾åƒå’Œæ©ç è¿æ¥èµ·æ¥
        local_x = torch.cat([local_input_attended, local_mask], dim=1)
        # local_x = torch.cat([local_input, local_mask], dim=1)
        global_x = torch.cat([global_input_attended, global_mask], dim=1)

        # æœ¬åœ°ç¼–ç å™¨çš„å‰å‘ä¼ æ’­
        local_feat1 = self.local_enc1(local_x)
        # self.print_sizes(local_feat1, "local_feat1")

        local_feat2 = self.local_enc2(local_feat1)
        # self.print_sizes(local_feat2, "local_feat2")

        local_feat3 = self.local_enc3(local_feat2)
        # self.print_sizes(local_feat3, "local_feat3")

        # å…¨å±€ç¼–ç å™¨çš„å‰å‘ä¼ æ’­
        global_feat1 = self.global_enc1(global_x)
        # self.print_sizes(global_feat1, "global_feat1")

        global_feat2 = self.global_enc2(global_feat1)
        # self.print_sizes(global_feat2, "global_feat2")

        global_feat3 = self.global_enc3(global_feat2)
        # self.print_sizes(global_feat3, "global_feat3")

        # å°†å…¨å±€ç‰¹å¾è°ƒæ•´ä¸ºä¸æœ¬åœ°ç‰¹å¾ç›¸åŒçš„å¤§å°
        global_feat3_resized = F.interpolate(
            global_feat3,
            size=local_feat3.size()[2:],
            mode="bilinear",
            align_corners=True,
        )
        self.print_sizes(global_feat3_resized, "global_feat3_resized")

        # èåˆæœ¬åœ°å’Œå…¨å±€ç‰¹å¾
        combined_features = torch.cat([local_feat3, global_feat3_resized], dim=1)
        self.print_sizes(combined_features, "combined_features")

        fused_features = self.fusion(combined_features)
        """# æ·»åŠ è¾¹ç•Œæ„ŸçŸ¥å¢å¼º
        fused_features = self.boundary_aware(fused_features, F.interpolate(1-local_mask, 
                                                                 size=fused_features.shape[2:], 
                                                                 mode='bilinear', 
                                                                 align_corners=True))"""
        self.print_sizes(fused_features, "fused_features")

        # è§£ç å™¨ç¬¬ä¸€å±‚ - å¸¦è·³è·ƒè¿æ¥
        dec1 = self.decoder1(fused_features)
        self.print_sizes(dec1, "dec1")

        # ç¡®ä¿è§£ç å™¨ç‰¹å¾ä¸ç¼–ç å™¨ç‰¹å¾å°ºå¯¸åŒ¹é…
        if dec1.size()[2:] != local_feat2.size()[2:]:
            dec1 = F.interpolate(
                dec1, size=local_feat2.size()[2:], mode="bilinear", align_corners=True
            )
            self.print_sizes(dec1, "dec1_resized")

        # ç¬¬ä¸€ä¸ªè·³è·ƒè¿æ¥
        skip1 = torch.cat([dec1, local_feat2], dim=1)
        self.print_sizes(skip1, "skip1")

        skip1_fused = self.skip_fusion1(skip1)
        self.print_sizes(skip1_fused, "skip1_fused")

        # è§£ç å™¨ç¬¬äºŒå±‚
        dec2 = self.decoder2(skip1_fused)
        self.print_sizes(dec2, "dec2")

        # ç¡®ä¿è§£ç å™¨ç‰¹å¾ä¸ç¼–ç å™¨ç‰¹å¾å°ºå¯¸åŒ¹é…
        if dec2.size()[2:] != local_feat1.size()[2:]:
            dec2 = F.interpolate(
                dec2, size=local_feat1.size()[2:], mode="bilinear", align_corners=True
            )
            self.print_sizes(dec2, "dec2_resized")

        # ç¬¬äºŒä¸ªè·³è·ƒè¿æ¥
        skip2 = torch.cat([dec2, local_feat1], dim=1)
        self.print_sizes(skip2, "skip2")

        skip2_fused = self.skip_fusion2(skip2)
        self.print_sizes(skip2_fused, "skip2_fused")

        # æœ€ç»ˆè¾“å‡º
        output = self.decoder3(skip2_fused)
        self.print_sizes(output, "output")

        # åˆ›å»ºå¹³æ»‘è¿‡æ¸¡
        # ä½¿ç”¨å¹³å‡æ± åŒ–åˆ›å»ºæ¨¡ç³Šçš„æ©ç è¾¹ç¼˜
        # blurred_mask = F.avg_pool2d(local_mask, kernel_size=3, stride=1, padding=1)
        blurred_mask = smooth_mask_generate(local_mask, 7.0, False)
        fused_mask = torch.max(blurred_mask, 1 - local_mask)
        # blurred_mask = blurred_mask*local_mask + (1-local_mask)

        # è®¡ç®—å…¨å±€åç§»
        # 1. æå–å·²çŸ¥åŒºåŸŸå’Œç”ŸæˆåŒºåŸŸçš„ç‰¹å¾
        known_region = local_input * local_mask
        generated_region = output * (1 - local_mask)

        # 2. è¿æ¥ç‰¹å¾ç”¨äºåç§»é¢„æµ‹
        offset_features = torch.cat(
            [
                known_region.mean(dim=(2, 3), keepdim=True),
                generated_region.mean(dim=(2, 3), keepdim=True),
            ],
            dim=1,
        )

        # 3. é¢„æµ‹å…¨å±€åç§»å€¼
        offset = self.offset_module(offset_features)
        offset = offset.view(-1, 1, 1, 1)  # * self.offset_scale

        # 4. æ ¹æ®æ©ç è®¡ç®—è‡ªé€‚åº”åç§»
        # åˆ›å»ºè·ç¦»æ©ç  - è·ç¦»è¾¹ç•Œè¶Šè¿œï¼Œåç§»è¶Šå¤§
        # distance_mask = self._compute_distance_mask(1 - local_mask)
        # adaptive_offset = offset * distance_mask

        # 5. åº”ç”¨åç§»åˆ°ç”Ÿæˆçš„è¾“å‡º
        offset_output = output + offset

        # 6. ä½¿ç”¨è¾¹ç•Œæ„ŸçŸ¥çš„è¿‡æ¸¡æ¨¡å—èåˆç»“æœ
        transition_input = torch.cat([offset_output, fused_mask], dim=1)
        smoothed_output = self.offset_transition(transition_input)

        # output_with_mask = torch.cat([output, blurred_mask], dim=1)
        # output_with_mask = torch.cat([output, fused_mask], dim=1)
        # smoothed_output = self.transition_conv(output_with_mask)

        # ä»…åœ¨æ©ç åŒºåŸŸåº”ç”¨ç”Ÿæˆç»“æœ - å¹³æ»‘è¿‡æ¸¡
        final_output = local_input * local_mask + smoothed_output * (1 - local_mask)
        # final_output = local_input * (1-blurred_mask) + smoothed_output * blurred_mask

        return final_output


import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils import spectral_norm

# from loss import smooth_mask_generate

'''class EnhancedTerrainFeatureExtractor(nn.Module):
    """ç®€åŒ–ç‰ˆåœ°å½¢ç‰¹å¾æå–å™¨ï¼šä»…åŒ…å«å¡åº¦å’ŒåŸå§‹é«˜ç¨‹"""
    def __init__(self, in_channels):
        super(EnhancedTerrainFeatureExtractor, self).__init__()
        
        # å¡åº¦æå–ï¼ˆä½¿ç”¨Sobelç®—å­ï¼‰
        self.sobel_x = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)
        self.sobel_y = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)
        
        # åˆå§‹åŒ–æ»¤æ³¢å™¨
        with torch.no_grad():
            # Sobelæ»¤æ³¢å™¨
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)
            
            # æ‰©å±•ä¸ºå·ç§¯æ ¸
            sobel_x = sobel_x.reshape(1, 1, 3, 3).repeat(in_channels, in_channels, 1, 1)
            sobel_y = sobel_y.reshape(1, 1, 3, 3).repeat(in_channels, in_channels, 1, 1)
            
            # è®¾ç½®å·ç§¯æƒé‡
            self.sobel_x.weight.data = sobel_x
            self.sobel_y.weight.data = sobel_y
        
    def forward(self, x, mask, ismask=False):
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦åŒ…å«NaN
        if torch.isnan(x).any():
            print("è­¦å‘Š: TerrainFeatureExtractorè¾“å…¥åŒ…å«NaN!")
            x = torch.nan_to_num(x, nan=0.0)
            
        # è®¡ç®—ä¸€é˜¶å¯¼æ•°
        grad_x = self.sobel_x(x)
        grad_y = self.sobel_y(x)
        
        # è®¡ç®—å¡åº¦
        epsilon = 1e-6
        slope = torch.sqrt(grad_x.pow(2) + grad_y.pow(2) + epsilon)
        
        # é™åˆ¶å€¼èŒƒå›´ï¼Œé˜²æ­¢æå€¼
        slope = torch.clamp(slope, min=0.0, max=10.0)
        
        if ismask:
            # è¾¹ç¼˜mask
            edge_mask = smooth_mask_generate_torch(mask, 0.5, False)
            edge_mask = (edge_mask > 0.05).float()
            features = torch.cat([x, slope, edge_mask], dim=1)
        else:
            features = torch.cat([x, slope], dim=1)

        # è¿”å›ç®€åŒ–çš„åœ°å½¢ç‰¹å¾ï¼šåŸå§‹é«˜ç¨‹ + å¡åº¦
        
        features = F.dropout(features, p=0.1, training=self.training)
        
        return features'''
"""class EnhancedTerrainFeatureExtractor(nn.Module):
    def __init__(self, in_channels):
        super(EnhancedTerrainFeatureExtractor, self).__init__()
        
        # Sobelç®—å­
        self.sobel_x = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)
        self.sobel_y = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)
        
        # å…³é”®ï¼šç¡®ä¿è¾“å‡ºé€šé“æ•°å›ºå®š
        self.output_channels = in_channels * 2  # åŸå§‹ + å¡åº¦
        
        # ç‰¹å¾èåˆå±‚ - å°†ä»»æ„è¾“å…¥é€šé“æ•°æ˜ å°„åˆ°å›ºå®šè¾“å‡º
        self.feature_adapter = nn.Sequential(
            nn.Conv2d(in_channels * 2, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, self.output_channels, kernel_size=1)
        )
        
        self._init_sobel_filters(in_channels)
        
    def _init_sobel_filters(self, in_channels):
        with torch.no_grad():
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)
            
            sobel_x = sobel_x.reshape(1, 1, 3, 3).repeat(in_channels, 1, 1, 1)
            sobel_y = sobel_y.reshape(1, 1, 3, 3).repeat(in_channels, 1, 1, 1)
            
            self.sobel_x.weight.data = sobel_x
            self.sobel_y.weight.data = sobel_y
        
    def forward(self, x, mask, ismask=False):
        if torch.isnan(x).any():
            x = torch.nan_to_num(x, nan=0.0)
            
        # è®¡ç®—å¡åº¦
        grad_x = self.sobel_x(x)
        grad_y = self.sobel_y(x)
        slope = torch.sqrt(grad_x.pow(2) + grad_y.pow(2) + 1e-6)
        slope = torch.clamp(slope, min=0.0, max=10.0)
        
        # åŸºç¡€ç‰¹å¾ï¼šåŸå§‹ + å¡åº¦
        adapted_features = torch.cat([x, slope], dim=1)
        
        # é€‚é…åˆ°å›ºå®šç»´åº¦
        # adapted_features = self.feature_adapter(base_features)
        
        if ismask:
            # è¾¹ç¼˜maskä½œä¸ºé¢å¤–é€šé“
            edge_mask = smooth_mask_generate_torch(mask, 1.0, False)
            edge_mask = (edge_mask > 0.05).float()
            
            # å…³é”®ï¼šé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶èåˆï¼Œè€Œä¸æ˜¯ç®€å•æ‹¼æ¥
            mask_attention = torch.sigmoid(edge_mask)
            enhanced_features = adapted_features * (1 + mask_attention)
            
            return enhanced_features
        else:
            return adapted_features"""


class EnhancedTerrainFeatureExtractor(nn.Module):
    def __init__(self, in_channels):
        super(EnhancedTerrainFeatureExtractor, self).__init__()

        # å¡åº¦æå–ï¼ˆä½¿ç”¨Sobelç®—å­ï¼‰
        self.sobel_x = nn.Conv2d(
            in_channels, in_channels, kernel_size=3, padding=1, bias=False
        )
        self.sobel_y = nn.Conv2d(
            in_channels, in_channels, kernel_size=3, padding=1, bias=False
        )

        # å¤šå°ºåº¦ç‰¹å¾æå–
        self.multi_scale_conv = nn.ModuleList(
            [
                nn.Conv2d(in_channels, 16, kernel_size=3, padding=1),
                nn.Conv2d(in_channels, 16, kernel_size=5, padding=2),
                nn.Conv2d(in_channels, 16, kernel_size=7, padding=3),
            ]
        )

        # ç‰¹å¾èåˆ
        self.feature_fusion = nn.Sequential(
            nn.Conv2d(
                in_channels * 2 + 48, 64, kernel_size=3, padding=1
            ),  # åŸå§‹+å¡åº¦+å¤šå°ºåº¦
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, in_channels * 2, kernel_size=1),  # è¾“å‡ºï¼šåŸå§‹+å¢å¼ºå¡åº¦
        )

        self.feature_fusion2 = nn.Sequential(
            GatedConv2d(
                in_channels * 2 + 48,
                64,
                kernel_size=3,
                stride=1,
                padding=1,
                activation="lrelu",
                norm="bn",
                sn=True,
            ),  # åŸå§‹+å¡åº¦+å¤šå°ºåº¦
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, in_channels * 2, kernel_size=1),  # è¾“å‡ºï¼šåŸå§‹+å¢å¼ºå¡åº¦
        )

        # åˆå§‹åŒ–Sobelæ»¤æ³¢å™¨
        self._init_sobel_filters(in_channels)

    def _init_sobel_filters(self, in_channels):
        with torch.no_grad():
            sobel_x = torch.tensor(
                [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32
            )
            sobel_y = torch.tensor(
                [[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32
            )

            # åˆ›å»ºåˆ†ç»„å·ç§¯æ ¸
            sobel_x = sobel_x.reshape(1, 1, 3, 3).repeat(in_channels, 1, 1, 1)
            sobel_y = sobel_y.reshape(1, 1, 3, 3).repeat(in_channels, 1, 1, 1)

            self.sobel_x.weight.data = sobel_x
            self.sobel_y.weight.data = sobel_y

    def forward(self, x, mask, ismask=False):
        if torch.isnan(x).any():
            x = torch.nan_to_num(x, nan=0.0)

        # è®¡ç®—åŸºç¡€å¡åº¦
        grad_x = self.sobel_x(x)
        grad_y = self.sobel_y(x)
        slope = torch.sqrt(grad_x.pow(2) + grad_y.pow(2) + 1e-6)
        slope = torch.clamp(slope, min=0.0, max=10.0)

        # å¤šå°ºåº¦ç‰¹å¾æå–
        multi_scale_features = []
        for conv in self.multi_scale_conv:
            multi_scale_features.append(conv(x))
        multi_scale = torch.cat(multi_scale_features, dim=1)

        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        combined = torch.cat([x, slope, multi_scale], dim=1)
        enhanced_features = self.feature_fusion2(combined)

        if ismask:
            # è¾¹ç¼˜mask - ä¿®å¤çš„ç‰ˆæœ¬
            edge_mask = smooth_mask_generate_torch(mask, 1.0, False)
            edge_mask = (edge_mask > 0.05).float()
            features = torch.cat([enhanced_features, edge_mask], dim=1)
        else:
            features = enhanced_features

        return F.dropout(features, p=0.1, training=self.training)
class SimplifiedTerrainFeatureExtractor(nn.Module):
    """ç®€åŒ–ç‰ˆåœ°å½¢ç‰¹å¾æå–å™¨ - ç§»é™¤æ˜¾å­˜å¯†é›†å‹æ“ä½œ"""
    def __init__(self, in_channels):
        super(SimplifiedTerrainFeatureExtractor, self).__init__()
        
        # ğŸ”¥ ç§»é™¤å¤šå°ºåº¦å·ç§¯ - è¿™æ˜¯æ˜¾å­˜å¤§æˆ·
        # self.multi_scale_conv = ...  # åˆ é™¤
        
        # ğŸ”¥ ç§»é™¤é—¨æ§å·ç§¯ - åŒè·¯å¾„æ¶ˆè€—æ˜¾å­˜
        # self.feature_fusion2 = ...  # åˆ é™¤
        
        # åªä¿ç•™åŸºç¡€çš„å¡åº¦æå–
        self.sobel_x = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)
        self.sobel_y = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)
        
        # ğŸ”¥ ç®€åŒ–ç‰¹å¾èåˆ - ç”¨æ ‡å‡†å·ç§¯æ›¿ä»£é—¨æ§å·ç§¯
        self.feature_fusion = nn.Sequential(
            nn.Conv2d(in_channels * 2, in_channels * 2, kernel_size=1),  # 1x1å·ç§¯ï¼Œè®¡ç®—é‡å°
            nn.BatchNorm2d(in_channels * 2),
            nn.LeakyReLU(0.2, inplace=True)
        )
        
        # åˆå§‹åŒ–Sobelæ»¤æ³¢å™¨
        self._init_sobel_filters(in_channels)
        
    def _init_sobel_filters(self, in_channels):
        with torch.no_grad():
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)
            
            # åˆ›å»ºåˆ†ç»„å·ç§¯æ ¸
            sobel_x = sobel_x.reshape(1, 1, 3, 3).repeat(in_channels, 1, 1, 1)
            sobel_y = sobel_y.reshape(1, 1, 3, 3).repeat(in_channels, 1, 1, 1)
            
            self.sobel_x.weight.data = sobel_x
            self.sobel_y.weight.data = sobel_y
        
    def forward(self, x, mask, ismask=False):
        if torch.isnan(x).any():
            x = torch.nan_to_num(x, nan=0.0)
            
        # è®¡ç®—åŸºç¡€å¡åº¦
        grad_x = self.sobel_x(x)
        grad_y = self.sobel_y(x)
        slope = torch.sqrt(grad_x.pow(2) + grad_y.pow(2) + 1e-6)
        slope = torch.clamp(slope, min=0.0, max=10.0)
        
        # ğŸ”¥ ç§»é™¤å¤šå°ºåº¦ç‰¹å¾æå–
        # multi_scale_features = []  # åˆ é™¤
        # for conv in self.multi_scale_conv:  # åˆ é™¤
        #     multi_scale_features.append(conv(x))  # åˆ é™¤
        # multi_scale = torch.cat(multi_scale_features, dim=1)  # åˆ é™¤
        
        # ğŸ”¥ ç®€åŒ–ç»„åˆï¼šåªæœ‰åŸå§‹+å¡åº¦
        combined = torch.cat([x, slope], dim=1)  # in_channels * 2
        enhanced_features = self.feature_fusion(combined)  # è¾“å‡º in_channels * 2
        
        if ismask:
            # ğŸ”¥ ç®€åŒ–è¾¹ç¼˜maskå¤„ç† - é¿å…å¤æ‚çš„smooth_mask_generate_torch
            # edge_mask = smooth_mask_generate_torch(mask, 1.0, False)  # åˆ é™¤
            # edge_mask = (edge_mask > 0.05).float()  # åˆ é™¤
            
            # ç›´æ¥ä½¿ç”¨åŸå§‹maskï¼Œé¿å…é¢å¤–è®¡ç®—
            features = torch.cat([enhanced_features, mask], dim=1)  # in_channels * 2 + 1
        else:
            features = enhanced_features  # in_channels * 2
            
        return F.dropout(features, p=0.1, training=self.training)

class StabilizedSelfAttention(nn.Module):
    """ç¨³å®šç‰ˆè‡ªæ³¨æ„åŠ›æ¨¡å—"""

    def __init__(self, in_channels):
        super(StabilizedSelfAttention, self).__init__()
        # ä½¿ç”¨è°±å½’ä¸€åŒ–æé«˜ç¨³å®šæ€§
        self.query_conv = spectral_norm(
            nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        )
        self.key_conv = spectral_norm(
            nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        )
        self.value_conv = spectral_norm(
            nn.Conv2d(in_channels, in_channels, kernel_size=1)
        )

        # ä»è¾ƒå°å€¼å¼€å§‹
        self.gamma = nn.Parameter(torch.zeros(1))

        # ç¼©æ”¾å› å­
        self.scale_factor = torch.sqrt(
            torch.tensor(in_channels // 8, dtype=torch.float32)
        )

    def forward(self, x):
        batch_size, C, height, width = x.size()

        # æ£€æŸ¥è¾“å…¥æ˜¯å¦åŒ…å«NaN
        if torch.isnan(x).any():
            print("è­¦å‘Š: SelfAttentionè¾“å…¥åŒ…å«NaN!")
            x = torch.nan_to_num(x, nan=0.0)

        # å‹ç¼©ç‰¹å¾ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        proj_query = (
            self.query_conv(x).view(batch_size, -1, height * width).permute(0, 2, 1)
        )
        proj_key = self.key_conv(x).view(batch_size, -1, height * width)

        # è®¡ç®—æ³¨æ„åŠ›å›¾ï¼Œæ·»åŠ ç¼©æ”¾å› å­æé«˜æ•°å€¼ç¨³å®šæ€§
        energy = torch.bmm(proj_query, proj_key) / self.scale_factor

        # ä½¿ç”¨æ›´å®‰å…¨çš„softmax
        attention = F.softmax(energy, dim=-1)

        # æ£€æŸ¥æ³¨æ„åŠ›æƒé‡æ˜¯å¦åŒ…å«NaN
        if torch.isnan(attention).any():
            print("è­¦å‘Š: æ³¨æ„åŠ›æƒé‡åŒ…å«NaN!")
            attention = torch.nan_to_num(attention, nan=1.0 / attention.size(-1))

        # åº”ç”¨æ³¨æ„åŠ›
        proj_value = self.value_conv(x).view(batch_size, -1, height * width)
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, height, width)

        # ä½¿ç”¨æ›´å°çš„åˆå§‹gammaå€¼ï¼Œå¹¶é™åˆ¶å…¶æœ€å¤§å½±å“
        gamma_clamped = torch.clamp(self.gamma, -1.0, 1.0)
        return gamma_clamped * out + x


class DisGatedConv2d(nn.Module):
    """ç¨³å®šç‰ˆé—¨æ§å·ç§¯å±‚"""

    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        stride=1,
        padding=0,
        dilation=1,
        groups=1,
        bias=True,
    ):
        super(DisGatedConv2d, self).__init__()
        self.conv2d = spectral_norm(
            nn.Conv2d(
                in_channels,
                out_channels,
                kernel_size,
                stride,
                padding,
                dilation,
                groups,
                bias,
            )
        )
        self.mask_conv2d = spectral_norm(
            nn.Conv2d(
                in_channels,
                out_channels,
                kernel_size,
                stride,
                padding,
                dilation,
                groups,
                bias,
            )
        )

        # åˆå§‹åŒ–é—¨æ§å·ç§¯çš„åç½®ï¼Œä½¿åˆå§‹é˜¶æ®µå¤§éƒ¨åˆ†é—¨æ§å€¼æ¥è¿‘1
        if bias:
            self.mask_conv2d.bias.data.fill_(1.0)

    def forward(self, x, mask=None):
        # ç‰¹å¾é€šé“
        features = self.conv2d(x)

        # ç”Ÿæˆé—¨æ§é€šé“
        gates = torch.sigmoid(self.mask_conv2d(x))

        # å¦‚æœæä¾›äº†å¤–éƒ¨æ©ç ï¼Œå°†å®ƒä¸é—¨æ§æœºåˆ¶ç»“åˆ
        if mask is not None:
            if mask.shape[2:] != gates.shape[2:]:
                mask = F.interpolate(mask, size=gates.shape[2:], mode="nearest")
            gates = gates * mask

        # åº”ç”¨é—¨æ§æœºåˆ¶
        return features * gates


def smooth_mask_generate_torch(mask, sigma=1.0, visualize=False):
    """
    ä¿®å¤å‚æ•°é”™è¯¯çš„ç‰ˆæœ¬
    """
    mask = mask.float()

    if sigma <= 0:
        return mask

    # è®°å½•åŸå§‹çš„æœ‰æ•ˆåŒºåŸŸ
    valid_region = (mask > 0).float()

    # åˆ›å»ºé«˜æ–¯æ ¸
    kernel_size = int(2 * sigma * 3) + 1
    if kernel_size % 2 == 0:
        kernel_size += 1

    coords = torch.arange(kernel_size, dtype=mask.dtype, device=mask.device)
    coords = coords - kernel_size // 2
    gaussian_1d = torch.exp(-(coords**2) / (2 * sigma**2))
    gaussian_1d = gaussian_1d / gaussian_1d.sum()
    gaussian_2d = gaussian_1d.unsqueeze(0) * gaussian_1d.unsqueeze(1)

    # å¤„ç†æ¯ä¸ªé€šé“
    batch_size, channels = mask.shape[:2]
    smoothed_channels = []

    for c in range(channels):
        channel = mask[:, c : c + 1, :, :]
        valid_channel = valid_region[:, c : c + 1, :, :]

        # æ‰‹åŠ¨æ·»åŠ åå°„å¡«å……
        pad_size = kernel_size // 2
        padded_channel = F.pad(
            channel, (pad_size, pad_size, pad_size, pad_size), mode="reflect"
        )

        kernel = gaussian_2d.unsqueeze(0).unsqueeze(0)

        # ä½¿ç”¨æ ‡å‡†conv2dï¼ˆä¸æŒ‡å®špadding_modeï¼‰
        smoothed = F.conv2d(
            padded_channel, kernel, padding=0
        )  # padding=0å› ä¸ºå·²ç»æ‰‹åŠ¨å¡«å……

        # å…³é”®ï¼šåªåœ¨åŸå§‹æœ‰æ•ˆåŒºåŸŸå†…ä¿ç•™å¹³æ»‘ç»“æœ
        # smoothed = smoothed * valid_channel

        smoothed_channels.append(smoothed)

    smoothed_mask = torch.cat(smoothed_channels, dim=1)
    smoothed_mask = smoothed_mask.clamp(0, 1)

    # è®¡ç®—è¾¹ç•Œæƒé‡
    boundary_distance = torch.abs(smoothed_mask - 0.5)
    boundary_weights = 1.0 - (boundary_distance / 0.5)
    boundary_mask = boundary_weights.clamp(0, 1)

    if visualize:
        with torch.no_grad():
            mask_np = mask.detach().cpu().numpy()
            boundary_np = boundary_mask.detach().cpu().numpy()

            for b in range(min(mask_np.shape[0], 2)):
                plt.figure(figsize=(10, 5))
                plt.subplot(1, 2, 1)
                plt.title("Original Mask")
                plt.imshow(mask_np[b, 0], cmap="gray")
                plt.subplot(1, 2, 2)
                plt.title("Smoothed Mask")
                plt.imshow(boundary_np[b, 0], cmap="gray")
                plt.show()

    return boundary_mask


class GlobalDiscriminator(nn.Module):
    """å¤§å·ç§¯æ ¸ä¼˜åŒ–çš„å…¨å±€åˆ¤åˆ«å™¨ - 5å±‚å‡å°‘åˆ°3å±‚"""
    def __init__(self, input_channels=1, input_size=600):
        super(GlobalDiscriminator, self).__init__()
        self.input_shape = (input_channels, input_size, input_size)
        
        # ä¿æŒåŸæœ‰çš„åœ°å½¢ç‰¹å¾æå–å™¨
        self.terrain_extractor = EnhancedTerrainFeatureExtractor(input_channels)
        # è®¡ç®—é€šé“æ•°ï¼šåŸå§‹ + å¡åº¦ + æ©ç 
        enhanced_channels = input_channels * 2 + 1
        
        # ä¿æŒåŸæœ‰çš„ç©ºé—´æ³¨æ„åŠ›æ¨¡å—
        self.spatial_attention = nn.Sequential(
            spectral_norm(nn.Conv2d(enhanced_channels, 16, kernel_size=7, padding=3)),
            nn.BatchNorm2d(16),
            nn.LeakyReLU(0.2, inplace=True),
            spectral_norm(nn.Conv2d(16, 1, kernel_size=7, padding=3)),
            nn.Sigmoid()
        )
        
        # ğŸ”¥ å¤§å·ç§¯æ ¸ç­–ç•¥ï¼šç”¨3å±‚å¤§å·ç§¯æ ¸æ›¿ä»£5å±‚å°å·ç§¯æ ¸
        # ç¬¬ä¸€å±‚ï¼šå¤§å·ç§¯æ ¸ + å¤§æ­¥é•¿ï¼Œå¿«é€Ÿä¸‹é‡‡æ ·
        self.conv1 = DisGatedConv2d(
            enhanced_channels, 64, kernel_size=8, stride=4, padding=2  # 8x8æ ¸ï¼Œstride=4
        )
        self.bn1 = nn.BatchNorm2d(64)
        self.act1 = nn.LeakyReLU(0.2, inplace=True)
        # è¾“å‡ºå°ºå¯¸ï¼š600 â†’ 150

        # ç¬¬äºŒå±‚ï¼šä¸­ç­‰å·ç§¯æ ¸ + ä¸­ç­‰æ­¥é•¿
        self.conv2 = DisGatedConv2d(
            64, 128, kernel_size=6, stride=3, padding=1  # 6x6æ ¸ï¼Œstride=3
        )
        self.bn2 = nn.BatchNorm2d(128)
        self.act2 = nn.LeakyReLU(0.2, inplace=True)
        # è¾“å‡ºå°ºå¯¸ï¼š150 â†’ 50

        # ç¬¬ä¸‰å±‚ï¼šå¤§å·ç§¯æ ¸ï¼Œå……åˆ†ä¸‹é‡‡æ ·åˆ°å°å°ºå¯¸
        self.conv3 = nn.Conv2d(
            128, 256, kernel_size=7, stride=5, padding=1  # 7x7æ ¸ï¼Œstride=5
        )
        self.bn3 = nn.BatchNorm2d(256)
        self.act3 = nn.LeakyReLU(0.2, inplace=True)
        # è¾“å‡ºå°ºå¯¸ï¼š50 â†’ 10

        # ä¿æŒè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†åœ¨æ›´å°çš„ç‰¹å¾å›¾ä¸Š(10x10)
        self.self_attention = StabilizedSelfAttention(256)

        # æœ€ç»ˆå…¨å±€æ± åŒ–ï¼Œç¡®ä¿å›ºå®šè¾“å‡ºå°ºå¯¸
        self.final_pool = nn.AdaptiveAvgPool2d(4)  # å¼ºåˆ¶åˆ°4x4
        self.flatten = nn.Flatten()
        
        # å›ºå®šç‰¹å¾å°ºå¯¸ï¼š256 * 4 * 4 = 4,096
        self.feature_size = 256 * 4 * 4
        
        self.linear = nn.Sequential(
            spectral_norm(nn.Linear(self.feature_size, 1024)),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.5)
        )

    def forward(self, globalin, mask):
        if torch.isnan(globalin).any():
            print("è­¦å‘Š: GlobalDiscriminatorè¾“å…¥åŒ…å«NaN!")
            globalin = torch.nan_to_num(globalin, nan=0.0)
            
        mask = mask.float()
        
        # æå–åœ°å½¢ç‰¹å¾
        terrain_features = self.terrain_extractor(globalin, mask)
        
        # æ‹¼æ¥ç‰¹å¾å’Œæ©ç 
        x = torch.cat([terrain_features, mask], dim=1)
        
        # åº”ç”¨ç©ºé—´æ³¨æ„åŠ›
        attention = self.spatial_attention(x)
        x = x * attention
        
        # åˆ›å»ºä¸‹é‡‡æ ·æ©ç è¿›è¡Œè·Ÿè¸ª
        current_mask = mask
        
        # ç¬¬ä¸€å±‚ï¼šå¤§å·ç§¯æ ¸å¿«é€Ÿä¸‹é‡‡æ ·
        x = self.conv1(x, current_mask)
        x = self.bn1(x)
        x = self.act1(x)
        current_mask = F.interpolate(current_mask, scale_factor=0.25, mode='nearest')  # stride=4
        
        # ç¬¬äºŒå±‚ï¼šä¸­ç­‰å·ç§¯æ ¸
        x = self.conv2(x, current_mask)
        x = self.bn2(x)
        x = self.act2(x)
        
        # ç¬¬ä¸‰å±‚ï¼šæœ€ç»ˆä¸‹é‡‡æ ·
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.act3(x)

        # åº”ç”¨è‡ªæ³¨æ„åŠ›ï¼ˆåœ¨å°ç‰¹å¾å›¾ä¸Šï¼Œæ˜¾å­˜å‹å¥½ï¼‰
        x = self.self_attention(x)

        # æœ€ç»ˆæ± åŒ–åˆ°å›ºå®šå°ºå¯¸
        x = self.final_pool(x)
        x = self.flatten(x)
        
        # print(f"GlobalDiscriminator - After pooling and flatten: {x.shape}")
        # print(f"Expected feature_size: {self.feature_size}")
        
        # å…¨è¿æ¥å±‚
        x = self.linear(x)
        
        # print(f"GlobalDiscriminator - Final output: {x.shape}")
        
        if torch.isnan(x).any():
            x = torch.nan_to_num(x, nan=0.0)

        return x


class LocalDiscriminator(nn.Module):
    """ä¿å®ˆä¼˜åŒ–çš„å±€éƒ¨åˆ¤åˆ«å™¨ - ä¿æŒ4å±‚ï¼Œå‡å°‘é€šé“æ•°"""
    def __init__(self, input_channels=1, input_size=33):
        super(LocalDiscriminator, self).__init__()
        self.input_shape = (input_channels, input_size, input_size)
        
        # ä¿æŒåŸæœ‰çš„åœ°å½¢ç‰¹å¾æå–å™¨
        self.terrain_extractor = EnhancedTerrainFeatureExtractor(input_channels)
        # è®¡ç®—è¾“å‡ºé€šé“æ•°ï¼šåŸå§‹ + å¡åº¦ + æ©ç 
        terrain_channels = input_channels * 3
        
        # ğŸ”§ ä¿å®ˆç­–ç•¥ï¼šä¿æŒ4å±‚ç»“æ„ï¼Œä½¿ç”¨æ ‡å‡†å·ç§¯æ ¸ï¼Œå‡å°‘é€šé“æ•°
        # ç¬¬ä¸€å±‚å·ç§¯ - ä½¿ç”¨è°±å½’ä¸€åŒ–
        self.conv1 = GatedConv2d(terrain_channels, 64, kernel_size=4, stride=2, padding=1,
                        pad_type='reflect', activation='lrelu', norm='bn')

        # ç¬¬äºŒå±‚å·ç§¯
        self.conv2 = GatedConv2d(64, 128, kernel_size=4, stride=2, padding=1,
                        pad_type='reflect', activation='lrelu', norm='bn')
        
        # ç¬¬ä¸‰å±‚å·ç§¯
        self.conv3 = GatedConv2d(128, 256, kernel_size=4, stride=2, padding=1,
                        pad_type='reflect', activation='lrelu', norm='bn')

        # ç¬¬å››å±‚å·ç§¯ - æœ€åä¸€å±‚å·ç§¯
        self.conv4 = GatedConv2d(256, 512, kernel_size=4, stride=2, padding=1,
                        pad_type='reflect', activation='lrelu', norm='bn')

        # æ ‡å‡†å±•å¹³
        self.flatten = nn.Flatten()
        
        # ğŸ“ ä¿®å¤ç‰¹å¾å°ºå¯¸è®¡ç®— - ä½¿ç”¨æ­£ç¡®çš„å…¬å¼å’Œæœ€ç»ˆé€šé“æ•°
        h = w = input_size
        print(f"LocalDiscriminator __init__ - Initial size: {h}x{w}")
        
        # è®¡ç®—æ¯å±‚å·ç§¯åçš„å°ºå¯¸å˜åŒ–
        for layer in range(4):  # 4å±‚å·ç§¯ï¼Œæ¯å±‚stride=2, padding=1, kernel=4
            h_new = (h + 2 * 1 - 4) // 2 + 1  # (input + 2*padding - kernel) // stride + 1
            w_new = (w + 2 * 1 - 4) // 2 + 1
            print(f"LocalDiscriminator __init__ - Layer {layer+1}: {h}x{w} -> {h_new}x{w_new}")
            h, w = h_new, w_new
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æœ€åä¸€å±‚çš„é€šé“æ•°512è€Œä¸æ˜¯256
        final_channels = 512  # conv4çš„è¾“å‡ºé€šé“æ•°
        self.feature_size = final_channels * h * w
        
        '''print(f"LocalDiscriminator __init__ - Final calculation:")
        print(f"  - Final spatial size: {h}x{w}")
        print(f"  - Final channels: {final_channels}")
        print(f"  - Calculated feature_size: {self.feature_size}")'''
        
        # ä¿æŒè¾“å‡º1024ç»´
        self.linear = nn.Sequential(
            spectral_norm(nn.Linear(self.feature_size, 1024)),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.5)
        )

    def forward(self, local, mask):
        if torch.isnan(local).any():
            print("è­¦å‘Š: LocalDiscriminatorè¾“å…¥åŒ…å«NaN!")
            local = torch.nan_to_num(local, nan=0.0)
            
        # print(f"LocalDiscriminator - Input shape: {local.shape}, {mask.shape}")
        
        # æå–åœ°å½¢ç‰¹å¾
        x = self.terrain_extractor(local, mask, True)
        # print(f"LocalDiscriminator - After terrain_extractor: {x.shape}")
        
        # ç¬¬ä¸€å±‚å·ç§¯
        x = self.conv1(x)
        # print(f"LocalDiscriminator - After conv1: {x.shape}")
        
        # ç¬¬äºŒå±‚å·ç§¯
        x = self.conv2(x)
        # print(f"LocalDiscriminator - After conv2: {x.shape}")
        
        # ç¬¬ä¸‰å±‚å·ç§¯
        x = self.conv3(x)
        # print(f"LocalDiscriminator - After conv3: {x.shape}")
        
        # ç¬¬å››å±‚å·ç§¯
        x = self.conv4(x)
        # print(f"LocalDiscriminator - After conv4: {x.shape}")

        # ç‰¹å¾å±•å¹³
        x = self.flatten(x)
        # print(f"LocalDiscriminator - After flatten: {x.shape}")
        # print(f"LocalDiscriminator - Expected feature_size: {self.feature_size}")
        
        # ğŸ”§ æ”¹è¿›çš„ç»´åº¦æ£€æŸ¥å’ŒåŠ¨æ€è°ƒæ•´
        actual_feature_size = x.size(1)
        if actual_feature_size != self.feature_size:
            print(f"âš ï¸ LocalDiscriminatorç‰¹å¾å°ºå¯¸ä¸åŒ¹é…!")
            print(f"   é¢„æœŸ: {self.feature_size}")
            print(f"   å®é™…: {actual_feature_size}")
            print(f"   æ¯”ä¾‹: {actual_feature_size / self.feature_size:.2f}")
            
            # é‡æ–°åˆ›å»ºçº¿æ€§å±‚ä»¥åŒ¹é…å®é™…å°ºå¯¸
            if not hasattr(self, '_feature_size_adjusted'):
                self._feature_size_adjusted = True
                print(f"   ğŸ”§ åŠ¨æ€è°ƒæ•´çº¿æ€§å±‚è¾“å…¥ç»´åº¦: {actual_feature_size} -> 1024")
                self.linear = nn.Sequential(
                    spectral_norm(nn.Linear(actual_feature_size, 1024)),
                    nn.LeakyReLU(0.2, inplace=True),
                    nn.Dropout(0.5)
                ).to(x.device)
                
                # æ›´æ–°feature_sizeä»¥é¿å…é‡å¤è­¦å‘Š
                self.feature_size = actual_feature_size
        
        # å…¨è¿æ¥å±‚
        x = self.linear(x)
        # print(f"LocalDiscriminator - Final output: {x.shape}")
        
        if torch.isnan(x).any():
            x = torch.nan_to_num(x, nan=0.0)

        return x

# ğŸ”§ è¾…åŠ©å‡½æ•°ï¼šç²¾ç¡®è®¡ç®—å·ç§¯å±‚è¾“å‡ºå°ºå¯¸
def calculate_conv_output_size(input_size, kernel_size, stride, padding):
    """
    ç²¾ç¡®è®¡ç®—å·ç§¯å±‚è¾“å‡ºå°ºå¯¸
    å…¬å¼: output_size = (input_size + 2*padding - kernel_size) // stride + 1
    """
    return (input_size + 2 * padding - kernel_size) // stride + 1


class ContextDiscriminator(nn.Module):
    """ä¿æŒåŸæœ‰ç»“æ„çš„ä¸Šä¸‹æ–‡åˆ¤åˆ«å™¨"""
    def __init__(
        self,
        local_input_channels=1,
        local_input_size=33,
        global_input_channels=1,
        global_input_size=600,
    ):
        super(ContextDiscriminator, self).__init__()
        
        # ä½¿ç”¨å¤§å·ç§¯æ ¸ä¼˜åŒ–åçš„åˆ¤åˆ«å™¨
        self.model_ld = LocalDiscriminator(
            input_channels=local_input_channels, input_size=local_input_size
        )
        self.model_gd = GlobalDiscriminator(
            input_channels=global_input_channels, input_size=global_input_size
        )

        # æ‰å¹³åŒ–å±‚
        self.flatten_ld = nn.Flatten()
        self.flatten_gd = nn.Flatten()

        # ä¿æŒåŸæœ‰çš„ç‰¹å¾èåˆå±‚ - ä¿®å¤ç»´åº¦åŒ¹é…é—®é¢˜
        self.fusion = nn.Sequential(
            spectral_norm(nn.Linear(1024 + 1024, 1024)),  # ç¡®ä¿è¾“å…¥æ˜¯2048ç»´
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.6)
        )

        # ä¿æŒåŸæœ‰çš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            spectral_norm(nn.Linear(1024, 512)),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.5),
            spectral_norm(nn.Linear(512, 1))
        )

    def forward(self, x_ld, x_lm, x_gd, x_gm):
        # é€šè¿‡å¤§å·ç§¯æ ¸ä¼˜åŒ–åçš„åˆ¤åˆ«å™¨
        x_ld = self.model_ld(x_ld, x_lm)  # åº”è¯¥æ˜¯1024ç»´ç‰¹å¾
        x_gd = self.model_gd(x_gd, x_gm)  # åº”è¯¥æ˜¯1024ç»´ç‰¹å¾

        # è°ƒè¯•ä¿¡æ¯
        # print(f"Local features shape: {x_ld.shape}")
        # print(f"Global features shape: {x_gd.shape}")

        # ç¡®ä¿å½¢çŠ¶æ­£ç¡®
        if len(x_ld.shape) > 2:
            x_ld = self.flatten_ld(x_ld)
        if len(x_gd.shape) > 2:
            x_gd = self.flatten_gd(x_gd)

        # print(f"After flatten - Local: {x_ld.shape}, Global: {x_gd.shape}")

        # è¿æ¥ç‰¹å¾
        combined = torch.cat([x_ld, x_gd], dim=1)
        # print(f"Combined features shape: {combined.shape}")
        
        # èåˆç‰¹å¾
        fused = self.fusion(combined)
        # print(f"Fused features shape: {fused.shape}")

        # è¾“å‡ºlogits
        logits = self.classifier(fused)
        
        if torch.isnan(logits).any():
            print("è­¦å‘Š: ContextDiscriminatorè¾“å‡ºåŒ…å«NaN!")
            logits = torch.zeros_like(logits, device=logits.device)

        return logits, x_ld, x_gd

if __name__ == "__main__":
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    test = 1
    # Remove the "logs" folder if it exists
    logs_path = "logsgd"
    if os.path.exists(logs_path):
        shutil.rmtree(logs_path)

    match test:
        case 0:  # Generator
            cn = CompletionNetwork().to(device)
            input1 = torch.ones(1, 1, 33, 33)
            input2 = torch.ones(1, 1, 600, 600)

            input1 = input1.to(device)
            input2 = input2.to(device)
            writer = SummaryWriter("logs")
            writer.add_graph(cn, [input1, input1, input2, input2])
            writer.close()
        case 1:  # Local Discriminator
            ld = LocalDiscriminator().to(device)
            input1 = torch.ones(1, 1, 33, 33)

            input1 = input1.to(device)
            writer = SummaryWriter("logsld")
            writer.add_graph(ld, [input1, input1])
            writer.close()
        case 2:  # Global Discriminator
            gd = GlobalDiscriminator().to(device)
            input1 = torch.ones(1, 1, 600, 600)

            input1 = input1.to(device)
            writer = SummaryWriter("logsgd")
            writer.add_graph(gd, [input1, input1])
            writer.close()

    # Run the tensorboard command
    # os.system("tensorboard --logdir=logs")
    # Open the specified URL directly in the default web browser
    # webbrowser.open("http://localhost:6006/?darkMode=true#graphs&run=.")
