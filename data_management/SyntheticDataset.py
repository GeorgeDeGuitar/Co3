import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import random
from typing import Tuple, List, Dict, Optional
from tqdm import tqdm
import matplotlib.pyplot as plt
from pathlib import Path
from scipy.ndimage import (
    binary_dilation,
    binary_erosion,
    binary_opening,
    binary_closing,
    label,
)

# ç§‘å­¦è®¡ç®—ç›¸å…³å¯¼å…¥
try:
    from scipy.ndimage import binary_dilation, binary_erosion, label, binary_closing
    from skimage.morphology import disk
    from skimage.draw import ellipse

    SCIPY_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: scipy/skimageæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„maskç”Ÿæˆæ–¹æ³•")
    SCIPY_AVAILABLE = False


class MultiScaleDEMDataset(Dataset):
    """
    å¤šå°ºå¯¸DEMæ•°æ®é›†
    æ”¯æŒä»600x600å‚è€ƒæ–‡ä»¶ç”Ÿæˆä¸åŒå°ºå¯¸çš„localæ•°æ®ç”¨äºè®­ç»ƒ
    """

    def __init__(
        self,
        reference_dir: str,
        local_sizes: List[int] = [64, 128, 256],  # æ”¯æŒå¤šç§localå°ºå¯¸
        samples_per_file: int = 10,  # æ¯ä¸ªå‚è€ƒæ–‡ä»¶ç”Ÿæˆçš„æ ·æœ¬æ•°
        global_coverage_range: Tuple[float, float] = (
            0.3,
            1.0,
        ),  # global maskè¦†ç›–ç‡èŒƒå›´
        local_missing_range: Tuple[float, float] = (0.3, 0.7),  # local maskç¼ºå¤±ç‡èŒƒå›´
        max_edge_missing_ratio: float = 0.3,  # localä¸­åŒ…å«globalè¾¹ç¼˜çš„æœ€å¤§æ¯”ä¾‹
        enable_edge_connection: bool = True,  # æ˜¯å¦å…è®¸local maskä¸è¾¹ç¼˜ç›¸æ¥
        edge_connection_prob: float = 0.02,  # é™ä½åˆ°2%çš„è¾¹ç¼˜ç›¸æ¥æ¦‚ç‡
        seed: int = 42,
        cache_reference_files: bool = True,  # æ˜¯å¦ç¼“å­˜å‚è€ƒæ–‡ä»¶
        visualization_samples: int = 0,  # å¯è§†åŒ–æ ·æœ¬æ•°é‡(0è¡¨ç¤ºä¸å¯è§†åŒ–)
    ):
        """
        Args:
            reference_dir: å‚è€ƒæ–‡ä»¶ç›®å½•ï¼ŒåŒ…å«600x600çš„.npyæ–‡ä»¶
            local_sizes: localæ•°æ®çš„å°ºå¯¸åˆ—è¡¨ï¼Œå¿…é¡»ä¸ºå¥‡æ•°
            samples_per_file: æ¯ä¸ªå‚è€ƒæ–‡ä»¶ç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬æ•°
            global_coverage_range: global maskçš„è¦†ç›–ç‡èŒƒå›´(30%-100%)
            local_missing_range: local maskçš„ç¼ºå¤±ç‡èŒƒå›´
            max_edge_missing_ratio: localä¸­å…è®¸åŒ…å«globalè¾¹ç¼˜çš„æœ€å¤§æ¯”ä¾‹
            enable_edge_connection: æ˜¯å¦å…è®¸local maskä¸è¾¹ç¼˜ç›¸æ¥
            edge_connection_prob: ä¸è¾¹ç¼˜ç›¸æ¥çš„æ¦‚ç‡(é™ä½åˆ°2%)
            seed: éšæœºç§å­
            cache_reference_files: æ˜¯å¦é¢„ç¼“å­˜å‚è€ƒæ–‡ä»¶åˆ°å†…å­˜
            visualization_samples: å¯è§†åŒ–çš„æ ·æœ¬æ•°é‡
        """

        # å‚æ•°éªŒè¯
        for size in local_sizes:
            if size % 2 == 0:
                raise ValueError(f"local_sizeå¿…é¡»ä¸ºå¥‡æ•°ï¼Œå½“å‰å€¼: {size}")
            if size >= 600:
                raise ValueError(f"local_sizeå¿…é¡»å°äº600ï¼Œå½“å‰å€¼: {size}")

        self.reference_dir = reference_dir
        self.local_sizes = sorted(local_sizes)  # æ’åºä»¥ä¾¿ä¸€è‡´æ€§
        self.samples_per_file = samples_per_file
        self.global_coverage_range = global_coverage_range
        self.local_missing_range = local_missing_range
        self.max_edge_missing_ratio = max_edge_missing_ratio
        self.enable_edge_connection = enable_edge_connection
        self.edge_connection_prob = edge_connection_prob
        self.cache_reference_files = cache_reference_files
        self.visualization_samples = visualization_samples

        # è®¾ç½®éšæœºç§å­
        random.seed(seed)
        np.random.seed(seed)

        # æ‰«æå‚è€ƒæ–‡ä»¶
        self.reference_files = self._scan_reference_files()

        # è®¡ç®—æ€»æ•°æ®é‡
        self.total_samples = len(self.reference_files) * samples_per_file

        # é¢„ç¼“å­˜å‚è€ƒæ–‡ä»¶(å¯é€‰)
        self.reference_cache = {}
        if cache_reference_files:
            self._cache_reference_files()

        print(f"ğŸ¯ å¤šå°ºå¯¸DEMæ•°æ®é›†åˆå§‹åŒ–å®Œæˆ:")
        print(f"   å‚è€ƒæ–‡ä»¶: {len(self.reference_files)} ä¸ª")
        print(f"   Localå°ºå¯¸: {self.local_sizes}")
        print(f"   æ¯æ–‡ä»¶æ ·æœ¬æ•°: {samples_per_file}")
        print(f"   æ€»æ ·æœ¬æ•°: {self.total_samples}")
        print(
            f"   Globalè¦†ç›–ç‡: {global_coverage_range[0]:.1%} - {global_coverage_range[1]:.1%}"
        )
        print(
            f"   Localç¼ºå¤±ç‡: {local_missing_range[0]:.1%} - {local_missing_range[1]:.1%}"
        )
        print(f"   è¾¹ç¼˜ç›¸æ¥æ¦‚ç‡: {edge_connection_prob:.1%}")

        # å¯è§†åŒ–éƒ¨åˆ†æ ·æœ¬
        """if visualization_samples > 0:
            self._visualize_samples(visualization_samples)"""

    def _scan_reference_files(self) -> List[str]:
        """æ‰«æå‚è€ƒæ–‡ä»¶ç›®å½•"""
        reference_files = []

        if not os.path.exists(self.reference_dir):
            raise ValueError(f"å‚è€ƒç›®å½•ä¸å­˜åœ¨: {self.reference_dir}")

        for file_path in Path(self.reference_dir).glob("*.npy"):
            try:
                # å¿«é€ŸéªŒè¯æ–‡ä»¶æ ¼å¼
                data = np.load(file_path, mmap_mode="r")
                if data.shape == (600, 600):
                    reference_files.append(str(file_path))
                else:
                    print(f"âš ï¸ è·³è¿‡é600x600æ–‡ä»¶: {file_path}")
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–‡ä»¶ {file_path}: {e}")

        if len(reference_files) == 0:
            raise ValueError(
                f"åœ¨ {self.reference_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„600x600 .npyæ–‡ä»¶"
            )

        return sorted(reference_files)

    def _cache_reference_files(self):
        """é¢„ç¼“å­˜å‚è€ƒæ–‡ä»¶åˆ°å†…å­˜"""
        print("ğŸ”„ æ­£åœ¨ç¼“å­˜å‚è€ƒæ–‡ä»¶...")

        for file_path in tqdm(self.reference_files, desc="ç¼“å­˜æ–‡ä»¶"):
            try:
                data = np.load(file_path).astype(np.float32)
                self.reference_cache[file_path] = data
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        print(f"âœ… å·²ç¼“å­˜ {len(self.reference_cache)} ä¸ªæ–‡ä»¶")

    def _load_reference_data(self, file_path: str) -> np.ndarray:
        """åŠ è½½å‚è€ƒæ•°æ®"""
        if self.cache_reference_files and file_path in self.reference_cache:
            return self.reference_cache[file_path].copy()
        else:
            return np.load(file_path).astype(np.float32)

    def _generate_initial_mask(self, data: np.ndarray) -> np.ndarray:
        """ç”Ÿæˆinitial mask (å€¼ä¸º0çš„ä½ç½®mask=0)"""
        return (data != 0).astype(np.float32)

    def _generate_global_mask(
        self, initial_mask: np.ndarray, coverage_ratio: float
    ) -> np.ndarray:
        """
        æ”¹è¿›ç‰ˆglobal maskç”Ÿæˆï¼šæé«˜è¾¹ç¼˜å¤šæ ·æ€§ï¼Œç¡®ä¿æ›´è‡ªç„¶çš„è¾¹ç•Œ
        """
        h, w = initial_mask.shape

        # å¦‚æœè¦†ç›–ç‡æ¥è¿‘1.0ï¼Œç›´æ¥è¿”å›initial_mask
        if coverage_ratio >= 0.95:
            return initial_mask.copy()

        # è®¡ç®—ç›®æ ‡åƒç´ æ•°
        initial_valid_pixels = np.sum(initial_mask)
        target_pixels = int(initial_valid_pixels * coverage_ratio)

        if target_pixels <= 0:
            return np.zeros_like(initial_mask)

        # é€‰æ‹©ç”Ÿæˆç­–ç•¥ï¼Œå¢åŠ å¤šæ ·æ€§
        strategy_choice = random.random()

        if strategy_choice < 0.3:  # 30% - æ‰©å±•å¼ç”Ÿæˆï¼ˆä»å°æ ¸å¿ƒæ‰©å±•ï¼‰
            current_mask = self._expansion_based_generation(initial_mask, target_pixels)
        elif strategy_choice < 0.6:  # 30% - è¾¹ç•Œæ”¶ç¼©å¼ï¼ˆä»å¤–å‘å†…ï¼‰
            current_mask = self._boundary_contraction_generation(
                initial_mask, target_pixels
            )
        else:  # 40% - æ··åˆå¼ç”Ÿæˆ
            current_mask = self._mixed_generation(initial_mask, target_pixels)

        # ç¡®ä¿è¿é€šæ€§å’Œå¹³æ»‘æ€§
        current_mask = self._ensure_connectivity_and_smoothness(
            current_mask, initial_mask
        )

        return current_mask

    def _expansion_based_generation(
        self, initial_mask: np.ndarray, target_pixels: int
    ) -> np.ndarray:
        """
        åŸºäºæ‰©å±•çš„ç”Ÿæˆæ–¹æ³•ï¼šä»æ ¸å¿ƒåŒºåŸŸå‘å¤–æ‰©å±•
        """
        h, w = initial_mask.shape

        # åˆ›å»ºä¸€ä¸ªè¾ƒå°çš„æ ¸å¿ƒåŒºåŸŸä½œä¸ºèµ·ç‚¹
        core_ratio = random.uniform(0.1, 0.3)  # æ ¸å¿ƒåŒºåŸŸå 10%-30%
        core_pixels = max(int(target_pixels * core_ratio), 100)

        # ç”Ÿæˆæ ¸å¿ƒåŒºåŸŸï¼ˆåœ¨ä¸­å¿ƒé™„è¿‘ï¼‰
        center_margin = min(h // 4, w // 4)
        core_mask = self._generate_core_region(initial_mask, core_pixels, center_margin)

        # ä»æ ¸å¿ƒåŒºåŸŸæ‰©å±•åˆ°ç›®æ ‡å¤§å°
        expansion_ratio = (target_pixels - np.sum(core_mask)) / (
            np.sum(initial_mask) - np.sum(core_mask)
        )
        expansion_ratio = max(0.0, min(1.0, expansion_ratio))

        final_mask = self._generate_partial_expansion(
            core_mask, initial_mask, initial_mask, expansion_ratio
        )

        return final_mask

    def _generate_core_region(
        self, initial_mask: np.ndarray, core_pixels: int, margin: int
    ) -> np.ndarray:
        """
        åœ¨ä¸­å¿ƒåŒºåŸŸç”Ÿæˆæ ¸å¿ƒmask
        """
        h, w = initial_mask.shape
        core_mask = np.zeros_like(initial_mask)

        # é€‰æ‹©æ ¸å¿ƒä¸­å¿ƒç‚¹ï¼ˆé¿å¼€è¾¹ç¼˜ï¼‰
        center_y = random.randint(margin, h - margin - 1)
        center_x = random.randint(margin, w - margin - 1)

        # ç¡®ä¿ä¸­å¿ƒç‚¹åœ¨initial_maskæœ‰æ•ˆåŒºåŸŸå†…
        valid_positions = np.where(initial_mask == 1)
        if len(valid_positions[0]) > 0:
            # æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡ä¸­å¿ƒçš„æœ‰æ•ˆç‚¹
            distances = (valid_positions[0] - center_y) ** 2 + (
                valid_positions[1] - center_x
            ) ** 2
            closest_idx = np.argmin(distances)
            center_y = valid_positions[0][closest_idx]
            center_x = valid_positions[1][closest_idx]

        # ä»ä¸­å¿ƒç‚¹å¼€å§‹åŒºåŸŸç”Ÿé•¿
        core_mask[center_y, center_x] = 1
        current_pixels = 1

        # ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œæ‰©å±•
        expansion_queue = [(center_y, center_x)]
        visited = {(center_y, center_x)}

        directions = [
            (-1, 0),
            (1, 0),
            (0, -1),
            (0, 1),
            (-1, -1),
            (-1, 1),
            (1, -1),
            (1, 1),
        ]

        while current_pixels < core_pixels and expansion_queue:
            y, x = expansion_queue.pop(0)

            # éšæœºé€‰æ‹©æ‰©å±•æ–¹å‘
            random.shuffle(directions)

            for dy, dx in directions:
                if current_pixels >= core_pixels:
                    break

                # éšæœºæ‰©å±•åŠå¾„ï¼Œå¢åŠ ä¸è§„åˆ™æ€§
                radius = random.randint(1, 3)
                ny, nx = y + dy * radius, x + dx * radius

                if (
                    0 <= ny < h
                    and 0 <= nx < w
                    and (ny, nx) not in visited
                    and initial_mask[ny, nx] == 1
                ):

                    # æ§åˆ¶æ‰©å±•æ¦‚ç‡ï¼Œåˆ›é€ ä¸è§„åˆ™è¾¹ç•Œ
                    expand_prob = 0.7 - 0.3 * (
                        current_pixels / core_pixels
                    )  # éšç€å¢é•¿é™ä½æ¦‚ç‡
                    if random.random() < expand_prob:
                        core_mask[ny, nx] = 1
                        current_pixels += 1
                        expansion_queue.append((ny, nx))
                        visited.add((ny, nx))

        return core_mask

    def _boundary_contraction_generation(
        self, initial_mask: np.ndarray, target_pixels: int
    ) -> np.ndarray:
        """
        è¾¹ç•Œæ”¶ç¼©å¼ç”Ÿæˆï¼šä»initial_maskå¼€å§‹ï¼Œéšæœºæ”¶ç¼©è¾¹ç•Œ
        """
        h, w = initial_mask.shape
        current_mask = initial_mask.copy()
        current_pixels = np.sum(current_mask)

        to_remove = current_pixels - target_pixels
        if to_remove <= 0:
            return current_mask

        removed_pixels = 0
        max_iterations = 50
        iteration = 0

        while removed_pixels < to_remove and iteration < max_iterations:
            iteration += 1

            # æ‰¾åˆ°è¾¹ç•Œç‚¹
            if SCIPY_AVAILABLE:
                boundary = current_mask - binary_erosion(current_mask, disk(1))
            else:
                boundary = self._find_boundary_simple(current_mask)

            boundary_coords = np.where(boundary > 0)
            if len(boundary_coords[0]) == 0:
                break

            boundary_points = list(zip(boundary_coords[0], boundary_coords[1]))

            # éšæœºé€‰æ‹©è¦ç§»é™¤çš„è¾¹ç•ŒåŒºåŸŸ
            removal_strategy = random.choice(
                ["single_edge", "random_patches", "corner_bias"]
            )

            if removal_strategy == "single_edge":
                removed_pixels += self._remove_from_single_edge(
                    current_mask,
                    boundary_points,
                    min(to_remove - removed_pixels, len(boundary_points) // 3),
                )
            elif removal_strategy == "random_patches":
                removed_pixels += self._remove_random_patches(
                    current_mask,
                    boundary_points,
                    min(to_remove - removed_pixels, len(boundary_points) // 2),
                )
            else:  # corner_bias
                removed_pixels += self._remove_with_corner_bias(
                    current_mask,
                    boundary_points,
                    min(to_remove - removed_pixels, len(boundary_points) // 2),
                )

        return current_mask

    def _mixed_generation(
        self, initial_mask: np.ndarray, target_pixels: int
    ) -> np.ndarray:
        """
        æ··åˆå¼ç”Ÿæˆï¼šç»“åˆå¤šç§ç­–ç•¥
        """
        h, w = initial_mask.shape

        # å…ˆç”¨è¾¹ç•Œæ”¶ç¼©ç”Ÿæˆä¸€ä¸ªä¸­ç­‰å¤§å°çš„mask
        intermediate_ratio = random.uniform(0.6, 0.8)
        intermediate_pixels = int(np.sum(initial_mask) * intermediate_ratio)
        intermediate_mask = self._boundary_contraction_generation(
            initial_mask, intermediate_pixels
        )

        # ç„¶åæ·»åŠ ä¸€äº›éšæœºçš„"å²›å±¿"åŒºåŸŸ
        remaining_pixels = target_pixels - np.sum(intermediate_mask)

        if remaining_pixels > 0:
            # åœ¨intermediate_maskä¹‹å¤–ä½†initial_maskä¹‹å†…æ·»åŠ å°åŒºåŸŸ
            available_region = initial_mask - intermediate_mask
            available_coords = np.where(available_region > 0)

            if len(available_coords[0]) > 0:
                # æ·»åŠ å‡ ä¸ªå°çš„è¿é€šåŒºåŸŸ
                num_islands = random.randint(1, min(5, remaining_pixels // 20))
                pixels_per_island = remaining_pixels // num_islands

                for _ in range(num_islands):
                    if remaining_pixels <= 0:
                        break

                    # éšæœºé€‰æ‹©å²›å±¿ä¸­å¿ƒ
                    idx = random.randint(0, len(available_coords[0]) - 1)
                    center_y, center_x = (
                        available_coords[0][idx],
                        available_coords[1][idx],
                    )

                    # ç”Ÿæˆå°å²›å±¿
                    island_pixels = min(pixels_per_island, remaining_pixels)
                    island_mask = self._create_small_island(
                        initial_mask,
                        intermediate_mask,
                        center_y,
                        center_x,
                        island_pixels,
                    )

                    intermediate_mask = np.maximum(intermediate_mask, island_mask)
                    remaining_pixels = target_pixels - np.sum(intermediate_mask)

        return intermediate_mask

    def _remove_from_single_edge(
        self, mask: np.ndarray, boundary_points: list, max_remove: int
    ) -> int:
        """
        ä»å•ä¸ªè¾¹ç¼˜ç§»é™¤åƒç´ ï¼Œåˆ›é€ ä¸è§„åˆ™çš„å‡¹é™·
        """
        h, w = mask.shape

        # æŒ‰è¾¹ç¼˜åˆ†ç»„
        edge_groups = {"top": [], "bottom": [], "left": [], "right": []}

        for y, x in boundary_points:
            if y < h // 4:
                edge_groups["top"].append((y, x))
            elif y > 3 * h // 4:
                edge_groups["bottom"].append((y, x))
            elif x < w // 4:
                edge_groups["left"].append((y, x))
            elif x > 3 * w // 4:
                edge_groups["right"].append((y, x))

        # é€‰æ‹©æœ‰ç‚¹çš„è¾¹ç¼˜
        available_edges = [
            edge for edge, points in edge_groups.items() if len(points) > 0
        ]
        if not available_edges:
            return 0

        selected_edge = random.choice(available_edges)
        edge_points = edge_groups[selected_edge]

        # ä»é€‰å®šè¾¹ç¼˜åˆ›å»ºä¸è§„åˆ™å‡¹é™·
        removed = 0
        start_point = random.choice(edge_points)

        # åˆ›å»ºä¸è§„åˆ™çš„ç§»é™¤åŒºåŸŸ
        to_remove = [start_point]
        processed = set()

        directions = [
            (-1, 0),
            (1, 0),
            (0, -1),
            (0, 1),
            (-1, -1),
            (-1, 1),
            (1, -1),
            (1, 1),
        ]

        while to_remove and removed < max_remove:
            y, x = to_remove.pop(0)

            if (y, x) in processed or mask[y, x] == 0:
                continue

            mask[y, x] = 0
            removed += 1
            processed.add((y, x))

            # ä¸è§„åˆ™æ‰©å±•
            for dy, dx in directions:
                ny, nx = y + dy, x + dx
                if (
                    0 <= ny < h
                    and 0 <= nx < w
                    and (ny, nx) not in processed
                    and mask[ny, nx] == 1
                    and random.random() < 0.4
                ):  # ä½æ¦‚ç‡æ‰©å±•ï¼Œåˆ›é€ ä¸è§„åˆ™æ€§
                    to_remove.append((ny, nx))

        return removed

    def _remove_random_patches(
        self, mask: np.ndarray, boundary_points: list, max_remove: int
    ) -> int:
        """
        éšæœºç§»é™¤è¾¹ç•Œè¡¥ä¸
        """
        removed = 0
        num_patches = random.randint(3, min(8, len(boundary_points) // 10))

        for _ in range(num_patches):
            if removed >= max_remove or not boundary_points:
                break

            # éšæœºé€‰æ‹©è¡¥ä¸ä¸­å¿ƒ
            patch_center = random.choice(boundary_points)
            patch_size = random.randint(3, min(15, max_remove - removed))

            # ç§»é™¤è¡¥ä¸
            patch_removed = self._remove_patch_around_point(
                mask, patch_center, patch_size
            )
            removed += patch_removed

            # æ›´æ–°è¾¹ç•Œç‚¹åˆ—è¡¨
            boundary_points = [(y, x) for y, x in boundary_points if mask[y, x] == 1]

        return removed

    def _remove_with_corner_bias(
        self, mask: np.ndarray, boundary_points: list, max_remove: int
    ) -> int:
        """
        åå‘è§’è½çš„ç§»é™¤ç­–ç•¥
        """
        h, w = mask.shape
        removed = 0

        # è®¡ç®—æ¯ä¸ªè¾¹ç•Œç‚¹åˆ°è§’è½çš„è·ç¦»
        corners = [(0, 0), (0, w - 1), (h - 1, 0), (h - 1, w - 1)]

        # ä¸ºè¾¹ç•Œç‚¹åˆ†é…æƒé‡ï¼ˆè·ç¦»è§’è½è¶Šè¿‘æƒé‡è¶Šé«˜ï¼‰
        weighted_points = []
        for y, x in boundary_points:
            min_corner_dist = min(abs(y - cy) + abs(x - cx) for cy, cx in corners)
            weight = 1.0 / (1.0 + min_corner_dist * 0.1)  # è·ç¦»è¶Šè¿‘æƒé‡è¶Šé«˜
            weighted_points.append(((y, x), weight))

        # æŒ‰æƒé‡æ’åº
        weighted_points.sort(key=lambda item: item[1], reverse=True)

        # ä¼˜å…ˆç§»é™¤æƒé‡é«˜çš„ç‚¹
        for (y, x), weight in weighted_points:
            if removed >= max_remove or mask[y, x] == 0:
                continue

            # ç§»é™¤æ¦‚ç‡ä¸æƒé‡ç›¸å…³
            if random.random() < weight:
                mask[y, x] = 0
                removed += 1

        return removed

    def _create_small_island(
        self,
        initial_mask: np.ndarray,
        current_mask: np.ndarray,
        center_y: int,
        center_x: int,
        target_pixels: int,
    ) -> np.ndarray:
        """
        åˆ›å»ºå°çš„å²›å±¿åŒºåŸŸ
        """
        h, w = initial_mask.shape
        island_mask = np.zeros_like(initial_mask)

        if (
            initial_mask[center_y, center_x] == 0
            or current_mask[center_y, center_x] == 1
        ):
            return island_mask

        # ä»ä¸­å¿ƒç‚¹æ‰©å±•
        island_mask[center_y, center_x] = 1
        current_pixels = 1

        expansion_queue = [(center_y, center_x)]
        visited = {(center_y, center_x)}

        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]

        while current_pixels < target_pixels and expansion_queue:
            y, x = expansion_queue.pop(0)

            for dy, dx in directions:
                if current_pixels >= target_pixels:
                    break

                ny, nx = y + dy, x + dx

                if (
                    0 <= ny < h
                    and 0 <= nx < w
                    and (ny, nx) not in visited
                    and initial_mask[ny, nx] == 1
                    and current_mask[ny, nx] == 0
                    and random.random() < 0.6
                ):

                    island_mask[ny, nx] = 1
                    current_pixels += 1
                    expansion_queue.append((ny, nx))
                    visited.add((ny, nx))

        return island_mask

    def _remove_patch_around_point(
        self, mask: np.ndarray, center: tuple, patch_size: int
    ) -> int:
        """
        åœ¨æŒ‡å®šç‚¹å‘¨å›´ç§»é™¤ä¸€ä¸ªè¡¥ä¸
        """
        h, w = mask.shape
        y, x = center
        removed = 0

        # åˆ›å»ºä¸è§„åˆ™çš„è¡¥ä¸å½¢çŠ¶
        max_radius = int(np.sqrt(patch_size / np.pi)) + 2

        for dy in range(-max_radius, max_radius + 1):
            for dx in range(-max_radius, max_radius + 1):
                ny, nx = y + dy, x + dx

                if (
                    0 <= ny < h
                    and 0 <= nx < w
                    and mask[ny, nx] == 1
                    and removed < patch_size
                ):

                    # ä½¿ç”¨æ¤­åœ†å½¢çŠ¶ + éšæœºæ€§
                    dist = (dy * dy) / (max_radius * max_radius) + (dx * dx) / (
                        max_radius * max_radius
                    )
                    remove_prob = max(0, 1 - dist) * random.uniform(0.5, 1.0)

                    if random.random() < remove_prob:
                        mask[ny, nx] = 0
                        removed += 1

        return removed

    def _find_boundary_simple(self, mask: np.ndarray) -> np.ndarray:
        """
        ç®€å•çš„è¾¹ç•ŒæŸ¥æ‰¾æ–¹æ³•ï¼ˆå½“scipyä¸å¯ç”¨æ—¶ï¼‰
        """
        h, w = mask.shape
        boundary = np.zeros_like(mask)

        for i in range(1, h - 1):
            for j in range(1, w - 1):
                if mask[i, j] == 1:
                    # æ£€æŸ¥8é‚»åŸŸ
                    neighbors = mask[i - 1 : i + 2, j - 1 : j + 2]
                    if np.any(neighbors == 0):
                        boundary[i, j] = 1

        return boundary

    def _ensure_connectivity_and_smoothness(
        self, mask: np.ndarray, initial_mask: np.ndarray
    ) -> np.ndarray:
        """
        ç¡®ä¿è¿é€šæ€§å’Œè¾¹ç¼˜å¹³æ»‘æ€§
        """
        if not SCIPY_AVAILABLE:
            return mask

        # ç¡®ä¿ä¸è¶…å‡ºinitial_maskçš„èŒƒå›´
        mask = np.minimum(mask, initial_mask)

        # æ ‡è®°è¿é€šåŒºåŸŸ
        labeled_array, num_features = label(mask)

        if num_features > 1:
            # ä¿ç•™æœ€å¤§çš„è¿é€šåŒºåŸŸ
            region_sizes = np.bincount(labeled_array.ravel())[1:]  # å¿½ç•¥èƒŒæ™¯
            if len(region_sizes) > 0:
                max_label = np.argmax(region_sizes) + 1
                mask = (labeled_array == max_label).astype(np.float32)

        # è½»å¾®çš„å½¢æ€å­¦å¹³æ»‘
        smoothing_iterations = random.randint(1, 3)
        for _ in range(smoothing_iterations):
            # å…ˆé—­åˆå†å¼€æ”¾ï¼Œå¹³æ»‘è¾¹ç¼˜
            kernel_size = random.choice([3, 5])
            kernel = np.ones((kernel_size, kernel_size))

            mask = binary_closing(mask, kernel)
            mask = binary_opening(mask, kernel)

            # ç¡®ä¿ä¸è¶…å‡ºinitial_mask
            mask = np.minimum(mask, initial_mask)

        return mask.astype(np.float32)

    def _generate_partial_expansion(
        self, original_mask, reference_mask, reference_data, expansion_ratio
    ):
        """
        æ”¹è¿›çš„éƒ¨åˆ†æ‰©å±•æ–¹æ³•ï¼Œå¢åŠ è¾¹ç¼˜å¤šæ ·æ€§
        """
        h, w = original_mask.shape
        original_mask = (original_mask > 0.5).astype(np.float32)
        reference_mask = (reference_data != 0).astype(np.float32)

        # è®¡ç®—å¯æ‰©å±•åŒºåŸŸ
        expansion_candidates = reference_mask - original_mask
        expansion_candidates = np.maximum(expansion_candidates, 0)
        total_expansion_candidates = np.sum(expansion_candidates)

        if total_expansion_candidates == 0 or expansion_ratio <= 0.0:
            return original_mask.copy()

        target_expansion_pixels = int(total_expansion_candidates * expansion_ratio)
        if target_expansion_pixels >= total_expansion_candidates:
            return reference_mask.copy()

        # åˆå§‹åŒ–
        final_mask = original_mask.copy()
        expanded_pixels = 0

        # ä½¿ç”¨è¾¹ç•Œç‚¹æ‰©å±•ï¼Œå¢åŠ éšæœºæ€§å’Œå¤šæ ·æ€§
        if SCIPY_AVAILABLE:
            boundary = original_mask - binary_erosion(original_mask, disk(1))
        else:
            boundary = self._find_boundary_simple(original_mask)

        boundary_coords = np.where(boundary > 0)
        boundary_points = list(zip(boundary_coords[0], boundary_coords[1]))

        if len(boundary_points) == 0:
            return original_mask.copy()

        # æ‰©å±•é˜Ÿåˆ— - éšæœºåŒ–èµ·å§‹ç‚¹
        random.shuffle(boundary_points)
        expansion_queue = boundary_points.copy()
        visited = set(boundary_points)

        # åŠ¨æ€è°ƒæ•´å‚æ•°ä»¥å¢åŠ å¤šæ ·æ€§
        interruption_prob = random.uniform(0.02, 0.08)  # éšæœºä¸­æ–­æ¦‚ç‡
        max_radius = random.randint(2, 6)  # éšæœºæœ€å¤§åŠå¾„
        smoothness_factor = random.uniform(0.7, 0.95)  # éšæœºå¹³æ»‘åº¦

        # éšæœºé€‰æ‹©æ‰©å±•ç­–ç•¥
        expansion_strategies = ["uniform", "directional", "clustered"]
        strategy = random.choice(expansion_strategies)

        while expanded_pixels < target_expansion_pixels and expansion_queue:
            y, x = expansion_queue.pop(0)

            if random.random() < interruption_prob:
                continue

            if strategy == "uniform":
                # å‡åŒ€æ‰©å±•
                directions = [
                    (-1, 0),
                    (0, -1),
                    (0, 1),
                    (1, 0),
                    (-1, -1),
                    (-1, 1),
                    (1, -1),
                    (1, 1),
                ]
                random.shuffle(directions)
            elif strategy == "directional":
                # æ–¹å‘æ€§æ‰©å±• - åå‘æŸä¸ªæ–¹å‘
                primary_dirs = [(-1, 0), (0, -1), (0, 1), (1, 0)]
                secondary_dirs = [(-1, -1), (-1, 1), (1, -1), (1, 1)]
                main_dir = random.choice(primary_dirs)
                directions = (
                    [main_dir] * 3 + primary_dirs + secondary_dirs
                )  # åå‘ä¸»æ–¹å‘
                random.shuffle(directions)
            else:  # clustered
                # èšé›†æ€§æ‰©å±• - åå‘å·²æ‰©å±•åŒºåŸŸé™„è¿‘
                directions = [(-1, 0), (0, -1), (0, 1), (1, 0)]
                random.shuffle(directions)

            radius = random.randint(1, max_radius)

            for dy, dx in directions:
                ny, nx = y + dy * radius, x + dx * radius

                if (
                    0 <= ny < h
                    and 0 <= nx < w
                    and (ny, nx) not in visited
                    and expansion_candidates[ny, nx] > 0
                ):

                    expand_prob = smoothness_factor

                    # ç­–ç•¥ç‰¹å®šçš„æ¦‚ç‡è°ƒæ•´
                    if strategy == "clustered":
                        # æ£€æŸ¥å‘¨å›´å·²æ‰©å±•åŒºåŸŸçš„å¯†åº¦
                        local_density = 0
                        for check_dy in range(-2, 3):
                            for check_dx in range(-2, 3):
                                check_y, check_x = ny + check_dy, nx + check_dx
                                if (
                                    0 <= check_y < h
                                    and 0 <= check_x < w
                                    and final_mask[check_y, check_x] == 1
                                ):
                                    local_density += 1
                        expand_prob *= (
                            local_density / 25.0
                        ) + 0.3  # åŸºäºå±€éƒ¨å¯†åº¦è°ƒæ•´æ¦‚ç‡

                    if random.random() < expand_prob:
                        final_mask[ny, nx] = 1
                        expanded_pixels += 1
                        expansion_queue.append((ny, nx))
                        visited.add((ny, nx))

                if expanded_pixels >= target_expansion_pixels:
                    break

            if expanded_pixels >= target_expansion_pixels:
                break

        # æœ€ç»ˆå¹³æ»‘å’Œè¿é€šæ€§å¤„ç†
        if SCIPY_AVAILABLE:
            # å¤šæ ·åŒ–çš„å¹³æ»‘æ“ä½œ
            smoothing_type = random.choice(["closing", "opening", "both"])
            kernel_size = random.randint(3, 7)
            kernel = np.ones((kernel_size, kernel_size))

            if smoothing_type == "closing":
                final_mask = binary_closing(final_mask, kernel)
            elif smoothing_type == "opening":
                final_mask = binary_opening(final_mask, kernel)
            else:  # both
                final_mask = binary_closing(final_mask, kernel)
                final_mask = binary_opening(final_mask, disk(kernel_size - 2))

            # ç¡®ä¿æœ€å¤§è¿é€šåŒºåŸŸ
            labeled_array, num_features = label(final_mask)
            if num_features > 0:
                region_sizes = np.bincount(labeled_array.ravel())[1:]
                max_label = np.argmax(region_sizes) + 1
                final_mask = (labeled_array == max_label).astype(np.float32)

            # ç¡®ä¿ä¸è¶…å‡ºå‚è€ƒåŒºåŸŸ
            final_mask = np.minimum(final_mask, reference_mask)

        return final_mask

    def _edge_inward_erosion(self, mask: np.ndarray, target_pixels: int) -> np.ndarray:
        """ä»è¾¹ç¼˜å‘å†…è…èš€ï¼Œç¡®ä¿è¿ç»­æ€§"""
        h, w = mask.shape
        current_mask = mask.copy()
        current_pixels = np.sum(current_mask)

        # éšæœºé€‰æ‹©1-2ä¸ªè¾¹ç¼˜
        edges = ["top", "bottom", "left", "right"]
        num_edges = random.choice([1, 2])
        selected_edges = random.sample(edges, num_edges)

        to_remove = current_pixels - target_pixels
        remove_per_edge = to_remove // num_edges

        for edge in selected_edges:
            current_mask = self._erode_from_edge_inward(
                current_mask, edge, remove_per_edge
            )
            if np.sum(current_mask) <= target_pixels:
                break

        return current_mask

    def _erode_from_edge_inward(
        self, mask: np.ndarray, edge: str, remove_pixels: int
    ) -> np.ndarray:
        """ä»æŒ‡å®šè¾¹ç¼˜å‘å†…è¿ç»­è…èš€"""
        h, w = mask.shape
        removed = 0

        # é™åˆ¶è…èš€æ·±åº¦ï¼Œç¡®ä¿ä»è¾¹ç¼˜è¿ç»­å‘å†…
        max_depth = min(h // 3, w // 3, 50)

        if edge == "top":
            for depth in range(max_depth):
                if removed >= remove_pixels:
                    break
                row = depth
                if row >= h:
                    break
                # åœ¨å½“å‰è¡Œæ‰¾åˆ°æ‰€æœ‰æœ‰æ•ˆåƒç´ 
                valid_cols = [j for j in range(w) if mask[row, j] == 1]
                if not valid_cols:
                    continue
                # ç§»é™¤è¿™ä¸€è¡Œçš„åƒç´ 
                for col in valid_cols:
                    if removed >= remove_pixels:
                        break
                    mask[row, col] = 0
                    removed += 1

        elif edge == "bottom":
            for depth in range(max_depth):
                if removed >= remove_pixels:
                    break
                row = h - 1 - depth
                if row < 0:
                    break
                valid_cols = [j for j in range(w) if mask[row, j] == 1]
                if not valid_cols:
                    continue
                for col in valid_cols:
                    if removed >= remove_pixels:
                        break
                    mask[row, col] = 0
                    removed += 1

        elif edge == "left":
            for depth in range(max_depth):
                if removed >= remove_pixels:
                    break
                col = depth
                if col >= w:
                    break
                valid_rows = [i for i in range(h) if mask[i, col] == 1]
                if not valid_rows:
                    continue
                for row in valid_rows:
                    if removed >= remove_pixels:
                        break
                    mask[row, col] = 0
                    removed += 1

        elif edge == "right":
            for depth in range(max_depth):
                if removed >= remove_pixels:
                    break
                col = w - 1 - depth
                if col < 0:
                    break
                valid_rows = [i for i in range(h) if mask[i, col] == 1]
                if not valid_rows:
                    continue
                for row in valid_rows:
                    if removed >= remove_pixels:
                        break
                    mask[row, col] = 0
                    removed += 1

        return mask

    def _gentle_uniform_erosion(
        self, mask: np.ndarray, target_pixels: int
    ) -> np.ndarray:
        """æ¸©å’Œçš„å‡åŒ€è…èš€ï¼Œç¡®ä¿ä»è¾¹ç¼˜å¼€å§‹"""
        current_mask = mask.copy()
        current_pixels = np.sum(current_mask)

        # ä½¿ç”¨å°æ­¥é•¿è…èš€
        max_iterations = 20
        iteration = 0

        while current_pixels > target_pixels and iteration < max_iterations:
            iteration += 1

            if SCIPY_AVAILABLE:
                # ä½¿ç”¨å°çš„è…èš€æ ¸
                erosion_size = 1
                kernel = disk(erosion_size)
                eroded_mask = binary_erosion(current_mask, kernel)

                # ç¡®ä¿è…èš€ç»“æœè¿é€šä¸”ä»è¾¹ç¼˜å¼€å§‹
                eroded_mask = self._ensure_edge_connectivity(eroded_mask)
            else:
                eroded_mask = self._simple_edge_erosion(current_mask)

            new_pixels = np.sum(eroded_mask)
            if new_pixels > 0 and new_pixels < current_pixels:
                current_mask = eroded_mask
                current_pixels = new_pixels
            else:
                break

        return current_mask

    def _ensure_edge_connectivity(self, mask: np.ndarray) -> np.ndarray:
        """ç¡®ä¿maskåªæœ‰ä¸è¾¹ç¼˜ç›¸è¿çš„åŒºåŸŸ"""
        if not SCIPY_AVAILABLE:
            return mask

        h, w = mask.shape

        # æ‰¾åˆ°æ‰€æœ‰è¿é€šåŒºåŸŸ
        labeled_mask, num_features = label(mask)

        if num_features <= 1:
            return mask

        # æ‰¾åˆ°ä¸è¾¹ç¼˜ç›¸è¿çš„åŒºåŸŸ
        edge_connected_regions = set()

        # æ£€æŸ¥å››ä¸ªè¾¹ç¼˜
        for i in range(h):
            if labeled_mask[i, 0] > 0:
                edge_connected_regions.add(labeled_mask[i, 0])
            if labeled_mask[i, w - 1] > 0:
                edge_connected_regions.add(labeled_mask[i, w - 1])
        for j in range(w):
            if labeled_mask[0, j] > 0:
                edge_connected_regions.add(labeled_mask[0, j])
            if labeled_mask[h - 1, j] > 0:
                edge_connected_regions.add(labeled_mask[h - 1, j])

        # åªä¿ç•™ä¸è¾¹ç¼˜ç›¸è¿çš„æœ€å¤§åŒºåŸŸ
        if edge_connected_regions:
            region_sizes = []
            for region_id in edge_connected_regions:
                size = np.sum(labeled_mask == region_id)
                region_sizes.append((size, region_id))

            # é€‰æ‹©æœ€å¤§çš„ä¸è¾¹ç¼˜ç›¸è¿çš„åŒºåŸŸ
            region_sizes.sort(reverse=True)
            largest_region_id = region_sizes[0][1]

            # åˆ›å»ºæ–°çš„maskï¼Œåªä¿ç•™æœ€å¤§çš„è¾¹ç¼˜è¿é€šåŒºåŸŸ
            new_mask = (labeled_mask == largest_region_id).astype(np.float32)
            return new_mask

        return mask

    def _simple_edge_erosion(self, mask: np.ndarray) -> np.ndarray:
        """ç®€åŒ–ç‰ˆè¾¹ç¼˜è…èš€"""
        h, w = mask.shape
        eroded_mask = mask.copy()

        # æ‰¾åˆ°è¾¹ç•Œç‚¹å¹¶è…èš€
        for i in range(1, h - 1):
            for j in range(1, w - 1):
                if mask[i, j] == 1:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºè¾¹ç•Œç‚¹
                    neighbors = mask[i - 1 : i + 2, j - 1 : j + 2]
                    if np.any(neighbors == 0):  # å¦‚æœé‚»åŸŸä¸­æœ‰0ï¼Œè¯´æ˜æ˜¯è¾¹ç•Œ
                        if random.random() < 0.3:  # 30%æ¦‚ç‡è…èš€
                            eroded_mask[i, j] = 0

        return eroded_mask

    def _generate_concentrated_mask(
        self, mask: np.ndarray, target_pixels: int, local_size: int
    ) -> np.ndarray:
        """ä¿®æ­£ç‰ˆé›†ä¸­åˆ†å¸ƒmaskç”Ÿæˆï¼šé¿å…è¾¹ç¼˜å‡ºç°ç¼ºå¤±"""
        h, w = mask.shape

        # ç¡®ä¿ç°‡ä¸­å¿ƒè¿œç¦»è¾¹ç¼˜ï¼Œå¢å¤§è¾¹è·
        margin = max(local_size // 6, 5)  # è‡³å°‘5åƒç´ è¾¹è·

        # å¦‚æœè¾¹è·å¤ªå¤§ï¼Œè°ƒæ•´
        if margin * 2 >= min(h, w):
            margin = max(2, min(h, w) // 8)

        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…éƒ¨ç©ºé—´
        inner_h = h - 2 * margin
        inner_w = w - 2 * margin

        if inner_h <= 0 or inner_w <= 0:
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿå†…éƒ¨ç©ºé—´ï¼Œåªåœ¨ä¸­å¿ƒåˆ›å»ºå°çš„ç¼ºå¤±
            center_y, center_x = h // 2, w // 2
            # åˆ›å»ºå°çš„åœ†å½¢ç¼ºå¤±
            radius = min(3, np.sqrt(target_pixels / np.pi))
            y, x = np.ogrid[:h, :w]
            dist = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)
            small_mask = dist <= radius
            mask[small_mask] = 0
            return mask

        # éšæœºç¡®å®šç°‡æ•°é‡ (1-2ä¸ªï¼Œå‡å°‘å¤æ‚æ€§)
        num_clusters = random.randint(1, 2)
        pixels_per_cluster = target_pixels // num_clusters

        for cluster_idx in range(num_clusters):
            if cluster_idx == num_clusters - 1:
                # æœ€åä¸€ä¸ªç°‡è·å¾—å‰©ä½™åƒç´ 
                cluster_pixels = target_pixels - np.sum(mask == 0)
            else:
                cluster_pixels = pixels_per_cluster

            if cluster_pixels <= 0:
                continue

            # åœ¨å†…éƒ¨åŒºåŸŸé€‰æ‹©ç°‡ä¸­å¿ƒ
            center_y = random.randint(margin, h - margin - 1)
            center_x = random.randint(margin, w - margin - 1)

            # ç”Ÿæˆç´§å‡‘çš„ç°‡ï¼Œé™åˆ¶æœ€å¤§åŠå¾„
            max_radius = min(margin - 1, local_size // 8)  # ç¡®ä¿ä¸ä¼šè§¦åŠè¾¹ç¼˜
            cluster_mask = self._generate_compact_cluster(
                (h, w), center_y, center_x, cluster_pixels, max_radius
            )

            # åº”ç”¨åˆ°ä¸»mask
            mask[cluster_mask] = 0

        return mask

    def _generate_compact_cluster(
        self,
        shape: Tuple[int, int],
        center_y: int,
        center_x: int,
        target_pixels: int,
        max_radius: int,
    ) -> np.ndarray:
        """ç”Ÿæˆç´§å‡‘çš„ç°‡ï¼Œç¡®ä¿ä¸è¶…å‡ºæŒ‡å®šåŠå¾„"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)

        # ä½¿ç”¨åŒºåŸŸç”Ÿé•¿ï¼Œä½†ä¸¥æ ¼é™åˆ¶èŒƒå›´
        if not (0 <= center_y < h and 0 <= center_x < w):
            return cluster_mask

        # ä»ä¸­å¿ƒå¼€å§‹
        cluster_mask[center_y, center_x] = True
        current_pixels = 1

        # å€™é€‰ç‚¹é˜Ÿåˆ—
        candidates = [(center_y, center_x)]
        processed = {(center_y, center_x)}

        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # åªä½¿ç”¨4è¿é€š

        while current_pixels < target_pixels and candidates:
            # ä»å€™é€‰ç‚¹ä¸­é€‰æ‹©è·ç¦»ä¸­å¿ƒæœ€è¿‘çš„
            candidates.sort(
                key=lambda p: (p[0] - center_y) ** 2 + (p[1] - center_x) ** 2
            )

            new_candidates = []

            for y, x in candidates:
                if current_pixels >= target_pixels:
                    break

                # æ·»åŠ é‚»å±…ç‚¹
                for dy, dx in directions:
                    ny, nx = y + dy, x + dx

                    if (ny, nx) in processed:
                        continue

                    # æ£€æŸ¥æ˜¯å¦åœ¨èŒƒå›´å†…
                    if not (0 <= ny < h and 0 <= nx < w):
                        processed.add((ny, nx))
                        continue

                    # æ£€æŸ¥è·ç¦»é™åˆ¶
                    dist = np.sqrt((ny - center_y) ** 2 + (nx - center_x) ** 2)
                    if dist > max_radius:
                        processed.add((ny, nx))
                        continue

                    # æ·»åŠ è¿™ä¸ªç‚¹
                    cluster_mask[ny, nx] = True
                    current_pixels += 1
                    processed.add((ny, nx))
                    new_candidates.append((ny, nx))

                    if current_pixels >= target_pixels:
                        break

            candidates = new_candidates

        return cluster_mask

    def _gentle_edge_erosion(self, mask: np.ndarray, target_pixels: int) -> np.ndarray:
        """æ¸©å’Œçš„è¾¹ç¼˜è…èš€ï¼Œå‚è€ƒåŸå§‹æ–¹æ³•ä½†å‡å°‘æ¯›åˆº"""
        current_mask = mask.copy()
        current_pixels = np.sum(current_mask)

        # ä½¿ç”¨å¤šæ¬¡å°å¹…åº¦è…èš€
        max_iterations = 15
        iteration = 0

        while current_pixels > target_pixels and iteration < max_iterations:
            iteration += 1

            if SCIPY_AVAILABLE:
                # ä½¿ç”¨è¾ƒå°çš„è…èš€æ ¸ï¼Œå‡å°‘æ¯›åˆº
                erosion_size = random.randint(1, 2)  # å‡å°è…èš€æ ¸å°ºå¯¸
                kernel = disk(erosion_size)
                eroded_mask = binary_erosion(current_mask, kernel)

                # åº”ç”¨å¹³æ»‘æ“ä½œ
                if random.random() < 0.5:
                    eroded_mask = binary_closing(eroded_mask, disk(1))
            else:
                eroded_mask = self._simple_gentle_erosion(current_mask)

            new_pixels = np.sum(eroded_mask)
            if new_pixels > 0 and new_pixels < current_pixels:
                current_mask = eroded_mask
                current_pixels = new_pixels
            else:
                break

        return current_mask

    def _selective_edge_erosion(
        self, mask: np.ndarray, target_pixels: int
    ) -> np.ndarray:
        """é€‰æ‹©æ€§è¾¹ç¼˜è…èš€"""
        h, w = mask.shape
        current_mask = mask.copy()
        current_pixels = np.sum(current_mask)
        to_remove = current_pixels - target_pixels

        if to_remove <= 0:
            return current_mask

        # éšæœºé€‰æ‹©1-2ä¸ªè¾¹ç¼˜
        edges = ["top", "bottom", "left", "right"]
        num_edges = random.choice([1, 2])
        selected_edges = random.sample(edges, num_edges)

        remove_per_edge = to_remove // num_edges

        for edge in selected_edges:
            current_mask = self._erode_edge_gradually(
                current_mask, edge, remove_per_edge
            )

        return current_mask

    def _erode_edge_gradually(
        self, mask: np.ndarray, edge: str, remove_pixels: int
    ) -> np.ndarray:
        """æ¸è¿›å¼è¾¹ç¼˜è…èš€ï¼Œé¿å…å‰§çƒˆæ¯›åˆº"""
        h, w = mask.shape
        removed = 0

        # åˆ›å»ºæ¸©å’Œçš„è…èš€æ¨¡å¼
        max_depth = min(h // 4, w // 4, 20)  # é™åˆ¶æœ€å¤§è…èš€æ·±åº¦

        for depth in range(max_depth):
            if removed >= remove_pixels:
                break

            # åœ¨å½“å‰æ·±åº¦éšæœºç§»é™¤ä¸€äº›åƒç´ 
            if edge == "top":
                row = depth
                if row < h:
                    valid_cols = [j for j in range(w) if mask[row, j] == 1]
                    remove_count = min(len(valid_cols), max(1, remove_pixels - removed))
                    if valid_cols:
                        selected_cols = random.sample(
                            valid_cols, min(remove_count, len(valid_cols))
                        )
                        for col in selected_cols:
                            mask[row, col] = 0
                            removed += 1
            elif edge == "bottom":
                row = h - 1 - depth
                if row >= 0:
                    valid_cols = [j for j in range(w) if mask[row, j] == 1]
                    remove_count = min(len(valid_cols), max(1, remove_pixels - removed))
                    if valid_cols:
                        selected_cols = random.sample(
                            valid_cols, min(remove_count, len(valid_cols))
                        )
                        for col in selected_cols:
                            mask[row, col] = 0
                            removed += 1
            elif edge == "left":
                col = depth
                if col < w:
                    valid_rows = [i for i in range(h) if mask[i, col] == 1]
                    remove_count = min(len(valid_rows), max(1, remove_pixels - removed))
                    if valid_rows:
                        selected_rows = random.sample(
                            valid_rows, min(remove_count, len(valid_rows))
                        )
                        for row in selected_rows:
                            mask[row, col] = 0
                            removed += 1
            elif edge == "right":
                col = w - 1 - depth
                if col >= 0:
                    valid_rows = [i for i in range(h) if mask[i, col] == 1]
                    remove_count = min(len(valid_rows), max(1, remove_pixels - removed))
                    if valid_rows:
                        selected_rows = random.sample(
                            valid_rows, min(remove_count, len(valid_rows))
                        )
                        for row in selected_rows:
                            mask[row, col] = 0
                            removed += 1

        return mask

    def _region_based_erosion(self, mask: np.ndarray, target_pixels: int) -> np.ndarray:
        """åŸºäºåŒºåŸŸçš„é®æŒ¡æ–¹æ³•ï¼Œå‚è€ƒç¬¬äºŒä»½ä»£ç çš„æ€è·¯"""
        h, w = mask.shape
        current_mask = mask.copy()
        current_pixels = np.sum(current_mask)
        to_remove = current_pixels - target_pixels

        if to_remove <= 0:
            return current_mask

        # é€‰æ‹©å‡ ä¸ªèµ·å§‹ç‚¹è¿›è¡ŒåŒºåŸŸç§»é™¤
        num_regions = random.randint(1, 3)
        remove_per_region = to_remove // num_regions

        for _ in range(num_regions):
            # éšæœºé€‰æ‹©èµ·å§‹ç‚¹
            valid_positions = np.where(current_mask == 1)
            if len(valid_positions[0]) == 0:
                break

            idx = random.randint(0, len(valid_positions[0]) - 1)
            start_y, start_x = valid_positions[0][idx], valid_positions[1][idx]

            # ä»èµ·å§‹ç‚¹è¿›è¡ŒåŒºåŸŸç§»é™¤
            current_mask = self._remove_region_from_point(
                current_mask, start_y, start_x, remove_per_region
            )

        return current_mask

    def _remove_region_from_point(
        self, mask: np.ndarray, start_y: int, start_x: int, target_remove: int
    ) -> np.ndarray:
        """ä»æŒ‡å®šç‚¹å¼€å§‹ç§»é™¤åŒºåŸŸ"""
        h, w = mask.shape
        removed = 0
        to_process = [(start_y, start_x)]
        processed = set()

        directions = [
            (-1, -1),
            (-1, 0),
            (-1, 1),
            (0, -1),
            (0, 1),
            (1, -1),
            (1, 0),
            (1, 1),
        ]

        while removed < target_remove and to_process:
            y, x = to_process.pop(0)

            if (y, x) in processed or removed >= target_remove:
                continue

            if 0 <= y < h and 0 <= x < w and mask[y, x] == 1:
                mask[y, x] = 0
                removed += 1
                processed.add((y, x))

                # æ·»åŠ é‚»å±…ç‚¹ï¼Œä½†æ§åˆ¶æ‰©å±•æ¦‚ç‡
                random.shuffle(directions)
                for dy, dx in directions:
                    ny, nx = y + dy, x + dx
                    if (
                        0 <= ny < h
                        and 0 <= nx < w
                        and (ny, nx) not in processed
                        and random.random() < 0.6
                    ):  # æ§åˆ¶æ‰©å±•æ¦‚ç‡
                        to_process.append((ny, nx))

                # éšæœºä¸­æ–­ï¼Œé¿å…è¿‡äºè§„åˆ™
                if random.random() < 0.15:
                    continue

        return mask

    def _simple_gentle_erosion(self, mask: np.ndarray) -> np.ndarray:
        """ç®€åŒ–ç‰ˆæ¸©å’Œè…èš€"""
        h, w = mask.shape
        eroded_mask = mask.copy()

        for i in range(1, h - 1):
            for j in range(1, w - 1):
                if mask[i, j] == 1:
                    neighbors = mask[i - 1 : i + 2, j - 1 : j + 2]
                    neighbor_count = np.sum(neighbors)

                    # æ›´æ¸©å’Œçš„è…èš€æ¡ä»¶
                    if neighbor_count >= 7 and random.random() < 0.2:
                        eroded_mask[i, j] = 0

        return eroded_mask

    def _generate_local_mask(
        self,
        local_size: int,
        missing_ratio: float,
        enable_edge_connection: bool = False,
    ) -> np.ndarray:
        """
        ç”Ÿæˆå¤šæ ·åŒ–çš„local maskï¼Œæ”¯æŒå¤šç§å½¢çŠ¶å’Œè¾¹ç•Œç›¸æ¥
        """
        shape = (local_size, local_size)
        missing_ratio_range = (missing_ratio * 0.8, missing_ratio * 1.2)  # å…è®¸Â±20%å˜åŒ–
        num_clusters_range = (1, 4)
        cluster_size_range = (8, min(18, local_size // 3))

        # 10%æ¦‚ç‡ä¸è¾¹ç•Œç›¸æ¥ï¼Œ90%æ¦‚ç‡å†…éƒ¨ç”Ÿæˆ
        edge_connection_prob = 0.1 if enable_edge_connection else 0.0
        scattered_probability = 0.2

        # å†³å®šç”Ÿæˆç­–ç•¥
        strategy_choice = np.random.random()

        if strategy_choice < edge_connection_prob:
            # 10% - è¾¹ç•Œç›¸æ¥
            return self._generate_edge_connected_mask_local(shape, missing_ratio_range)
            """elif strategy_choice < edge_connection_prob + scattered_probability * (
                1 - edge_connection_prob
            ):
                # 18% - åˆ†æ•£åˆ†å¸ƒï¼ˆåœ¨å‰©ä½™90%ä¸­çš„20%ï¼‰
                return self._generate_scattered_mask_local(shape, missing_ratio_range)"""
        else:
            # 72% - é›†ä¸­åˆ†å¸ƒï¼ˆåœ¨å‰©ä½™90%ä¸­çš„80%ï¼‰
            return self._generate_concentrated_mask_pattern_local(
                shape, missing_ratio_range, num_clusters_range, cluster_size_range
            )

    def _generate_edge_connected_mask_local(self, shape, missing_ratio_range):
        """
        ç”Ÿæˆä¸è¾¹ç•Œç›¸æ¥çš„maskï¼ˆ10%æ¦‚ç‡ä½¿ç”¨ï¼‰
        """
        h, w = shape
        mask = np.ones(shape, dtype=np.float32)

        # éšæœºç¡®å®šç¼ºå¤±æ¯”ä¾‹
        target_missing_ratio = np.random.uniform(*missing_ratio_range)
        total_pixels = h * w
        target_missing_pixels = int(total_pixels * target_missing_ratio)

        # é€‰æ‹©è¾¹ç•Œç›¸æ¥çš„ç­–ç•¥
        connection_strategy = np.random.choice(["single_edge", "corner", "multi_edge"])

        if connection_strategy == "single_edge":
            mask = self._create_single_edge_mask(mask, target_missing_pixels, h, w)
        elif connection_strategy == "corner":
            mask = self._create_corner_mask(mask, target_missing_pixels, h, w)
        else:  # multi_edge
            mask = self._create_multi_edge_mask(mask, target_missing_pixels, h, w)

        return mask

    def _create_single_edge_mask(self, mask, target_missing_pixels, h, w):
        """ä»å•ä¸ªè¾¹ç¼˜åˆ›å»ºç¼ºå¤±åŒºåŸŸ"""
        # é€‰æ‹©ä¸€ä¸ªè¾¹ç¼˜
        edge = np.random.choice(["top", "bottom", "left", "right"])

        removed = 0
        if edge == "top":
            # ä»ä¸Šè¾¹ç¼˜å¼€å§‹ï¼Œåˆ›å»ºä¸è§„åˆ™å‘ä¸‹å»¶ä¼¸
            start_x = np.random.randint(w // 4, 3 * w // 4)
            max_depth = min(h // 2, int(np.sqrt(target_missing_pixels * 2)))

            for depth in range(max_depth):
                if removed >= target_missing_pixels:
                    break
                y = depth
                # åˆ›å»ºå˜åŒ–çš„å®½åº¦
                width_at_depth = max(
                    1, int(w // 4 * (1 - depth / max_depth) + np.random.randint(-2, 3))
                )
                left_x = max(0, start_x - width_at_depth // 2)
                right_x = min(w, start_x + width_at_depth // 2)

                for x in range(left_x, right_x):
                    if removed < target_missing_pixels:
                        mask[y, x] = 0
                        removed += 1

        elif edge == "bottom":
            start_x = np.random.randint(w // 4, 3 * w // 4)
            max_depth = min(h // 2, int(np.sqrt(target_missing_pixels * 2)))

            for depth in range(max_depth):
                if removed >= target_missing_pixels:
                    break
                y = h - 1 - depth
                width_at_depth = max(
                    1, int(w // 4 * (1 - depth / max_depth) + np.random.randint(-2, 3))
                )
                left_x = max(0, start_x - width_at_depth // 2)
                right_x = min(w, start_x + width_at_depth // 2)

                for x in range(left_x, right_x):
                    if removed < target_missing_pixels:
                        mask[y, x] = 0
                        removed += 1

        elif edge == "left":
            start_y = np.random.randint(h // 4, 3 * h // 4)
            max_depth = min(w // 2, int(np.sqrt(target_missing_pixels * 2)))

            for depth in range(max_depth):
                if removed >= target_missing_pixels:
                    break
                x = depth
                height_at_depth = max(
                    1, int(h // 4 * (1 - depth / max_depth) + np.random.randint(-2, 3))
                )
                top_y = max(0, start_y - height_at_depth // 2)
                bottom_y = min(h, start_y + height_at_depth // 2)

                for y in range(top_y, bottom_y):
                    if removed < target_missing_pixels:
                        mask[y, x] = 0
                        removed += 1

        else:  # right
            start_y = np.random.randint(h // 4, 3 * h // 4)
            max_depth = min(w // 2, int(np.sqrt(target_missing_pixels * 2)))

            for depth in range(max_depth):
                if removed >= target_missing_pixels:
                    break
                x = w - 1 - depth
                height_at_depth = max(
                    1, int(h // 4 * (1 - depth / max_depth) + np.random.randint(-2, 3))
                )
                top_y = max(0, start_y - height_at_depth // 2)
                bottom_y = min(h, start_y + height_at_depth // 2)

                for y in range(top_y, bottom_y):
                    if removed < target_missing_pixels:
                        mask[y, x] = 0
                        removed += 1

        return mask

    def _create_corner_mask(self, mask, target_missing_pixels, h, w):
        """ä»è§’è½åˆ›å»ºLå½¢ç¼ºå¤±åŒºåŸŸ"""
        # é€‰æ‹©ä¸€ä¸ªè§’è½
        corner = np.random.choice(
            ["top_left", "top_right", "bottom_left", "bottom_right"]
        )

        # Lå½¢çš„ä¸¤ä¸ªè‡‚é•¿
        max_arm_length = min(h // 3, w // 3)
        arm1_length = np.random.randint(3, max_arm_length)
        arm2_length = np.random.randint(3, max_arm_length)
        arm_width = np.random.randint(2, min(5, max_arm_length // 2))

        removed = 0

        if corner == "top_left":
            # æ°´å¹³è‡‚
            for x in range(min(arm1_length, w)):
                for y in range(min(arm_width, h)):
                    if removed < target_missing_pixels:
                        mask[y, x] = 0
                        removed += 1
            # å‚ç›´è‡‚
            for y in range(min(arm2_length, h)):
                for x in range(min(arm_width, w)):
                    if (
                        removed < target_missing_pixels and mask[y, x] == 1
                    ):  # é¿å…é‡å¤è®¾ç½®
                        mask[y, x] = 0
                        removed += 1

        elif corner == "top_right":
            # æ°´å¹³è‡‚
            for x in range(max(0, w - arm1_length), w):
                for y in range(min(arm_width, h)):
                    if removed < target_missing_pixels:
                        mask[y, x] = 0
                        removed += 1
            # å‚ç›´è‡‚
            for y in range(min(arm2_length, h)):
                for x in range(max(0, w - arm_width), w):
                    if removed < target_missing_pixels and mask[y, x] == 1:
                        mask[y, x] = 0
                        removed += 1

        elif corner == "bottom_left":
            # æ°´å¹³è‡‚
            for x in range(min(arm1_length, w)):
                for y in range(max(0, h - arm_width), h):
                    if removed < target_missing_pixels:
                        mask[y, x] = 0
                        removed += 1
            # å‚ç›´è‡‚
            for y in range(max(0, h - arm2_length), h):
                for x in range(min(arm_width, w)):
                    if removed < target_missing_pixels and mask[y, x] == 1:
                        mask[y, x] = 0
                        removed += 1

        else:  # bottom_right
            # æ°´å¹³è‡‚
            for x in range(max(0, w - arm1_length), w):
                for y in range(max(0, h - arm_width), h):
                    if removed < target_missing_pixels:
                        mask[y, x] = 0
                        removed += 1
            # å‚ç›´è‡‚
            for y in range(max(0, h - arm2_length), h):
                for x in range(max(0, w - arm_width), w):
                    if removed < target_missing_pixels and mask[y, x] == 1:
                        mask[y, x] = 0
                        removed += 1

        return mask

    def _create_multi_edge_mask(self, mask, target_missing_pixels, h, w):
        """ä»å¤šä¸ªè¾¹ç¼˜åˆ›å»ºç»†é•¿è¿æ¥"""
        # åˆ›å»º2-3ä¸ªç»†é•¿çš„è¾¹ç¼˜è¿æ¥
        num_connections = np.random.randint(2, 4)
        pixels_per_connection = target_missing_pixels // num_connections

        removed = 0

        for conn_idx in range(num_connections):
            if removed >= target_missing_pixels:
                break

            # éšæœºé€‰æ‹©èµ·å§‹è¾¹ç¼˜
            start_edge = np.random.choice(["top", "bottom", "left", "right"])

            # è¿æ¥é•¿åº¦å’Œå®½åº¦
            connection_length = np.random.randint(3, min(h // 3, w // 3))
            connection_width = np.random.randint(1, 3)

            connection_removed = 0

            if start_edge == "top":
                start_x = np.random.randint(connection_width, w - connection_width)
                for depth in range(connection_length):
                    if (
                        removed >= target_missing_pixels
                        or connection_removed >= pixels_per_connection
                    ):
                        break
                    y = depth
                    if y >= h:
                        break
                    for dx in range(-connection_width, connection_width + 1):
                        x = start_x + dx
                        if 0 <= x < w and removed < target_missing_pixels:
                            mask[y, x] = 0
                            removed += 1
                            connection_removed += 1

            elif start_edge == "bottom":
                start_x = np.random.randint(connection_width, w - connection_width)
                for depth in range(connection_length):
                    if (
                        removed >= target_missing_pixels
                        or connection_removed >= pixels_per_connection
                    ):
                        break
                    y = h - 1 - depth
                    if y < 0:
                        break
                    for dx in range(-connection_width, connection_width + 1):
                        x = start_x + dx
                        if 0 <= x < w and removed < target_missing_pixels:
                            mask[y, x] = 0
                            removed += 1
                            connection_removed += 1

            elif start_edge == "left":
                start_y = np.random.randint(connection_width, h - connection_width)
                for depth in range(connection_length):
                    if (
                        removed >= target_missing_pixels
                        or connection_removed >= pixels_per_connection
                    ):
                        break
                    x = depth
                    if x >= w:
                        break
                    for dy in range(-connection_width, connection_width + 1):
                        y = start_y + dy
                        if 0 <= y < h and removed < target_missing_pixels:
                            mask[y, x] = 0
                            removed += 1
                            connection_removed += 1

            else:  # right
                start_y = np.random.randint(connection_width, h - connection_width)
                for depth in range(connection_length):
                    if (
                        removed >= target_missing_pixels
                        or connection_removed >= pixels_per_connection
                    ):
                        break
                    x = w - 1 - depth
                    if x < 0:
                        break
                    for dy in range(-connection_width, connection_width + 1):
                        y = start_y + dy
                        if 0 <= y < h and removed < target_missing_pixels:
                            mask[y, x] = 0
                            removed += 1
                            connection_removed += 1

        return mask

    def _generate_concentrated_mask_pattern_local(
        self, shape, missing_ratio_range, num_clusters_range, cluster_size_range
    ):
        """ç”Ÿæˆé›†ä¸­åˆ†å¸ƒçš„maskæ¨¡å¼ - å¢å¼ºå¤šæ ·æ€§"""
        h, w = shape
        mask = np.ones(shape, dtype=np.float32)

        # éšæœºç¡®å®šç¼ºå¤±æ¯”ä¾‹å’Œç°‡æ•°é‡
        target_missing_ratio = np.random.uniform(*missing_ratio_range)
        # print(f"target missing ratio: {target_missing_ratio}")
        num_clusters = 1  # np.random.randint(*num_clusters_range)
        # print(f"num clusters: {num_clusters}")

        total_pixels = h * w
        target_missing_pixels = int(total_pixels * target_missing_ratio)
        # print(f"target_missing_pixels: {target_missing_pixels}")

        # è¿½è¸ªå®é™…ç”Ÿæˆçš„ç¼ºå¤±åƒç´ 
        actual_missing_pixels = 0

        for cluster_idx in range(num_clusters):
            # ä¸ºæ¯ä¸ªç°‡åˆ†é…åƒç´ æ•°
            remaining_pixels = target_missing_pixels - actual_missing_pixels
            if cluster_idx == num_clusters - 1:
                # æœ€åä¸€ä¸ªç°‡è·å¾—å‰©ä½™æ‰€æœ‰åƒç´ 
                cluster_pixels = remaining_pixels
            else:
                # å‰é¢çš„ç°‡å¹³å‡åˆ†é…å‰©ä½™åƒç´ 
                cluster_pixels = remaining_pixels // (num_clusters - cluster_idx)

            if cluster_pixels <= 0:
                continue

            # print(f"Cluster {cluster_idx}: target pixels = {cluster_pixels}")

            # é€‰æ‹©ç°‡ä¸­å¿ƒï¼Œé¿å…å¤ªé è¿‘è¾¹ç¼˜
            margin = max(3, min(h, w) // 3)
            center_x = np.random.randint(margin, h - margin)
            center_y = np.random.randint(margin, w - margin)

            # ç”Ÿæˆå¤šæ ·åŒ–çš„é›†ä¸­ç°‡
            cluster_mask = self._generate_diverse_cluster_local(
                shape, center_x, center_y, cluster_pixels
            )

            # æ‰“å°ç”Ÿæˆçš„ç°‡å¤§å°ï¼ˆå½¢æ€å­¦æ“ä½œå‰ï¼‰
            cluster_size_before = np.sum(cluster_mask)
            # print(f"  Cluster size before morphology: {cluster_size_before}")

            # åº”ç”¨å½¢æ€å­¦æ“ä½œç¡®ä¿é›†ä¸­æ€§ï¼ˆå¯é€‰ï¼‰
            if True:  # æ·»åŠ ä¸€ä¸ªæ§åˆ¶å¼€å…³
                cluster_mask = self._apply_morphological_operations_local(cluster_mask)
                cluster_size_after = np.sum(cluster_mask)
                # print(f"  Cluster size after morphology: {cluster_size_after}")

            # åº”ç”¨åˆ°ä¸»mask
            mask[cluster_mask] = 0
            actual_missing_pixels += np.sum(cluster_mask)

        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        final_missing_ratio = 1 - np.mean(mask)
        """print(
            f"Final missing ratio: {final_missing_ratio:.4f} (target was {target_missing_ratio:.4f})"
        )
        print(
            f"Final missing pixels: {int(total_pixels * final_missing_ratio)} (target was {target_missing_pixels})"
        )"""

        return mask

    def _apply_morphological_operations_local(self, mask):
        """åº”ç”¨å½¢æ€å­¦æ“ä½œæ¥è¿›ä¸€æ­¥é›†ä¸­mask - ä¿å®ˆç‰ˆæœ¬"""
        if not SCIPY_AVAILABLE:
            return mask

        # ä½¿ç”¨æ›´å°çš„æ ¸æ¥å‡å°‘åŒºåŸŸæŸå¤±
        kernel_close = disk(1)  # ä» 2 æ”¹ä¸º 1
        mask_closed = binary_dilation(mask, kernel_close)
        mask_closed = binary_erosion(mask_closed, kernel_close)

        # å¯é€‰ï¼šä¿ç•™å¤šä¸ªè¾ƒå¤§çš„è¿é€šåŒºåŸŸè€Œä¸æ˜¯åªä¿ç•™æœ€å¤§çš„
        labeled_mask, num_labels = label(mask_closed)
        if num_labels > 1:
            # è®¡ç®—æ‰€æœ‰åŒºåŸŸçš„å¤§å°
            region_sizes = []
            for i in range(1, num_labels + 1):
                size = np.sum(labeled_mask == i)
                region_sizes.append((size, i))

            region_sizes.sort(reverse=True)

            # ä¿ç•™å åŸå§‹åŒºåŸŸ 80% ä»¥ä¸Šçš„æ‰€æœ‰å¤§åŒºåŸŸ
            original_size = np.sum(mask)
            new_mask = np.zeros_like(mask_closed)
            accumulated_size = 0

            for size, region_id in region_sizes:
                new_mask[labeled_mask == region_id] = True
                accumulated_size += size
                # å¦‚æœå·²ç»æ¢å¤äº†80%çš„åŸå§‹åŒºåŸŸï¼Œåœæ­¢
                if accumulated_size >= 0.8 * original_size:
                    break

            mask_closed = new_mask

        return mask_closed

    def _generate_diverse_cluster_local(self, shape, center_x, center_y, target_pixels):
        """ç”Ÿæˆå¤šæ ·åŒ–å½¢çŠ¶çš„é›†ä¸­ç°‡ - ç¡®ä¿ç”Ÿæˆè¶³å¤Ÿå¤§çš„åŒºåŸŸ"""
        # æ ¹æ®ç›®æ ‡åƒç´ æ•°ä¼°ç®—éœ€è¦çš„åŠå¾„
        estimated_radius = int(np.sqrt(target_pixels / np.pi))

        # ç¡®ä¿åŠå¾„ä¸ä¼šå¤ªå°
        actual_radius = max(estimated_radius, 10)

        print(
            f"  Generating cluster with estimated radius: {estimated_radius}, actual: {actual_radius}"
        )

        # éšæœºé€‰æ‹©å½¢çŠ¶ç±»å‹
        # æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨æ¦‚ç‡æƒé‡
        shape_types = ["growth", "ellipse", "multi_circle", "polygon"]
        shape_weights = [0.5, 0.2, 0.2, 0.1]  # å¯¹åº”æ¯ä¸ªshape_typeçš„æ¦‚ç‡

        shape_type = np.random.choice(shape_types, p=shape_weights)
        print(f"  Using shape type: {shape_type}")

        if shape_type == "ellipse":
            return self._generate_ellipse_cluster_local_v2(
                shape, center_x, center_y, actual_radius, target_pixels
            )
        elif shape_type == "polygon":
            return self._generate_polygon_cluster_local(
                shape, center_x, center_y, actual_radius, target_pixels
            )
        elif shape_type == "growth":
            return self._generate_growth_cluster_local_v2(
                shape, center_x, center_y, actual_radius, target_pixels
            )
        else:  # multi_circle
            return self._generate_multi_circle_cluster_local(
                shape, center_x, center_y, actual_radius, target_pixels
            )

    def _generate_ellipse_cluster_local_v2(
        self, shape, center_x, center_y, radius, target_pixels
    ):
        """ç”Ÿæˆæ¤­åœ†å½¢ç°‡ - æ”¹è¿›ç‰ˆæœ¬"""
        h, w = shape
        mask = np.zeros(shape, dtype=bool)

        # æ ¹æ®ç›®æ ‡åƒç´ æ•°è°ƒæ•´åŠå¾„
        # æ¤­åœ†é¢ç§¯ = Ï€ * a * b
        aspect_ratio = np.random.uniform(0.5, 2.0)
        a = radius * np.sqrt(aspect_ratio)
        b = radius / np.sqrt(aspect_ratio)

        # è¿­ä»£è°ƒæ•´åŠå¾„ç›´åˆ°æ¥è¿‘ç›®æ ‡åƒç´ æ•°
        scale = 1.0
        for _ in range(10):  # æœ€å¤šè¿­ä»£10æ¬¡
            temp_mask = np.zeros(shape, dtype=bool)

            # åˆ›å»ºåæ ‡ç½‘æ ¼
            y, x = np.ogrid[:h, :w]

            # æ¤­åœ†æ–¹ç¨‹
            ellipse = ((x - center_y) / (b * scale)) ** 2 + (
                (y - center_x) / (a * scale)
            ) ** 2 <= 1
            temp_mask[ellipse] = True

            current_pixels = np.sum(temp_mask)
            if current_pixels >= target_pixels * 0.9:  # å…è®¸10%çš„è¯¯å·®
                mask = temp_mask
                break

            # è°ƒæ•´ç¼©æ”¾å› å­
            scale *= np.sqrt(target_pixels / current_pixels)

        return mask

    def _generate_growth_cluster_local_v2(
        self, shape, center_x, center_y, radius, target_pixels
    ):
        """ç”Ÿæˆç”Ÿé•¿å‹ç°‡ - ä½¿ç”¨åŒºåŸŸç”Ÿé•¿ç®—æ³•"""
        h, w = shape
        mask = np.zeros(shape, dtype=bool)

        # åˆå§‹åŒ–ç§å­ç‚¹
        mask[center_x, center_y] = True

        # å¾…å¤„ç†çš„è¾¹ç•Œç‚¹
        boundary = [(center_x, center_y)]
        directions = [
            (-1, 0),
            (1, 0),
            (0, -1),
            (0, 1),
            (-1, -1),
            (-1, 1),
            (1, -1),
            (1, 1),
        ]

        current_pixels = 1

        while boundary and current_pixels < target_pixels:
            # éšæœºé€‰æ‹©ä¸€ä¸ªè¾¹ç•Œç‚¹
            idx = np.random.randint(len(boundary))
            x, y = boundary.pop(idx)

            # éšæœºæ‰“ä¹±æ–¹å‘
            np.random.shuffle(directions)

            for dx, dy in directions:
                nx, ny = x + dx, y + dy

                # æ£€æŸ¥è¾¹ç•Œå’Œæ˜¯å¦å·²è®¿é—®
                if 0 <= nx < h and 0 <= ny < w and not mask[nx, ny]:
                    # æ ¹æ®è·ç¦»ä¸­å¿ƒçš„è·ç¦»å†³å®šç”Ÿé•¿æ¦‚ç‡
                    dist = np.sqrt((nx - center_x) ** 2 + (ny - center_y) ** 2)

                    # è·ç¦»è¶Šè¿œï¼Œç”Ÿé•¿æ¦‚ç‡è¶Šä½
                    growth_prob = max(0.1, 1.0 - dist / (2 * radius))

                    if np.random.random() < growth_prob:
                        mask[nx, ny] = True
                        boundary.append((nx, ny))
                        current_pixels += 1

                        if current_pixels >= target_pixels:
                            break

        return mask

    def _generate_polygon_cluster_local(
        self, shape, center_x, center_y, max_radius, target_pixels
    ):
        """ç”Ÿæˆå¤šè¾¹å½¢ç¼ºå¤±åŒºåŸŸ"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)

        # ç”Ÿæˆä¸è§„åˆ™å¤šè¾¹å½¢çš„é¡¶ç‚¹
        num_vertices = np.random.randint(5, 9)  # 5-8ä¸ªé¡¶ç‚¹
        angles = np.linspace(0, 2 * np.pi, num_vertices, endpoint=False)

        # æ·»åŠ éšæœºåç§»åˆ›é€ ä¸è§„åˆ™æ€§
        angles += np.random.uniform(-0.4, 0.4, num_vertices)

        # è®¡ç®—å¹³å‡åŠå¾„
        avg_radius = min(max_radius * 0.7, np.sqrt(target_pixels / np.pi))

        # ç”Ÿæˆé¡¶ç‚¹
        vertices = []
        for angle in angles:
            # éšæœºåŒ–åŠå¾„åˆ›é€ ä¸è§„åˆ™å½¢çŠ¶
            radius = avg_radius * np.random.uniform(0.5, 1.5)
            radius = min(radius, max_radius)

            x = center_x + radius * np.cos(angle)
            y = center_y + radius * np.sin(angle)

            # ç¡®ä¿åœ¨èŒƒå›´å†…
            x = max(0, min(h - 1, x))
            y = max(0, min(w - 1, y))

            vertices.append((int(x), int(y)))

        # å¡«å……å¤šè¾¹å½¢å†…éƒ¨
        if len(vertices) >= 3:
            cluster_mask = self._fill_polygon_local(cluster_mask, vertices)

        return cluster_mask

    def _fill_polygon_local(self, mask, vertices):
        """ç®€å•çš„å¤šè¾¹å½¢å¡«å……ç®—æ³•"""
        h, w = mask.shape

        if len(vertices) < 3:
            return mask

        # æ‰¾åˆ°è¾¹ç•Œæ¡†
        min_x = max(0, min(v[0] for v in vertices))
        max_x = min(h - 1, max(v[0] for v in vertices))
        min_y = max(0, min(v[1] for v in vertices))
        max_y = min(w - 1, max(v[1] for v in vertices))

        # ä½¿ç”¨ç‚¹åœ¨å¤šè¾¹å½¢å†…æµ‹è¯•
        for x in range(min_x, max_x + 1):
            for y in range(min_y, max_y + 1):
                if self._point_in_polygon(x, y, vertices):
                    mask[x, y] = True

        return mask

    def _point_in_polygon(self, x, y, vertices):
        """ç‚¹åœ¨å¤šè¾¹å½¢å†…æµ‹è¯•ï¼ˆå°„çº¿æ³•ï¼‰"""
        n = len(vertices)
        inside = False

        p1x, p1y = vertices[0]
        for i in range(1, n + 1):
            p2x, p2y = vertices[i % n]
            if y > min(p1y, p2y):
                if y <= max(p1y, p2y):
                    if x <= max(p1x, p2x):
                        if p1y != p2y:
                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x
                        if p1x == p2x or x <= xinters:
                            inside = not inside
            p1x, p1y = p2x, p2y

        return inside

    def _generate_multi_circle_cluster_local(
        self, shape, center_x, center_y, max_radius, target_pixels
    ):
        """ç”Ÿæˆå¤šä¸ªé‡å åœ†å½¢ç»„æˆçš„ç°‡"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)

        # ç¡®å®šåœ†çš„æ•°é‡
        num_circles = np.random.randint(2, 5)
        pixels_per_circle = target_pixels // num_circles

        for i in range(num_circles):
            # åœ¨ä¸»ä¸­å¿ƒé™„è¿‘é€‰æ‹©å­åœ†å¿ƒ
            offset_range = min(max_radius // 3, 6)
            sub_center_x = center_x + np.random.randint(-offset_range, offset_range + 1)
            sub_center_y = center_y + np.random.randint(-offset_range, offset_range + 1)

            # ç¡®ä¿åœ¨èŒƒå›´å†…
            sub_center_x = max(0, min(h - 1, sub_center_x))
            sub_center_y = max(0, min(w - 1, sub_center_y))

            # è®¡ç®—å­åœ†åŠå¾„
            if i == num_circles - 1:
                # æœ€åä¸€ä¸ªåœ†è°ƒæ•´ä»¥è¾¾åˆ°ç›®æ ‡åƒç´ æ•°
                remaining_pixels = target_pixels - np.sum(cluster_mask)
                radius = (
                    np.sqrt(remaining_pixels / np.pi) if remaining_pixels > 0 else 0
                )
            else:
                radius = np.sqrt(pixels_per_circle / np.pi)

            radius = min(radius, max_radius * 0.6)

            if radius > 0:
                # ç”Ÿæˆå­åœ†
                y, x = np.ogrid[:h, :w]
                dist = np.sqrt((x - sub_center_y) ** 2 + (y - sub_center_x) ** 2)
                sub_circle = dist <= radius

                cluster_mask = cluster_mask | sub_circle

        return cluster_mask

    def _generate_growth_cluster_local(
        self, shape, center_x, center_y, max_radius, target_pixels
    ):
        """ä½¿ç”¨åŒºåŸŸç”Ÿé•¿æ–¹æ³•ç”Ÿæˆé›†ä¸­çš„ç¼ºå¤±åŒºåŸŸ - å¢åŠ ä¸è§„åˆ™æ€§"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)

        # ä»ä¸­å¿ƒç‚¹å¼€å§‹
        if 0 <= center_x < h and 0 <= center_y < w:
            cluster_mask[center_x, center_y] = True
            current_pixels = 1
        else:
            return cluster_mask

        # å®šä¹‰é‚»åŸŸ(8è¿é€š)
        neighbors = [
            (-1, -1),
            (-1, 0),
            (-1, 1),
            (0, -1),
            (0, 1),
            (1, -1),
            (1, 0),
            (1, 1),
        ]

        # åŒºåŸŸç”Ÿé•¿å‚æ•°
        max_iterations = target_pixels * 2
        iteration = 0

        # å¢åŠ ç”Ÿé•¿çš„éšæœºæ€§å’Œæ–¹å‘åå¥½
        growth_randomness = np.random.uniform(0.3, 0.8)
        directional_bias = np.random.choice(
            [None, "horizontal", "vertical", "diagonal"]
        )

        while current_pixels < target_pixels and iteration < max_iterations:
            iteration += 1

            # æ‰¾åˆ°å½“å‰åŒºåŸŸçš„è¾¹ç•Œç‚¹
            boundary_points = []

            for i in range(
                max(0, center_x - max_radius), min(h, center_x + max_radius + 1)
            ):
                for j in range(
                    max(0, center_y - max_radius), min(w, center_y + max_radius + 1)
                ):
                    if cluster_mask[i, j]:
                        # æ£€æŸ¥8é‚»åŸŸ
                        for di, dj in neighbors:
                            ni, nj = i + di, j + dj
                            if 0 <= ni < h and 0 <= nj < w and not cluster_mask[ni, nj]:

                                # è®¡ç®—è·ç¦»ä¸­å¿ƒçš„è·ç¦»
                                dist = np.sqrt(
                                    (ni - center_x) ** 2 + (nj - center_y) ** 2
                                )
                                if dist <= max_radius:
                                    # æ·»åŠ æ–¹å‘åå¥½æƒé‡
                                    bias_weight = 1.0
                                    if directional_bias == "horizontal":
                                        bias_weight = 1.0 + 0.5 * (
                                            1.0 - abs(di) / max(abs(di) + abs(dj), 1)
                                        )
                                    elif directional_bias == "vertical":
                                        bias_weight = 1.0 + 0.5 * (
                                            1.0 - abs(dj) / max(abs(di) + abs(dj), 1)
                                        )
                                    elif directional_bias == "diagonal":
                                        bias_weight = 1.0 + 0.3 * (abs(di) == abs(dj))

                                    boundary_points.append((ni, nj, dist, bias_weight))

            if not boundary_points:
                break

            # æ ¹æ®è·ç¦»å’Œåå¥½æƒé‡æ’åº
            boundary_points.sort(key=lambda x: x[2] / x[3])

            # è®¡ç®—æœ¬æ¬¡è¦æ·»åŠ çš„ç‚¹æ•°ï¼ˆå¢åŠ éšæœºæ€§ï¼‰
            remaining_pixels = target_pixels - current_pixels
            base_add = max(1, remaining_pixels // np.random.randint(3, 8))
            num_to_add = min(len(boundary_points), base_add, remaining_pixels)

            # æ ¹æ®éšæœºæ€§å†³å®šé€‰æ‹©ç­–ç•¥
            if np.random.random() < growth_randomness:
                # éšæœºé€‰æ‹©
                selected_indices = np.random.choice(
                    min(len(boundary_points), num_to_add * 2),
                    size=num_to_add,
                    replace=False,
                )
                selected_points = [boundary_points[i] for i in selected_indices]
            else:
                # é€‰æ‹©æœ€ä¼˜çš„ç‚¹
                selected_points = boundary_points[:num_to_add]

            # æ·»åŠ é€‰ä¸­çš„ç‚¹
            for ni, nj, _, _ in selected_points:
                if current_pixels >= target_pixels:
                    break
                cluster_mask[ni, nj] = True
                current_pixels += 1

        return cluster_mask

    def _generate_scattered_mask(
        self, mask: np.ndarray, target_pixels: int, local_size: int
    ) -> np.ndarray:
        """
        ç”Ÿæˆåˆ†æ•£åˆ†å¸ƒçš„maskï¼Œç¡®ä¿æ‰€æœ‰å°ç°‡éƒ½è¿œç¦»è¾¹ç¼˜
        """
        h, w = mask.shape

        # å¤šä¸ªå°ç°‡åˆ†æ•£åˆ†å¸ƒ
        num_small_clusters = random.randint(3, min(8, max(3, local_size // 8)))
        pixels_per_cluster = max(1, target_pixels // num_small_clusters)

        # å¢å¤§è¾¹è·ï¼Œç¡®ä¿å°ç°‡ä¸é è¿‘è¾¹ç¼˜
        margin = max(3, local_size // 6)  # æ›´å¤§çš„è¾¹è·

        for cluster_idx in range(num_small_clusters):
            if np.sum(mask == 0) >= target_pixels:
                break

            cluster_pixels = min(pixels_per_cluster, target_pixels - np.sum(mask == 0))
            if cluster_pixels <= 0:
                continue

            # éšæœºé€‰æ‹©å°ç°‡ä¸­å¿ƒï¼Œç¡®ä¿è¿œç¦»è¾¹ç¼˜
            if margin * 2 >= min(h, w):
                center_y = h // 2
                center_x = w // 2
            else:
                center_y = random.randint(margin, h - margin - 1)
                center_x = random.randint(margin, w - margin - 1)

            # ç”Ÿæˆå°çš„åœ†å½¢ç°‡
            if cluster_pixels <= 4:
                # å¾ˆå°çš„ç°‡ï¼šç›´æ¥è®¾ç½®å‡ ä¸ªç‚¹
                mask[center_y, center_x] = 0
                directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]
                for i, (dy, dx) in enumerate(directions):
                    if i >= cluster_pixels - 1:
                        break
                    ny, nx = center_y + dy, center_x + dx
                    if margin <= ny < h - margin and margin <= nx < w - margin:
                        mask[ny, nx] = 0
            else:
                # è¾ƒå¤§ç°‡ï¼šå°åœ†å½¢ï¼Œä½†ç¡®ä¿ä¸è¶…å‡ºè¾¹ç•Œ
                radius = min(np.sqrt(cluster_pixels / np.pi), margin - 1)
                y, x = np.ogrid[:h, :w]
                dist = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)
                small_cluster = dist <= radius

                # åªåœ¨å†…éƒ¨åŒºåŸŸåº”ç”¨
                for i in range(h):
                    for j in range(w):
                        if (
                            small_cluster[i, j]
                            and margin <= i < h - margin
                            and margin <= j < w - margin
                        ):
                            mask[i, j] = 0

        return mask

    def _generate_compact_cluster(
        self,
        shape: Tuple[int, int],
        center_y: int,
        center_x: int,
        target_pixels: int,
        max_radius: int,
    ) -> np.ndarray:
        """
        ç”Ÿæˆç´§å‡‘çš„ç°‡ï¼Œä¸¥æ ¼é™åˆ¶åœ¨æŒ‡å®šåŠå¾„å†…ï¼Œç¡®ä¿ä¸è§¦åŠè¾¹ç¼˜
        """
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)

        # éªŒè¯ä¸­å¿ƒç‚¹æ˜¯å¦æœ‰æ•ˆ
        if not (0 <= center_y < h and 0 <= center_x < w):
            return cluster_mask

        # ç¡®ä¿æœ€å¤§åŠå¾„ä¸ä¼šå¯¼è‡´è§¦åŠè¾¹ç¼˜
        margin = 1  # è‡³å°‘3åƒç´ çš„è¾¹è·
        max_allowed_radius = min(
            center_y - margin,
            center_x - margin,
            h - center_y - margin - 1,
            w - center_x - margin - 1,
            max_radius,
        )

        if max_allowed_radius <= 0:
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿç©ºé—´ï¼Œåªè®¾ç½®ä¸­å¿ƒç‚¹
            cluster_mask[center_y, center_x] = True
            return cluster_mask

        # ä»ä¸­å¿ƒå¼€å§‹
        cluster_mask[center_y, center_x] = True
        current_pixels = 1

        # å€™é€‰ç‚¹é˜Ÿåˆ—
        candidates = [(center_y, center_x)]
        processed = {(center_y, center_x)}

        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # åªä½¿ç”¨4è¿é€š

        while current_pixels < target_pixels and candidates:
            # ä»å€™é€‰ç‚¹ä¸­é€‰æ‹©è·ç¦»ä¸­å¿ƒæœ€è¿‘çš„
            candidates.sort(
                key=lambda p: (p[0] - center_y) ** 2 + (p[1] - center_x) ** 2
            )

            new_candidates = []

            for y, x in candidates:
                if current_pixels >= target_pixels:
                    break

                # æ·»åŠ é‚»å±…ç‚¹
                for dy, dx in directions:
                    ny, nx = y + dy, x + dx

                    if (ny, nx) in processed:
                        continue

                    # æ£€æŸ¥æ˜¯å¦åœ¨èŒƒå›´å†…
                    if not (0 <= ny < h and 0 <= nx < w):
                        processed.add((ny, nx))
                        continue

                    # æ£€æŸ¥è·ç¦»é™åˆ¶ï¼Œç¡®ä¿ä¸è¶…å‡ºå…è®¸çš„åŠå¾„
                    dist = np.sqrt((ny - center_y) ** 2 + (nx - center_x) ** 2)
                    if dist > max_allowed_radius:
                        processed.add((ny, nx))
                        continue

                    # æ·»åŠ è¿™ä¸ªç‚¹
                    cluster_mask[ny, nx] = True
                    current_pixels += 1
                    processed.add((ny, nx))
                    new_candidates.append((ny, nx))

                    if current_pixels >= target_pixels:
                        break

            candidates = new_candidates

        return cluster_mask

    def _generate_cluster_shape(
        self, shape: Tuple[int, int], center_y: int, center_x: int, target_pixels: int
    ) -> np.ndarray:
        """ç”Ÿæˆå¤šæ ·åŒ–çš„ç°‡å½¢çŠ¶"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)

        # éšæœºé€‰æ‹©ç°‡å½¢çŠ¶
        shape_type = random.choice(["ellipse", "growth", "irregular"])

        if shape_type == "ellipse":
            cluster_mask = self._generate_ellipse_cluster_new(
                shape, center_y, center_x, target_pixels
            )
        elif shape_type == "growth":
            cluster_mask = self._generate_growth_cluster_new(
                shape, center_y, center_x, target_pixels
            )
        else:  # irregular
            cluster_mask = self._generate_irregular_cluster(
                shape, center_y, center_x, target_pixels
            )

        return cluster_mask

    def _generate_ellipse_cluster_new(
        self, shape: Tuple[int, int], center_y: int, center_x: int, target_pixels: int
    ) -> np.ndarray:
        """ç”Ÿæˆæ¤­åœ†å½¢ç°‡"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)

        if not SCIPY_AVAILABLE:
            # ç®€åŒ–ç‰ˆï¼šåœ†å½¢
            radius = np.sqrt(target_pixels / np.pi)
            y, x = np.ogrid[:h, :w]
            dist = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)
            cluster_mask = dist <= radius
            return cluster_mask

        # ä¼°ç®—æ¤­åœ†å‚æ•°
        aspect_ratio = random.uniform(0.5, 1.0)  # æ¤­åœ†é•¿çŸ­è½´æ¯”ä¾‹

        # è®¡ç®—åŠé•¿è½´å’ŒåŠçŸ­è½´
        area = target_pixels
        semi_major = np.sqrt(area / (np.pi * aspect_ratio))
        semi_minor = semi_major * aspect_ratio

        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        max_radius = min(h // 3, w // 3)
        semi_major = min(semi_major, max_radius)
        semi_minor = min(semi_minor, max_radius)

        try:
            # ç”Ÿæˆæ¤­åœ†
            rr, cc = ellipse(center_y, center_x, semi_major, semi_minor, shape=shape)
            cluster_mask[rr, cc] = True
        except:
            # å›é€€åˆ°åœ†å½¢
            radius = min(semi_major, semi_minor)
            y, x = np.ogrid[:h, :w]
            dist = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)
            cluster_mask = dist <= radius

        return cluster_mask

    def _generate_growth_cluster_new(
        self, shape: Tuple[int, int], center_y: int, center_x: int, target_pixels: int
    ) -> np.ndarray:
        """ä½¿ç”¨åŒºåŸŸç”Ÿé•¿ç”Ÿæˆç°‡"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)

        if not (0 <= center_y < h and 0 <= center_x < w):
            return cluster_mask

        # ä»ä¸­å¿ƒç‚¹å¼€å§‹ç”Ÿé•¿
        cluster_mask[center_y, center_x] = True
        current_pixels = 1

        # 8è¿é€šé‚»åŸŸ
        directions = [
            (-1, -1),
            (-1, 0),
            (-1, 1),
            (0, -1),
            (0, 1),
            (1, -1),
            (1, 0),
            (1, 1),
        ]

        # åŒºåŸŸç”Ÿé•¿
        max_iterations = target_pixels * 3
        iteration = 0

        while current_pixels < target_pixels and iteration < max_iterations:
            iteration += 1

            # æ‰¾åˆ°è¾¹ç•Œç‚¹
            boundary_points = []
            for y in range(h):
                for x in range(w):
                    if cluster_mask[y, x]:
                        for dy, dx in directions:
                            ny, nx = y + dy, x + dx
                            if 0 <= ny < h and 0 <= nx < w and not cluster_mask[ny, nx]:
                                dist = np.sqrt(
                                    (ny - center_y) ** 2 + (nx - center_x) ** 2
                                )
                                boundary_points.append((ny, nx, dist))

            if not boundary_points:
                break

            # å»é‡å¹¶æŒ‰è·ç¦»æ’åº
            boundary_points = list(set(boundary_points))
            boundary_points.sort(key=lambda x: x[2])

            # é€‰æ‹©è¦æ·»åŠ çš„ç‚¹
            remaining = target_pixels - current_pixels
            num_to_add = min(len(boundary_points), max(1, remaining // 3), remaining)

            selected_points = boundary_points[:num_to_add]

            for ny, nx, _ in selected_points:
                if current_pixels >= target_pixels:
                    break
                cluster_mask[ny, nx] = True
                current_pixels += 1

        return cluster_mask

    def _generate_irregular_cluster(
        self, shape: Tuple[int, int], center_y: int, center_x: int, target_pixels: int
    ) -> np.ndarray:
        """ç”Ÿæˆä¸è§„åˆ™å½¢çŠ¶çš„ç°‡"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)

        # ä½¿ç”¨å¤šä¸ªå°åœ†çš„è”åˆæ¥åˆ›å»ºä¸è§„åˆ™å½¢çŠ¶
        num_circles = random.randint(2, 4)
        pixels_per_circle = target_pixels // num_circles

        for i in range(num_circles):
            # åœ¨ä¸­å¿ƒç‚¹é™„è¿‘é€‰æ‹©å­åœ†å¿ƒ
            offset_range = min(h // 4, w // 4, 8)
            sub_center_y = center_y + random.randint(-offset_range, offset_range)
            sub_center_x = center_x + random.randint(-offset_range, offset_range)

            # ç¡®ä¿åœ¨èŒƒå›´å†…
            sub_center_y = max(0, min(h - 1, sub_center_y))
            sub_center_x = max(0, min(w - 1, sub_center_x))

            # è®¡ç®—å­åœ†åŠå¾„
            radius = np.sqrt(pixels_per_circle / np.pi)

            # ç”Ÿæˆå­åœ†
            y, x = np.ogrid[:h, :w]
            dist = np.sqrt((x - sub_center_x) ** 2 + (y - sub_center_y) ** 2)
            sub_circle = dist <= radius

            cluster_mask = cluster_mask | sub_circle

        return cluster_mask

    def _extract_local_region(
        self, global_data: np.ndarray, global_mask: np.ndarray, local_size: int
    ) -> Tuple[np.ndarray, np.ndarray, Tuple[int, int]]:
        """
        ä»globalæ•°æ®ä¸­æå–localåŒºåŸŸ - ä¿®æ­£ç‰ˆæœ¬
        ç¡®ä¿æå–çš„localåŒºåŸŸä¸­global_mask=1çš„éƒ¨åˆ†å ç»å¤§å¤šæ•°
        """
        h, w = global_data.shape
        half_size = local_size // 2

        # å¯»æ‰¾æœ‰æ•ˆçš„ä¸­å¿ƒç‚¹ä½ç½®ï¼Œæ›´ä¸¥æ ¼çš„æ¡ä»¶
        valid_centers = []
        for y in range(half_size, h - half_size):
            for x in range(half_size, w - half_size):
                # æå–å€™é€‰åŒºåŸŸ
                local_region_mask = global_mask[
                    y - half_size : y + half_size + 1, x - half_size : x + half_size + 1
                ]

                # æ£€æŸ¥globalè¾¹ç¼˜åŒºåŸŸçš„æ¯”ä¾‹ - æ›´ä¸¥æ ¼çš„æ¡ä»¶
                valid_pixels = np.sum(local_region_mask == 1)
                valid_ratio = valid_pixels / (local_size * local_size)

                # ç¡®ä¿è‡³å°‘95%çš„åŒºåŸŸæ˜¯global_mask=1
                if valid_ratio >= 0.95:
                    valid_centers.append((y, x))

        if not valid_centers:
            # å¦‚æœæ²¡æœ‰95%çš„åŒºåŸŸï¼Œé™ä½åˆ°90%
            for y in range(half_size, h - half_size):
                for x in range(half_size, w - half_size):
                    local_region_mask = global_mask[
                        y - half_size : y + half_size + 1,
                        x - half_size : x + half_size + 1,
                    ]
                    valid_pixels = np.sum(local_region_mask == 1)
                    valid_ratio = valid_pixels / (local_size * local_size)

                    if valid_ratio >= 0.90:
                        valid_centers.append((y, x))

        if not valid_centers:
            # å¦‚æœè¿˜æ²¡æœ‰ï¼Œé™ä½åˆ°80%
            for y in range(half_size, h - half_size):
                for x in range(half_size, w - half_size):
                    local_region_mask = global_mask[
                        y - half_size : y + half_size + 1,
                        x - half_size : x + half_size + 1,
                    ]
                    valid_pixels = np.sum(local_region_mask == 1)
                    valid_ratio = valid_pixels / (local_size * local_size)

                    if valid_ratio >= 0.80:
                        valid_centers.append((y, x))

        if not valid_centers:
            # æœ€åå›é€€ï¼šé€‰æ‹©ä¸­å¿ƒåŒºåŸŸ
            center_y = h // 2
            center_x = w // 2
        else:
            center_y, center_x = random.choice(valid_centers)

        # æå–localåŒºåŸŸ
        local_target = global_data[
            center_y - half_size : center_y + half_size + 1,
            center_x - half_size : center_x + half_size + 1,
        ].copy()
        local_global_mask = global_mask[
            center_y - half_size : center_y + half_size + 1,
            center_x - half_size : center_x + half_size + 1,
        ].copy()

        return local_target, local_global_mask, (center_y, center_x)

    def _generate_single_sample(
        self, reference_data: np.ndarray, sample_idx: int
    ) -> Dict:
        """ç”Ÿæˆå•ä¸ªè®­ç»ƒæ ·æœ¬ - ä¿®æ­£ç‰ˆæœ¬ï¼Œç¡®ä¿local maskè¾¹ç¼˜ä¸º1"""

        # 1. ç”Ÿæˆinitial mask
        initial_mask = self._generate_initial_mask(reference_data)

        # 2. ç”Ÿæˆglobal mask
        coverage_ratio = random.uniform(*self.global_coverage_range)
        global_mask = self._generate_global_mask(initial_mask, coverage_ratio)

        # 3. éšæœºé€‰æ‹©localå°ºå¯¸
        local_size = random.choice(self.local_sizes)

        # 4. æå–localåŒºåŸŸ - ç¡®ä¿å¤§éƒ¨åˆ†åŒºåŸŸéƒ½æ˜¯global_mask=1
        local_target, local_global_mask, (center_y, center_x) = (
            self._extract_local_region(reference_data, global_mask, local_size)
        )

        # 5. ç”Ÿæˆlocal synthetic mask - åœ¨å®Œå…¨æœ‰æ•ˆçš„åŒºåŸŸå†…ç”Ÿæˆ
        missing_ratio = random.uniform(*self.local_missing_range)

        # æ£€æŸ¥local_global_maskçš„æœ‰æ•ˆåŒºåŸŸæ¯”ä¾‹
        valid_ratio = np.mean(local_global_mask)

        if valid_ratio >= 0.95:
            # å¦‚æœ95%ä»¥ä¸ŠåŒºåŸŸæœ‰æ•ˆï¼Œæ­£å¸¸ç”Ÿæˆlocal mask

            local_synthetic_mask = self._generate_local_mask(
                local_size, missing_ratio, self.enable_edge_connection
            )
        else:
            # å¦‚æœæœ‰æ•ˆåŒºåŸŸä¸è¶³95%ï¼Œåªåœ¨æœ‰æ•ˆåŒºåŸŸå†…ç”Ÿæˆç¼ºå¤±
            local_synthetic_mask = self._generate_local_mask_in_valid_region(
                local_global_mask, missing_ratio
            )

        # 6. åˆå¹¶local mask (ç¡®ä¿globalæ— æ•ˆåŒºåŸŸä¹Ÿä¸º0)
        local_mask = local_synthetic_mask * local_global_mask

        # 7. ç”Ÿæˆlocal input
        local_input = local_target.copy()

        # è®¡ç®—local_mask=1åŒºåŸŸçš„å‡å€¼ç”¨äºå¡«å……local_mask=0çš„åŒºåŸŸ
        valid_local_values = local_input[local_mask == 1]
        if len(valid_local_values) > 0:
            local_fill_value = np.mean(valid_local_values)
        else:
            local_fill_value = 0.0

        # å°†local_mask=0çš„åŒºåŸŸè®¾ä¸ºlocal_mask=1åŒºåŸŸçš„å‡å€¼
        local_input[local_mask == 0] = local_fill_value

        # 8. ç”Ÿæˆglobal input
        global_input = reference_data.copy()

        # è®¡ç®—global_mask=1åŒºåŸŸçš„å‡å€¼ä½œä¸ºå…¨å±€å¡«å……å€¼
        global_valid_values = global_input[global_mask == 1]
        if len(global_valid_values) > 0:
            global_fill_value = np.mean(global_valid_values)
        else:
            global_fill_value = 0.0

        # å…ˆå°†global_mask=0çš„åŒºåŸŸè®¾ä¸ºglobalå‡å€¼
        global_input[global_mask == 0] = global_fill_value

        # ç„¶åå°†localåŒºåŸŸä¸­local_mask=0çš„éƒ¨åˆ†ä¹Ÿè®¾ä¸ºglobalå‡å€¼
        half_size = local_size // 2
        local_start_y = center_y - half_size
        local_end_y = center_y + half_size + 1
        local_start_x = center_x - half_size
        local_end_x = center_x + half_size + 1

        # åˆ›å»ºä¸€ä¸ªæ–°çš„global_maskï¼Œå°†localç¼ºå¤±åŒºåŸŸä¹Ÿæ ‡è®°ä¸º0
        global_mask_updated = global_mask.copy()
        local_missing_mask = local_mask == 0
        global_mask_updated[local_start_y:local_end_y, local_start_x:local_end_x][
            local_missing_mask
        ] = 0

        # å¯¹åº”åœ°ï¼Œå°†global_inputä¸­çš„è¿™äº›åŒºåŸŸä¹Ÿè®¾ä¸ºglobalå‡å€¼
        global_input[local_start_y:local_end_y, local_start_x:local_end_x][
            local_missing_mask
        ] = global_fill_value

        # 9. ç”Ÿæˆmetadata
        metadata = np.array(
            [
                center_y,
                center_x,  # localä¸­å¿ƒåœ¨globalä¸­çš„ä½ç½®
                local_size,  # localå°ºå¯¸
                sample_idx,  # æ ·æœ¬ç´¢å¼•
                coverage_ratio,  # globalè¦†ç›–ç‡
                missing_ratio,  # localç¼ºå¤±ç‡
                local_size,  # é‡å¤local_sizeä»¥ä¿æŒå…¼å®¹æ€§
            ],
            dtype=np.float32,
        )

        return {
            "local_input": local_input,
            "local_mask": local_mask,
            "local_target": local_target,
            "global_input": global_input,
            "global_mask": global_mask_updated,
            "global_target": reference_data.copy(),
            "metadata": metadata,
            "local_size": local_size,
            "initial_mask": initial_mask,
        }

    def _generate_local_mask_in_valid_region(
        self, local_global_mask: np.ndarray, missing_ratio: float
    ) -> np.ndarray:
        """
        åœ¨æœ‰æ•ˆåŒºåŸŸå†…ç”Ÿæˆlocal maskï¼Œé¿å…åœ¨globalæ— æ•ˆåŒºåŸŸç”Ÿæˆç¼ºå¤±
        """
        h, w = local_global_mask.shape
        local_mask = local_global_mask.copy()  # ä»global maskå¼€å§‹

        # æ‰¾åˆ°æ‰€æœ‰æœ‰æ•ˆä½ç½®
        valid_positions = np.where(local_global_mask == 1)
        total_valid_pixels = len(valid_positions[0])

        if total_valid_pixels == 0:
            return local_mask

        # è®¡ç®—è¦ç§»é™¤çš„åƒç´ æ•°ï¼ˆåªåœ¨æœ‰æ•ˆåŒºåŸŸå†…ï¼‰
        target_missing_pixels = int(total_valid_pixels * missing_ratio)
        if target_missing_pixels <= 0:
            return local_mask

        # åœ¨æœ‰æ•ˆåŒºåŸŸå†…ç”Ÿæˆé›†ä¸­çš„ç¼ºå¤±åŒºåŸŸ
        if target_missing_pixels < total_valid_pixels * 0.8:  # å¦‚æœç¼ºå¤±ä¸è¶…è¿‡80%
            # é€‰æ‹©ä¸€ä¸ªæˆ–å‡ ä¸ªä¸­å¿ƒç‚¹è¿›è¡Œé›†ä¸­ç¼ºå¤±
            num_clusters = random.randint(
                1, min(3, max(1, target_missing_pixels // 20))
            )
            pixels_per_cluster = target_missing_pixels // num_clusters

            removed_pixels = 0

            for cluster_idx in range(num_clusters):
                if removed_pixels >= target_missing_pixels:
                    break

                cluster_pixels = min(
                    pixels_per_cluster, target_missing_pixels - removed_pixels
                )

                # éšæœºé€‰æ‹©ä¸€ä¸ªæœ‰æ•ˆä½ç½®ä½œä¸ºä¸­å¿ƒ
                valid_indices = np.where(local_mask == 1)
                if len(valid_indices[0]) == 0:
                    break

                center_idx = random.randint(0, len(valid_indices[0]) - 1)
                center_y = valid_indices[0][center_idx]
                center_x = valid_indices[1][center_idx]

                # ä»ä¸­å¿ƒç‚¹å¼€å§‹åŒºåŸŸç”Ÿé•¿å¼ç§»é™¤
                to_remove = [(center_y, center_x)]
                removed_in_cluster = 0
                processed = set()

                directions = [
                    (-1, -1),
                    (-1, 0),
                    (-1, 1),
                    (0, -1),
                    (0, 1),
                    (1, -1),
                    (1, 0),
                    (1, 1),
                ]

                while to_remove and removed_in_cluster < cluster_pixels:
                    y, x = to_remove.pop(0)

                    if (y, x) in processed:
                        continue

                    if 0 <= y < h and 0 <= x < w and local_mask[y, x] == 1:
                        local_mask[y, x] = 0
                        removed_pixels += 1
                        removed_in_cluster += 1
                        processed.add((y, x))

                        # æ·»åŠ é‚»å±…
                        for dy, dx in directions:
                            ny, nx = y + dy, x + dx
                            if (
                                0 <= ny < h
                                and 0 <= nx < w
                                and (ny, nx) not in processed
                                and local_mask[ny, nx] == 1
                                and random.random() < 0.7
                            ):
                                to_remove.append((ny, nx))

                        if removed_pixels >= target_missing_pixels:
                            break
        else:
            # å¦‚æœç¼ºå¤±æ¯”ä¾‹å¾ˆé«˜ï¼Œéšæœºç§»é™¤
            valid_indices = list(zip(valid_positions[0], valid_positions[1]))
            random.shuffle(valid_indices)

            for i, (y, x) in enumerate(valid_indices):
                if i >= target_missing_pixels:
                    break
                local_mask[y, x] = 0

        return local_mask

    def __len__(self) -> int:
        return self.total_samples

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, ...]:
        """è·å–è®­ç»ƒæ ·æœ¬"""

        # è®¡ç®—æ–‡ä»¶ç´¢å¼•å’Œæ ·æœ¬ç´¢å¼•
        file_idx = idx // self.samples_per_file
        sample_idx = idx % self.samples_per_file

        # è·å–å‚è€ƒæ–‡ä»¶
        file_path = self.reference_files[file_idx]

        try:
            # åŠ è½½å‚è€ƒæ•°æ®
            reference_data = self._load_reference_data(file_path)

            # ç”Ÿæˆæ ·æœ¬
            sample = self._generate_single_sample(reference_data, sample_idx)

            # è½¬æ¢ä¸ºtensor
            return (
                torch.tensor(sample["local_input"], dtype=torch.float32).unsqueeze(
                    0
                ),  # (1, H, W)
                torch.tensor(sample["local_mask"], dtype=torch.float32).unsqueeze(
                    0
                ),  # (1, H, W)
                torch.tensor(sample["local_target"], dtype=torch.float32),  # (H, W)
                torch.tensor(sample["global_input"], dtype=torch.float32).unsqueeze(
                    0
                ),  # (1, H, W)
                torch.tensor(sample["global_mask"], dtype=torch.float32).unsqueeze(
                    0
                ),  # (1, H, W)
                torch.tensor(sample["global_target"], dtype=torch.float32),  # (H, W)
                sample["metadata"].astype(np.int32),  # metadata
            )

        except Exception as e:
            print(f" ç”Ÿæˆæ ·æœ¬å¤±è´¥ (idx={idx}): {e}")
            # è¿”å›fallbackæ•°æ®
            return self._get_fallback_sample()

    def _get_fallback_sample(self) -> Tuple[torch.Tensor, ...]:
        """ç”Ÿæˆfallbackæ ·æœ¬"""
        local_size = self.local_sizes[0]  # ä½¿ç”¨æœ€å°å°ºå¯¸

        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        local_input = torch.randn(1, local_size, local_size) * 0.1 + 0.5
        local_mask = torch.ones(1, local_size, local_size)
        local_target = torch.randn(local_size, local_size) * 0.1 + 0.5
        global_input = torch.randn(1, 600, 600) * 0.1 + 0.5
        global_mask = torch.ones(1, 600, 600)
        global_target = torch.randn(600, 600) * 0.1 + 0.5
        metadata = np.array(
            [300, 300, local_size, 0, 0.8, 0.5, local_size], dtype=np.int32
        )

        return (
            local_input,
            local_mask,
            local_target,
            global_input,
            global_mask,
            global_target,
            metadata,
        )

    def get_sample_info(self, idx: int) -> Dict:
        """è·å–æ ·æœ¬ä¿¡æ¯(ç”¨äºè°ƒè¯•)"""
        file_idx = idx // self.samples_per_file
        sample_idx = idx % self.samples_per_file
        file_path = self.reference_files[file_idx]

        return {
            "file_idx": file_idx,
            "sample_idx": sample_idx,
            "file_path": file_path,
            "file_name": os.path.basename(file_path),
        }

    def _visualize_samples(self, num_samples: int):
        """å¯è§†åŒ–ç”Ÿæˆçš„æ ·æœ¬"""
        print(f" æ­£åœ¨å¯è§†åŒ– {num_samples} ä¸ªæ ·æœ¬...")

        fig, axes = plt.subplots(num_samples, 6, figsize=(18, 3 * num_samples))
        if num_samples == 1:
            axes = axes.reshape(1, -1)

        for i in range(num_samples):
            try:
                # è·å–æ ·æœ¬
                sample_data = self[i * (self.total_samples // num_samples)]
                (
                    local_input,
                    local_mask,
                    local_target,
                    global_input,
                    global_mask,
                    global_target,
                    metadata,
                ) = sample_data

                # è½¬æ¢ä¸ºnumpyç”¨äºæ˜¾ç¤º
                local_input_np = local_input.squeeze().numpy()
                local_mask_np = local_mask.squeeze().numpy()
                local_target_np = local_target.numpy()
                global_input_np = global_input.squeeze().numpy()
                global_mask_np = global_mask.squeeze().numpy()
                global_target_np = global_target.numpy()

                # è·å–å°ºå¯¸ä¿¡æ¯
                local_size = metadata[2]
                center_y, center_x = metadata[0], metadata[1]

                # è®¡ç®—é¢œè‰²èŒƒå›´
                all_valid = np.concatenate(
                    [
                        local_target_np[local_target_np != 0],
                        global_target_np[global_target_np != 0],
                    ]
                )
                if len(all_valid) > 0:
                    vmin, vmax = np.percentile(all_valid, [2, 98])
                else:
                    vmin, vmax = 0, 1

                # ç»˜åˆ¶å›¾åƒ
                axes[i, 0].imshow(
                    global_target_np, cmap="terrain", vmin=vmin, vmax=vmax
                )
                axes[i, 0].set_title(f"Global Target\n{global_target_np.shape}")
                axes[i, 0].plot(
                    center_x, center_y, "r+", markersize=10, markeredgewidth=2
                )

                axes[i, 1].imshow(global_mask_np, cmap="binary")
                axes[i, 1].set_title(
                    f"Global Mask\nè¦†ç›–ç‡: {np.mean(global_mask_np):.1%}"
                )

                axes[i, 2].imshow(global_input_np, cmap="terrain", vmin=vmin, vmax=vmax)
                axes[i, 2].set_title("Global Input")

                axes[i, 3].imshow(local_target_np, cmap="terrain", vmin=vmin, vmax=vmax)
                axes[i, 3].set_title(f"Local Target\n{local_size}x{local_size}")

                axes[i, 4].imshow(local_mask_np, cmap="binary")
                missing_ratio = 1 - np.mean(local_mask_np)
                # æ£€æŸ¥æ˜¯å¦ä¸è¾¹ç¼˜ç›¸æ¥
                edge_connected = self._check_edge_connection(local_mask_np)
                edge_text = " (è¾¹ç¼˜)" if edge_connected else ""
                axes[i, 4].set_title(
                    f"Local Mask\nç¼ºå¤±: {missing_ratio:.1%}{edge_text}"
                )

                # æ˜¾ç¤ºmasked local input
                masked_local = local_input_np.copy()
                masked_local[local_mask_np == 0] = np.nan
                axes[i, 5].imshow(masked_local, cmap="terrain", vmin=vmin, vmax=vmax)
                axes[i, 5].set_title("Local Input (Masked)")

                # ç§»é™¤åæ ‡è½´
                for ax in axes[i]:
                    ax.set_xticks([])
                    ax.set_yticks([])

            except Exception as e:
                print(f" å¯è§†åŒ–æ ·æœ¬ {i} å¤±è´¥: {e}")
                for ax in axes[i]:
                    ax.text(
                        0.5,
                        0.5,
                        f"Error: {e}",
                        ha="center",
                        va="center",
                        transform=ax.transAxes,
                    )

        plt.tight_layout()
        plt.show()
        plt.pause(3.0)
        plt.close()

    def _check_edge_connection(self, mask: np.ndarray) -> bool:
        """æ£€æŸ¥maskæ˜¯å¦ä¸è¾¹ç¼˜ç›¸æ¥"""
        h, w = mask.shape

        # æ£€æŸ¥å››ä¸ªè¾¹ç¼˜æ˜¯å¦æœ‰ç¼ºå¤±å€¼(0)
        edges = [
            mask[0, :],  # ä¸Šè¾¹ç¼˜
            mask[-1, :],  # ä¸‹è¾¹ç¼˜
            mask[:, 0],  # å·¦è¾¹ç¼˜
            mask[:, -1],  # å³è¾¹ç¼˜
        ]

        for edge in edges:
            if np.any(edge == 0):
                return True

        return False


def multiscale_collate_fn(batch):
    """
    å¤šå°ºå¯¸æ•°æ®çš„collateå‡½æ•°
    å¤„ç†ä¸åŒå°ºå¯¸çš„localæ•°æ®
    """
    # æŒ‰localå°ºå¯¸åˆ†ç»„
    size_groups = {}

    for item in batch:
        if item is not None and len(item) == 7:
            local_size = item[6][2]  # metadataä¸­çš„local_size
            if local_size not in size_groups:
                size_groups[local_size] = []
            size_groups[local_size].append(item)

    # å¦‚æœåªæœ‰ä¸€ç§å°ºå¯¸ï¼Œç›´æ¥å¤„ç†
    if len(size_groups) == 1:
        size = list(size_groups.keys())[0]
        items = size_groups[size]

        local_inputs = torch.stack([item[0] for item in items])
        local_masks = torch.stack([item[1] for item in items])
        local_targets = torch.stack([item[2] for item in items])
        global_inputs = torch.stack([item[3] for item in items])
        global_masks = torch.stack([item[4] for item in items])
        global_targets = torch.stack([item[5] for item in items])
        metadata = [item[6] for item in items]

        return (
            local_inputs,
            local_masks,
            local_targets,
            global_inputs,
            global_masks,
            global_targets,
            metadata,
        )

    else:
        # å¤šç§å°ºå¯¸ï¼šé€‰æ‹©æœ€å¸¸è§çš„å°ºå¯¸
        size_counts = {size: len(items) for size, items in size_groups.items()}
        most_common_size = max(size_counts, key=size_counts.get)

        items = size_groups[most_common_size]

        # å¦‚æœæ•°é‡ä¸å¤Ÿä¸€ä¸ªbatchï¼Œç”¨å…¶ä»–å°ºå¯¸è¡¥å……
        target_batch_size = len(batch)
        while len(items) < target_batch_size:
            for size, group_items in size_groups.items():
                if size != most_common_size and group_items:
                    items.append(group_items[0])
                    if len(items) >= target_batch_size:
                        break
            break

        items = items[:target_batch_size]  # æˆªæ–­åˆ°ç›®æ ‡å¤§å°

        local_inputs = torch.stack([item[0] for item in items])
        local_masks = torch.stack([item[1] for item in items])
        local_targets = torch.stack([item[2] for item in items])
        global_inputs = torch.stack([item[3] for item in items])
        global_masks = torch.stack([item[4] for item in items])
        global_targets = torch.stack([item[5] for item in items])
        metadata = [item[6] for item in items]

        return (
            local_inputs,
            local_masks,
            local_targets,
            global_inputs,
            global_masks,
            global_targets,
            metadata,
        )


def create_multiscale_dataloader(
    reference_dir: str,
    local_sizes: List[int] = [64, 128, 256],
    batch_size: int = 8,
    shuffle: bool = True,
    num_workers: int = 4,
    **dataset_kwargs,
) -> DataLoader:
    """
    åˆ›å»ºå¤šå°ºå¯¸DEMæ•°æ®åŠ è½½å™¨

    Args:
        reference_dir: å‚è€ƒæ–‡ä»¶ç›®å½•
        local_sizes: localæ•°æ®å°ºå¯¸åˆ—è¡¨
        batch_size: æ‰¹æ¬¡å¤§å°
        shuffle: æ˜¯å¦éšæœºæ‰“ä¹±
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        **dataset_kwargs: ä¼ é€’ç»™æ•°æ®é›†çš„å…¶ä»–å‚æ•°

    Returns:
        DataLoaderå®ä¾‹
    """

    dataset = MultiScaleDEMDataset(
        reference_dir=reference_dir, local_sizes=local_sizes, **dataset_kwargs
    )

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        collate_fn=multiscale_collate_fn,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=2 if num_workers > 0 else 2,
    )

    return dataloader


def analyze_dataset_statistics(dataset: MultiScaleDEMDataset, num_samples: int = 100):
    """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    print(f" åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ (é‡‡æ · {num_samples} ä¸ªæ ·æœ¬)...")

    local_size_counts = {size: 0 for size in dataset.local_sizes}
    global_coverage_stats = []
    local_missing_stats = []
    edge_connection_count = 0

    sample_indices = np.random.choice(
        len(dataset), min(num_samples, len(dataset)), replace=False
    )

    for idx in tqdm(sample_indices, desc="ç»Ÿè®¡åˆ†æ"):
        try:
            sample_data = dataset[idx]
            metadata = sample_data[6]

            local_size = metadata[2]
            local_size_counts[local_size] += 1

            global_coverage = metadata[4]
            local_missing = metadata[5]

            global_coverage_stats.append(global_coverage)
            local_missing_stats.append(local_missing)

            # æ£€æŸ¥æ˜¯å¦ä¸è¾¹ç¼˜ç›¸æ¥
            local_mask = sample_data[1].squeeze().numpy()
            if dataset._check_edge_connection(local_mask):
                edge_connection_count += 1

        except Exception as e:
            print(f" ç»Ÿè®¡æ ·æœ¬ {idx} å¤±è´¥: {e}")
            continue

    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡ç»“æœ:")
    print(f"Localå°ºå¯¸åˆ†å¸ƒ:")
    for size, count in local_size_counts.items():
        ratio = count / sum(local_size_counts.values()) * 100
        print(f"   {size}x{size}: {count} ({ratio:.1f}%)")

    if global_coverage_stats:
        print(f"\nGlobalè¦†ç›–ç‡ç»Ÿè®¡:")
        print(f"   å‡å€¼: {np.mean(global_coverage_stats):.3f}")
        print(
            f"   èŒƒå›´: [{np.min(global_coverage_stats):.3f}, {np.max(global_coverage_stats):.3f}]"
        )

        print(f"\nLocalç¼ºå¤±ç‡ç»Ÿè®¡:")
        print(f"   å‡å€¼: {np.mean(local_missing_stats):.3f}")
        print(
            f"   èŒƒå›´: [{np.min(local_missing_stats):.3f}, {np.max(local_missing_stats):.3f}]"
        )

        actual_edge_ratio = edge_connection_count / len(sample_indices) * 100
        print(
            f"\nè¾¹ç¼˜ç›¸æ¥æ ·æœ¬: {edge_connection_count}/{len(sample_indices)} ({actual_edge_ratio:.1f}%)"
        )
        print(f"   è®¾ç½®æ¦‚ç‡: {dataset.edge_connection_prob:.1%}")
        print(f"   å®é™…æ¯”ä¾‹: {actual_edge_ratio:.1f}%")


import os
import time
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import pickle
import h5py
from tqdm import tqdm
import json
from datetime import datetime


class PreGeneratedDEMDataset(Dataset):
    """é¢„ç”Ÿæˆçš„DEMæ•°æ®é›†ï¼Œä»æ–‡ä»¶åŠ è½½"""

    def __init__(self, data_dir, file_format="npz"):
        """
        Args:
            data_dir: é¢„ç”Ÿæˆæ•°æ®çš„ç›®å½•
            file_format: æ•°æ®æ ¼å¼ ('npz', 'h5', 'pt')
        """
        self.data_dir = Path(data_dir)
        self.file_format = file_format

        # è¯»å–å…ƒæ•°æ®
        with open(self.data_dir / "metadata.json", "r") as f:
            self.metadata = json.load(f)

        self.total_samples = self.metadata["total_samples"]
        self.local_sizes = self.metadata["local_sizes"]

        # æ ¹æ®æ ¼å¼è®¾ç½®æ–‡ä»¶åˆ—è¡¨
        if file_format == "h5":
            self.data_file = self.data_dir / "data.h5"
            self._open_h5_file()
        else:
            self.sample_files = sorted(self.data_dir.glob(f"sample_*.{file_format}"))

    def _open_h5_file(self):
        """æ‰“å¼€HDF5æ–‡ä»¶"""
        self.h5_file = h5py.File(self.data_file, "r")

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):
        if self.file_format == "npz":
            # NPZæ ¼å¼
            file_path = self.data_dir / f"sample_{idx:06d}.npz"
            data = np.load(file_path)

            return (
                torch.from_numpy(data["local_input"])
                .float()
                .unsqueeze(0),  # æ·»åŠ channelç»´åº¦
                torch.from_numpy(data["local_mask"]).float().unsqueeze(0),
                torch.from_numpy(data["local_target"]).float(),  # æ²¡æœ‰channelç»´åº¦
                torch.from_numpy(data["global_input"]).float().unsqueeze(0),
                torch.from_numpy(data["global_mask"]).float().unsqueeze(0),
                torch.from_numpy(data["global_target"]).float(),
                data["metadata"].astype(np.int32),  # ä¿æŒä¸åŸå§‹æ•°æ®é›†ä¸€è‡´
            )

        elif self.file_format == "h5":
            # HDF5æ ¼å¼
            group = self.h5_file[f"sample_{idx:06d}"]

            # é‡å»ºmetadataæ•°ç»„
            metadata = np.array(
                [group.attrs.get(f"metadata_{i}", 0) for i in range(7)], dtype=np.int32
            )

            return (
                torch.from_numpy(group["local_input"][:]).float().unsqueeze(0),
                torch.from_numpy(group["local_mask"][:]).float().unsqueeze(0),
                torch.from_numpy(group["local_target"][:]).float(),
                torch.from_numpy(group["global_input"][:]).float().unsqueeze(0),
                torch.from_numpy(group["global_mask"][:]).float().unsqueeze(0),
                torch.from_numpy(group["global_target"][:]).float(),
                metadata,
            )

        elif self.file_format == "pt":
            # PyTorchæ ¼å¼
            file_path = self.data_dir / f"sample_{idx:06d}.pt"
            data = torch.load(file_path)

            # å¦‚æœä¿å­˜çš„æ˜¯å­—å…¸æ ¼å¼
            if isinstance(data, dict):
                return (
                    data["local_input"],
                    data["local_mask"],
                    data["local_target"],
                    data["global_input"],
                    data["global_mask"],
                    data["global_target"],
                    data["metadata"],
                )
            else:
                # å¦‚æœä¿å­˜çš„æ˜¯tupleæ ¼å¼
                return data

    def __del__(self):
        if hasattr(self, "h5_file"):
            self.h5_file.close()


def pregenerate_and_save_dataset(
    dataset, save_dir, file_format="npz", num_samples=None
):
    """
    é¢„ç”Ÿæˆæ•°æ®é›†å¹¶ä¿å­˜åˆ°æ–‡ä»¶

    Args:
        dataset: MultiScaleDEMDatasetå®ä¾‹
        save_dir: ä¿å­˜ç›®å½•
        file_format: ä¿å­˜æ ¼å¼ ('npz', 'h5', 'pt')
        num_samples: è¦ç”Ÿæˆçš„æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨

    Returns:
        ç”Ÿæˆæ—¶é—´ç»Ÿè®¡
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    if num_samples is None:
        num_samples = len(dataset)
    else:
        num_samples = min(num_samples, len(dataset))

    print(f"\n å¼€å§‹é¢„ç”Ÿæˆæ•°æ®é›†...")
    print(f"   ä¿å­˜ç›®å½•: {save_dir}")
    print(f"   æ–‡ä»¶æ ¼å¼: {file_format}")
    print(f"   æ ·æœ¬æ•°é‡: {num_samples}")

    start_time = time.time()
    generation_times = []
    save_times = []

    # ä¿å­˜å…ƒæ•°æ®
    # ä¿®æ”¹ pregenerate_and_save_dataset å‡½æ•°ä¸­çš„å…ƒæ•°æ®éƒ¨åˆ†
    ref_data = dataset._load_reference_data(dataset.reference_files[0])
    metadata_dict = {
        "total_samples": num_samples,
        "local_sizes": dataset.local_sizes,
        "global_size": ref_data.shape,  # ä½¿ç”¨å®é™…æ•°æ®çš„å½¢çŠ¶
        "creation_time": datetime.now().isoformat(),
        "global_coverage_range": dataset.global_coverage_range,
        "local_missing_range": dataset.local_missing_range,
        "samples_per_file": dataset.samples_per_file,
    }

    with open(save_dir / "metadata.json", "w") as f:
        json.dump(metadata_dict, f, indent=2)

    if file_format == "h5":
        # HDF5æ ¼å¼ - æ‰€æœ‰æ•°æ®ä¿å­˜åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­
        h5_file = h5py.File(save_dir / "data.h5", "w")

    # ä½¿ç”¨è¿›åº¦æ¡
    with tqdm(total=num_samples, desc="ç”Ÿæˆæ•°æ®") as pbar:
        for idx in range(num_samples):
            # ç”Ÿæˆæ•°æ®
            gen_start = time.time()
            data = dataset[idx]
            gen_time = time.time() - gen_start
            generation_times.append(gen_time)

            # è§£åŒ…æ•°æ® - æŒ‰ç…§æ‚¨çš„__getitem__è¿”å›é¡ºåº
            (
                local_input,
                local_mask,
                local_target,
                global_input,
                global_mask,
                global_target,
                metadata,
            ) = data

            # ä¿å­˜æ•°æ®
            save_start = time.time()

            if file_format == "npz":
                # NPZæ ¼å¼ - ç§»é™¤squeezeæ“ä½œï¼Œå› ä¸ºæ•°æ®å·²ç»æœ‰channelç»´åº¦
                np.savez_compressed(
                    save_dir / f"sample_{idx:06d}.npz",
                    local_input=local_input.squeeze(0).numpy(),  # ç§»é™¤channelç»´åº¦ä¿å­˜
                    local_mask=local_mask.squeeze(0).numpy(),
                    local_target=local_target.numpy(),  # è¿™ä¸ªæ²¡æœ‰channelç»´åº¦
                    global_input=global_input.squeeze(0).numpy(),
                    global_mask=global_mask.squeeze(0).numpy(),
                    global_target=global_target.numpy(),
                    metadata=metadata,  # å·²ç»æ˜¯numpy array
                )

            elif file_format == "h5":
                # HDF5æ ¼å¼
                group = h5_file.create_group(f"sample_{idx:06d}")
                group.create_dataset(
                    "local_input",
                    data=local_input.squeeze(0).numpy(),
                    compression="gzip",
                )
                group.create_dataset(
                    "local_mask", data=local_mask.squeeze(0).numpy(), compression="gzip"
                )
                group.create_dataset(
                    "local_target", data=local_target.numpy(), compression="gzip"
                )
                group.create_dataset(
                    "global_input",
                    data=global_input.squeeze(0).numpy(),
                    compression="gzip",
                )
                group.create_dataset(
                    "global_mask",
                    data=global_mask.squeeze(0).numpy(),
                    compression="gzip",
                )
                group.create_dataset(
                    "global_target", data=global_target.numpy(), compression="gzip"
                )
                # HDF5ä¸æ”¯æŒç›´æ¥å­˜å‚¨numpy objectæ•°ç»„ï¼Œè½¬æ¢ä¸ºå±æ€§
                for i, val in enumerate(metadata):
                    group.attrs[f"metadata_{i}"] = float(val)

            elif file_format == "pt":
                # PyTorchæ ¼å¼ - ç›´æ¥ä¿å­˜æ‰€æœ‰tensor
                torch.save(
                    {
                        "local_input": local_input,
                        "local_mask": local_mask,
                        "local_target": local_target,
                        "global_input": global_input,
                        "global_mask": global_mask,
                        "global_target": global_target,
                        "metadata": metadata,
                    },
                    save_dir / f"sample_{idx:06d}.pt",
                )

            save_time = time.time() - save_start
            save_times.append(save_time)

            # æ›´æ–°è¿›åº¦æ¡
            pbar.update(1)
            pbar.set_postfix(
                {"gen_time": f"{gen_time:.3f}s", "save_time": f"{save_time:.3f}s"}
            )

            # æ¯100ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
            if (idx + 1) % 100 == 0:
                print(f"\n  å·²ç”Ÿæˆ {idx + 1} ä¸ªæ ·æœ¬")
                print(f"  å½“å‰å¹³å‡ç”Ÿæˆæ—¶é—´: {np.mean(generation_times):.3f}ç§’/æ ·æœ¬")
                print(f"  å½“å‰å¹³å‡ä¿å­˜æ—¶é—´: {np.mean(save_times):.3f}ç§’/æ ·æœ¬")

    if file_format == "h5":
        h5_file.close()

    total_time = time.time() - start_time

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_time": total_time,
        "avg_generation_time": np.mean(generation_times),
        "avg_save_time": np.mean(save_times),
        "total_generation_time": np.sum(generation_times),
        "total_save_time": np.sum(save_times),
        "samples_per_second": num_samples / total_time,
    }

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    with open(save_dir / "generation_stats.json", "w") as f:
        json.dump(stats, f, indent=2)

    print(f"\n æ•°æ®ç”Ÿæˆå®Œæˆ!")
    print(f"   æ€»æ—¶é—´: {total_time:.2f}ç§’")
    print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {stats['avg_generation_time']:.3f}ç§’/æ ·æœ¬")
    print(f"   å¹³å‡ä¿å­˜æ—¶é—´: {stats['avg_save_time']:.3f}ç§’/æ ·æœ¬")
    print(f"   ç”Ÿæˆé€Ÿåº¦: {stats['samples_per_second']:.2f}æ ·æœ¬/ç§’")

    # è®¡ç®—å­˜å‚¨ç©ºé—´
    total_size = sum(f.stat().st_size for f in save_dir.rglob("*")) / (1024**2)
    print(f"   æ€»å­˜å‚¨ç©ºé—´: {total_size:.2f} MB")
    print(f"   å¹³å‡æ¯æ ·æœ¬: {total_size/num_samples:.2f} MB")

    return stats


def test_data_loading(data_dir, file_format="npz", num_tests=10):
    """æµ‹è¯•æ•°æ®åŠ è½½é€Ÿåº¦"""
    print(f"\n æµ‹è¯•æ•°æ®åŠ è½½...")
    print(f"   æ•°æ®ç›®å½•: {data_dir}")
    print(f"   æ–‡ä»¶æ ¼å¼: {file_format}")

    # åˆ›å»ºæ•°æ®é›†
    dataset = PreGeneratedDEMDataset(data_dir, file_format)

    # éšæœºæµ‹è¯•
    load_times = []
    indices = np.random.randint(0, len(dataset), num_tests)

    print(f"\n éšæœºåŠ è½½æµ‹è¯• ({num_tests}ä¸ªæ ·æœ¬):")
    for i, idx in enumerate(indices):
        start_time = time.time()
        data = dataset[idx]
        load_time = time.time() - start_time
        load_times.append(load_time)
        print(f"   æ ·æœ¬ {idx}: {load_time*1000:.2f} ms")

    print(f"\n   å¹³å‡åŠ è½½æ—¶é—´: {np.mean(load_times)*1000:.2f} ms")
    print(f"   æœ€å¤§åŠ è½½æ—¶é—´: {np.max(load_times)*1000:.2f} ms")
    print(f"   æœ€å°åŠ è½½æ—¶é—´: {np.min(load_times)*1000:.2f} ms")

    # æµ‹è¯•DataLoader
    print(f"\n DataLoaderæµ‹è¯•:")
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)

    batch_times = []
    for i, batch in enumerate(dataloader):
        if i >= 5:  # åªæµ‹è¯•5ä¸ªbatch
            break
        start_time = time.time()
        # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
        _ = batch
        batch_time = time.time() - start_time
        batch_times.append(batch_time)
        print(f"   Batch {i+1}: {batch_time*1000:.2f} ms")

    print(f"\n   å¹³å‡batchæ—¶é—´: {np.mean(batch_times)*1000:.2f} ms")

    return load_times, batch_times


def compare_formats(dataset, save_base_dir, num_samples=100):
    """æ¯”è¾ƒä¸åŒä¿å­˜æ ¼å¼çš„æ€§èƒ½"""
    formats = ["npz", "h5", "pt"]
    results = {}

    print(f"\n æ¯”è¾ƒä¸åŒä¿å­˜æ ¼å¼ (å„{num_samples}ä¸ªæ ·æœ¬)")
    print("=" * 60)

    for fmt in formats:
        print(f"\nğŸ”¸ æµ‹è¯•æ ¼å¼: {fmt}")
        save_dir = Path(save_base_dir) / f"format_{fmt}"

        # ç”Ÿæˆå¹¶ä¿å­˜
        gen_stats = pregenerate_and_save_dataset(
            dataset, save_dir, file_format=fmt, num_samples=num_samples
        )

        # æµ‹è¯•åŠ è½½
        load_times, batch_times = test_data_loading(save_dir, file_format=fmt)

        results[fmt] = {
            "generation": gen_stats,
            "load_times": load_times,
            "batch_times": batch_times,
        }

    # æ‰“å°æ¯”è¾ƒç»“æœ
    print("\n æ ¼å¼æ¯”è¾ƒæ€»ç»“:")
    print("=" * 60)
    print(
        f"{'æ ¼å¼':<10} {'ç”Ÿæˆæ—¶é—´':<15} {'ä¿å­˜æ—¶é—´':<15} {'åŠ è½½æ—¶é—´':<15} {'å­˜å‚¨ç©ºé—´':<15}"
    )
    print("-" * 60)

    for fmt in formats:
        gen = results[fmt]["generation"]
        load = results[fmt]["load_times"]

        save_dir = Path(save_base_dir) / f"format_{fmt}"
        total_size = sum(f.stat().st_size for f in save_dir.rglob("*")) / (1024**2)

        print(
            f"{fmt:<10} "
            f"{gen['avg_generation_time']*1000:<15.2f} "
            f"{gen['avg_save_time']*1000:<15.2f} "
            f"{np.mean(load)*1000:<15.2f} "
            f"{total_size:<15.2f}"
        )

    return results


# ä¾¿æ·çš„æ‰¹é‡ç”Ÿæˆè„šæœ¬
def batch_pregenerate(
    reference_dir,
    save_dir,
    total_samples=10000,
    samples_per_batch=1000,
    local_sizes=[257],
    file_format="npz",
):
    """
    æ‰¹é‡é¢„ç”Ÿæˆæ•°æ®é›†ï¼Œé€‚åˆç”Ÿæˆå¤§é‡æ•°æ®

    Args:
        reference_dir: å‚è€ƒæ•°æ®ç›®å½•
        save_dir: ä¿å­˜ç›®å½•
        total_samples: æ€»æ ·æœ¬æ•°
        samples_per_batch: æ¯æ‰¹ç”Ÿæˆçš„æ ·æœ¬æ•°
        local_sizes: localå°ºå¯¸åˆ—è¡¨
        file_format: ä¿å­˜æ ¼å¼
    """
    from datetime import datetime

    # é…ç½®
    config = {
        "reference_dir": reference_dir,
        "local_sizes": local_sizes,
        "samples_per_file": 10,
        "global_coverage_range": (0.5, 1.0),
        "local_missing_range": (0.3, 0.7),
        "max_edge_missing_ratio": 0.3,
        "enable_edge_connection": True,
        "edge_connection_prob": 0.02,
        "seed": 41,
        "cache_reference_files": True,
        "visualization_samples": 0,
    }

    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = datetime.now()
    print(f"\n å¼€å§‹æ‰¹é‡ç”Ÿæˆæ•°æ®é›†")
    print(f"   å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   æ¯æ‰¹æ ·æœ¬æ•°: {samples_per_batch}")
    print(f"   ä¿å­˜æ ¼å¼: {file_format}")
    print(f"   ä¿å­˜ç›®å½•: {save_path}")

    # åˆ†æ‰¹ç”Ÿæˆ
    num_batches = (total_samples + samples_per_batch - 1) // samples_per_batch

    all_stats = []

    for batch_idx in range(num_batches):
        batch_start = batch_idx * samples_per_batch
        batch_end = min((batch_idx + 1) * samples_per_batch, total_samples)
        batch_size = batch_end - batch_start

        print(f"\n å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}")
        print(f"   æ ·æœ¬èŒƒå›´: {batch_start} - {batch_end-1}")

        # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
        batch_dir = save_path / f"batch_{batch_idx:04d}"

        # åˆ›å»ºæ•°æ®é›†ï¼ˆæ¯æ‰¹éƒ½é‡æ–°åˆ›å»ºä»¥é¿å…å†…å­˜é—®é¢˜ï¼‰
        dataset = MultiScaleDEMDataset(**config)

        # ç”Ÿæˆå¹¶ä¿å­˜
        stats = pregenerate_and_save_dataset(
            dataset, batch_dir, file_format=file_format, num_samples=batch_size
        )

        all_stats.append(stats)

        # æ¸…ç†å†…å­˜
        del dataset

        # ä¼°è®¡å‰©ä½™æ—¶é—´
        if batch_idx > 0:
            elapsed = (datetime.now() - start_time).total_seconds()
            avg_time_per_batch = elapsed / (batch_idx + 1)
            remaining_batches = num_batches - batch_idx - 1
            eta = remaining_batches * avg_time_per_batch
            print(f"   é¢„è®¡å‰©ä½™æ—¶é—´: {eta/60:.1f} åˆ†é’Ÿ")

    # åˆ›å»ºä¸»å…ƒæ•°æ®æ–‡ä»¶
    main_metadata = {
        "total_samples": total_samples,
        "num_batches": num_batches,
        "samples_per_batch": samples_per_batch,
        "local_sizes": local_sizes,
        "file_format": file_format,
        "creation_time": start_time.isoformat(),
        "completion_time": datetime.now().isoformat(),
        "batch_dirs": [f"batch_{i:04d}" for i in range(num_batches)],
        "config": config,
    }

    with open(save_path / "main_metadata.json", "w") as f:
        json.dump(main_metadata, f, indent=2)

    # æ±‡æ€»ç»Ÿè®¡
    total_time = (datetime.now() - start_time).total_seconds()
    avg_gen_time = np.mean([s["avg_generation_time"] for s in all_stats])
    avg_save_time = np.mean([s["avg_save_time"] for s in all_stats])

    print(f"\n æ‰¹é‡ç”Ÿæˆå®Œæˆ!")
    print(f"   å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"   å¹³å‡ç”Ÿæˆé€Ÿåº¦: {total_samples/total_time:.2f} æ ·æœ¬/ç§’")
    print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_gen_time:.3f} ç§’/æ ·æœ¬")
    print(f"   å¹³å‡ä¿å­˜æ—¶é—´: {avg_save_time:.3f} ç§’/æ ·æœ¬")

    # è®¡ç®—æ€»å­˜å‚¨ç©ºé—´
    total_size = sum(f.stat().st_size for f in save_path.rglob("*")) / (1024**3)
    print(f"   æ€»å­˜å‚¨ç©ºé—´: {total_size:.2f} GB")
    print(f"   å¹³å‡æ¯æ ·æœ¬: {total_size*1024/total_samples:.2f} MB")


class BatchPreGeneratedDataset(Dataset):
    """ä»æ‰¹é‡ç”Ÿæˆçš„ç›®å½•åŠ è½½æ•°æ®é›†"""

    def __init__(self, data_dir, file_format="npz"):
        self.data_dir = Path(data_dir)
        self.file_format = file_format

        # è¯»å–ä¸»å…ƒæ•°æ®
        with open(self.data_dir / "main_metadata.json", "r") as f:
            self.main_metadata = json.load(f)

        self.total_samples = self.main_metadata["total_samples"]
        self.samples_per_batch = self.main_metadata["samples_per_batch"]
        self.batch_dirs = self.main_metadata["batch_dirs"]

        # é¢„åŠ è½½æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®é›†ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.batch_datasets = {}

    def _get_batch_dataset(self, batch_idx):
        """å»¶è¿ŸåŠ è½½æ‰¹æ¬¡æ•°æ®é›†"""
        if batch_idx not in self.batch_datasets:
            batch_dir = self.data_dir / self.batch_dirs[batch_idx]
            self.batch_datasets[batch_idx] = PreGeneratedDEMDataset(
                batch_dir, self.file_format
            )
        return self.batch_datasets[batch_idx]

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):
        # è®¡ç®—æ‰¹æ¬¡ç´¢å¼•å’Œæ‰¹æ¬¡å†…ç´¢å¼•
        batch_idx = idx // self.samples_per_batch
        within_batch_idx = idx % self.samples_per_batch

        # è·å–å¯¹åº”æ‰¹æ¬¡çš„æ•°æ®é›†
        batch_dataset = self._get_batch_dataset(batch_idx)

        # è¿”å›æ•°æ®
        return batch_dataset[within_batch_idx]

    def __del__(self):
        # æ¸…ç†æ‰€æœ‰æ‰“å¼€çš„æ•°æ®é›†
        for dataset in self.batch_datasets.values():
            if hasattr(dataset, "__del__"):
                dataset.__del__()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":  # è®¾ä¸ºTrueè¿è¡Œæ‰¹é‡ç”Ÿæˆç¤ºä¾‹
    try:
        # æ‰¹é‡ç”Ÿæˆå¤§æ•°æ®é›†
        batch_pregenerate(
            reference_dir=r"F:\Dataset\Simulate\data0\ST2\statusarray",
            save_dir=r"F:\Dataset\Simulate\data0\ST2\pregenerated_large",
            total_samples=10000,  # ç”Ÿæˆ5ä¸‡ä¸ªæ ·æœ¬
            samples_per_batch=1000,  # æ¯æ‰¹5000ä¸ª
            local_sizes=[257],
            file_format="h5",  # ä½¿ç”¨HDF5æ ¼å¼èŠ‚çœç©ºé—´
        )  #  æ•°æ®é›†åˆ›å»ºæˆåŠŸ! æ€»æ ·æœ¬æ•°: {len(dataset)}")

        # é€‰æ‹©æ“ä½œ
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. é¢„ç”Ÿæˆå¹¶ä¿å­˜æ•°æ®é›†")
        print("2. æµ‹è¯•åŠ è½½å·²ä¿å­˜çš„æ•°æ®é›†")
        print("3. æ¯”è¾ƒä¸åŒä¿å­˜æ ¼å¼")

        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()

        if choice == "1":
            # é¢„ç”Ÿæˆå¹¶ä¿å­˜
            save_dir = (
                Path(PREGENERATE_CONFIG["save_base_dir"])
                / PREGENERATE_CONFIG["file_format"]
            )
            stats = pregenerate_and_save_dataset(
                dataset,
                save_dir,
                file_format=PREGENERATE_CONFIG["file_format"],
                num_samples=PREGENERATE_CONFIG["num_samples"],
            )

            # æµ‹è¯•åŠ è½½
            print("\n" + "=" * 60)
            test_data_loading(
                save_dir,
                PREGENERATE_CONFIG["file_format"],
                PREGENERATE_CONFIG["test_samples"],
            )

        elif choice == "2":
            # åªæµ‹è¯•åŠ è½½
            save_dir = (
                Path(PREGENERATE_CONFIG["save_base_dir"])
                / PREGENERATE_CONFIG["file_format"]
            )
            if not save_dir.exists():
                print(f" é”™è¯¯: ç›®å½• {save_dir} ä¸å­˜åœ¨!")
                print("   è¯·å…ˆè¿è¡Œé€‰é¡¹1ç”Ÿæˆæ•°æ®é›†")
            else:
                test_data_loading(
                    save_dir,
                    PREGENERATE_CONFIG["file_format"],
                    PREGENERATE_CONFIG["test_samples"],
                )

                # æµ‹è¯•å®é™…ä½¿ç”¨
                print("\n æµ‹è¯•DataLoaderå®é™…ä½¿ç”¨:")
                pre_dataset = PreGeneratedDEMDataset(
                    save_dir, PREGENERATE_CONFIG["file_format"]
                )
                dataloader = DataLoader(
                    pre_dataset, batch_size=8, shuffle=True, num_workers=2
                )

                start_time = time.time()
                for i, batch in enumerate(dataloader):
                    if i >= 10:
                        break
                    # æ¨¡æ‹Ÿè®­ç»ƒ
                    pass

                elapsed = time.time() - start_time
                print(f"   å¤„ç†10ä¸ªbatchç”¨æ—¶: {elapsed:.2f}ç§’")

        elif choice == "3":
            # æ¯”è¾ƒæ ¼å¼
            compare_results = compare_formats(
                dataset,
                PREGENERATE_CONFIG["save_base_dir"],
                num_samples=100,  # æ¯ç§æ ¼å¼æµ‹è¯•100ä¸ªæ ·æœ¬
            )

        else:
            print(" æ— æ•ˆé€‰æ‹©!")

    except Exception as e:
        print(f" é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()

    print("\n å®Œæˆ!")
"""    # ä¿®æ”¹åçš„mainå‡½æ•°
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    CONFIG = {
        "reference_dir": r"F:\Dataset\Simulate\data1\ST2\statusarray",
        "local_sizes": [257],
        "samples_per_file": 10,
        "global_coverage_range": (0.5, 1.0),
        "local_missing_range": (0.3, 0.7),
        "max_edge_missing_ratio": 0.3,
        "enable_edge_connection": True,
        "edge_connection_prob": 0.02,
        "seed": 41,
        "cache_reference_files": True,
        "visualization_samples": 0,  # é¢„ç”Ÿæˆæ—¶å…³é—­å¯è§†åŒ–
    }

    # é¢„ç”Ÿæˆé…ç½®
    PREGENERATE_CONFIG = {
        "save_base_dir": r"F:\Dataset\Simulate\data1\ST2\pregenerated",
        "num_samples": 1000,  # é¢„ç”Ÿæˆ1000ä¸ªæ ·æœ¬
        "file_format": "npz",  # å¯é€‰: 'npz', 'h5', 'pt'
        "test_samples": 20,  # æµ‹è¯•åŠ è½½çš„æ ·æœ¬æ•°
    }

    print(" é¢„ç”ŸæˆDEMæ•°æ®é›†")
    print("=" * 60)

    try:
        # åˆ›å»ºåŸå§‹æ•°æ®é›†
        print("\n åˆ›å»ºåŸå§‹æ•°æ®é›†...")
        dataset = MultiScaleDEMDataset(**CONFIG)
        print(f" æ•°æ®é›†åˆ›å»ºæˆåŠŸ! æ€»æ ·æœ¬æ•°: {len(dataset)}")

        # é€‰æ‹©æ“ä½œ
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. é¢„ç”Ÿæˆå¹¶ä¿å­˜æ•°æ®é›†")
        print("2. æµ‹è¯•åŠ è½½å·²ä¿å­˜çš„æ•°æ®é›†")
        print("3. æ¯”è¾ƒä¸åŒä¿å­˜æ ¼å¼")
        print("4. éªŒè¯æ•°æ®ä¸€è‡´æ€§")

        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3/4): ").strip()

        if choice == "1":
            # é¢„ç”Ÿæˆå¹¶ä¿å­˜
            save_dir = (
                Path(PREGENERATE_CONFIG["save_base_dir"])
                / PREGENERATE_CONFIG["file_format"]
            )
            stats = pregenerate_and_save_dataset(
                dataset,
                save_dir,
                file_format=PREGENERATE_CONFIG["file_format"],
                num_samples=PREGENERATE_CONFIG["num_samples"],
            )

            # æµ‹è¯•åŠ è½½
            print("\n" + "=" * 60)
            test_data_loading(
                save_dir,
                PREGENERATE_CONFIG["file_format"],
                PREGENERATE_CONFIG["test_samples"],
            )

        elif choice == "2":
            # åªæµ‹è¯•åŠ è½½
            save_dir = (
                Path(PREGENERATE_CONFIG["save_base_dir"])
                / PREGENERATE_CONFIG["file_format"]
            )
            if not save_dir.exists():
                print(f" é”™è¯¯: ç›®å½• {save_dir} ä¸å­˜åœ¨!")
                print("   è¯·å…ˆè¿è¡Œé€‰é¡¹1ç”Ÿæˆæ•°æ®é›†")
            else:
                test_data_loading(
                    save_dir,
                    PREGENERATE_CONFIG["file_format"],
                    PREGENERATE_CONFIG["test_samples"],
                )

                # æµ‹è¯•å®é™…ä½¿ç”¨
                print("\n æµ‹è¯•DataLoaderå®é™…ä½¿ç”¨:")
                pre_dataset = PreGeneratedDEMDataset(
                    save_dir, PREGENERATE_CONFIG["file_format"]
                )
                dataloader = DataLoader(
                    pre_dataset, batch_size=8, shuffle=True, num_workers=2
                )

                start_time = time.time()
                for i, batch in enumerate(dataloader):
                    if i >= 10:
                        break
                    # éªŒè¯æ•°æ®å½¢çŠ¶
                    (
                        local_inputs,
                        local_masks,
                        local_targets,
                        global_inputs,
                        global_masks,
                        global_targets,
                        metadata,
                    ) = batch
                    if i == 0:
                        print(f"   Batch shapes:")
                        print(f"     local_inputs: {local_inputs.shape}")
                        print(f"     local_masks: {local_masks.shape}")
                        print(f"     local_targets: {local_targets.shape}")
                        print(f"     global_inputs: {global_inputs.shape}")
                        print(f"     global_masks: {global_masks.shape}")
                        print(f"     global_targets: {global_targets.shape}")
                        print(f"     metadata: {metadata.shape}")

                elapsed = time.time() - start_time
                print(f"\n   å¤„ç†10ä¸ªbatchç”¨æ—¶: {elapsed:.2f}ç§’")

        elif choice == "3":
            # æ¯”è¾ƒæ ¼å¼
            compare_results = compare_formats(
                dataset,
                PREGENERATE_CONFIG["save_base_dir"],
                num_samples=100,  # æ¯ç§æ ¼å¼æµ‹è¯•100ä¸ªæ ·æœ¬
            )

        elif choice == "4":
            # éªŒè¯æ•°æ®ä¸€è‡´æ€§
            print("\nğŸ” éªŒè¯æ•°æ®ä¸€è‡´æ€§...")
            save_dir = (
                Path(PREGENERATE_CONFIG["save_base_dir"])
                / PREGENERATE_CONFIG["file_format"]
            )

            if not save_dir.exists():
                print(f" é”™è¯¯: ç›®å½• {save_dir} ä¸å­˜åœ¨!")
            else:
                # åˆ›å»ºé¢„ç”Ÿæˆæ•°æ®é›†
                pre_dataset = PreGeneratedDEMDataset(
                    save_dir, PREGENERATE_CONFIG["file_format"]
                )

                # éšæœºé€‰æ‹©å‡ ä¸ªæ ·æœ¬è¿›è¡Œæ¯”è¾ƒ
                test_indices = np.random.randint(
                    0, min(len(dataset), len(pre_dataset)), 5
                )

                print(f"\næ¯”è¾ƒ {len(test_indices)} ä¸ªæ ·æœ¬...")
                all_match = True

                for idx in test_indices:
                    # ä»åŸå§‹æ•°æ®é›†è·å–
                    orig_data = dataset[idx]
                    # ä»é¢„ç”Ÿæˆæ•°æ®é›†è·å–
                    pre_data = pre_dataset[idx]

                    # æ¯”è¾ƒæ¯ä¸ªç»„ä»¶
                    match = True
                    for i, (orig, pre) in enumerate(zip(orig_data, pre_data)):
                        if isinstance(orig, torch.Tensor) and isinstance(
                            pre, torch.Tensor
                        ):
                            if not torch.allclose(orig, pre, rtol=1e-5):
                                print(f"   æ ·æœ¬ {idx} çš„ç¬¬ {i} ä¸ªç»„ä»¶ä¸åŒ¹é…!")
                                match = False
                                all_match = False
                        elif isinstance(orig, np.ndarray) and isinstance(
                            pre, np.ndarray
                        ):
                            if not np.allclose(orig, pre, rtol=1e-5):
                                print(f"   æ ·æœ¬ {idx} çš„metadataä¸åŒ¹é…!")
                                match = False
                                all_match = False

                    if match:
                        print(f"    æ ·æœ¬ {idx} å®Œå…¨åŒ¹é…")

                if all_match:
                    print("\n æ‰€æœ‰æµ‹è¯•æ ·æœ¬å®Œå…¨åŒ¹é…ï¼æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡ã€‚")
                else:
                    print("\n å‘ç°æ•°æ®ä¸ä¸€è‡´ï¼")

        else:
            print(" æ— æ•ˆé€‰æ‹©!")

    except Exception as e:
        print(f" é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()

    print("\n å®Œæˆ!")"""
