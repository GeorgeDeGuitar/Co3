import os
import json
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from matplotlib import cm
import matplotlib.colors as colors
import time
import itertools
from scipy.ndimage import binary_dilation, binary_erosion, label
from skimage.morphology import disk, square, diamond
from skimage.draw import ellipse, polygon
import random
from concurrent.futures import ThreadPoolExecutor, as_completed


class EnhancedDemDataset(Dataset):
    def __init__(self, jsonDir, arrayDir, maskDir, targetDir, transform=None, 
                 enable_synthetic_masks=True, synthetic_ratio=1.0,
                 use_cache=True, cache_dir=None, num_workers_cache=4):
        """
        å¢å¼ºçš„DEMæ•°æ®é›†ï¼Œæ”¯æŒç”Ÿæˆå¼maskæ•°æ®å¢å¼º
        
        å‚æ•°:
        - enable_synthetic_masks: æ˜¯å¦å¯ç”¨ç”Ÿæˆå¼maskå¢å¼º
        - synthetic_ratio: ç”Ÿæˆå¼æ•°æ®å åŸå§‹æ•°æ®çš„æ¯”ä¾‹ï¼ˆ1.0è¡¨ç¤ºæ¯ä¸ªåŸå§‹æ•°æ®ç”Ÿæˆ1ä¸ªåˆæˆæ•°æ®ï¼‰
        - use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜åŠ é€Ÿæ•°æ®åŠ è½½
        - cache_dir: ç¼“å­˜ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç›®å½•
        - num_workers_cache: ç¼“å­˜ç”Ÿæˆæ—¶çš„å¹¶è¡Œworkeræ•°é‡
        """
        self.jsonDir = jsonDir
        self.arrayDir = arrayDir
        self.maskDir = maskDir
        self.targetDir = targetDir
        self.transform = transform
        self.enable_synthetic_masks = enable_synthetic_masks
        self.synthetic_ratio = synthetic_ratio
        self.use_cache = use_cache
        self.num_workers_cache = num_workers_cache
        
        # ç¼“å­˜è®¾ç½®
        if cache_dir is None:
            self.cache_dir = os.path.join(os.path.dirname(jsonDir), "dataset_cache")
        else:
            self.cache_dir = cache_dir
        
        # åŸå§‹æ•°æ®ç»„
        print("ğŸ” æ‰«ææ•°æ®æ–‡ä»¶...")
        self.original_data_groups = self._groupJsonFiles()
        self.original_length = len(self.original_data_groups)
        
        # å¦‚æœå¯ç”¨ç”Ÿæˆå¼maskï¼Œè®¡ç®—æ€»é•¿åº¦
        if self.enable_synthetic_masks:
            self.synthetic_length = int(self.original_length * self.synthetic_ratio)
            self.total_length = self.original_length + self.synthetic_length
        else:
            self.total_length = self.original_length
            
        self.kernel_size = 33
        self.target_size = 600
        
        # åˆå§‹åŒ–ç¼“å­˜
        if self.use_cache:
            self._init_cache()
        
        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"  åŸå§‹æ•°æ®: {self.original_length}")
        if self.enable_synthetic_masks:
            print(f"  ç”Ÿæˆå¼æ•°æ®: {self.synthetic_length}")
            print(f"  æ€»æ•°æ®é‡: {self.total_length}")
        if self.use_cache:
            print(f"  ç¼“å­˜ç›®å½•: {self.cache_dir}")

    def _init_cache(self):
        """åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ"""
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
            print(f"ğŸ“ åˆ›å»ºç¼“å­˜ç›®å½•: {self.cache_dir}")
        
        # ç¼“å­˜ç‰ˆæœ¬æ§åˆ¶
        self.cache_version_file = os.path.join(self.cache_dir, "cache_version.txt")
        self.current_version = "v1.2"  # å¢åŠ ç‰ˆæœ¬å·ä»¥æ”¯æŒæ–°çš„maskç”Ÿæˆ
        
        # æ£€æŸ¥ç¼“å­˜ç‰ˆæœ¬
        cache_valid = False
        if os.path.exists(self.cache_version_file):
            try:
                with open(self.cache_version_file, 'r') as f:
                    cached_version = f.read().strip()
                if cached_version == self.current_version:
                    cache_valid = True
                else:
                    print(f"ğŸ”„ ç¼“å­˜ç‰ˆæœ¬æ›´æ–° ({cached_version} -> {self.current_version})")
            except:
                pass
        
        if not cache_valid:
            print("ğŸš€ ç”Ÿæˆæ•°æ®ç¼“å­˜ä»¥åŠ é€Ÿåç»­åŠ è½½...")
            self._build_cache()
            with open(self.cache_version_file, 'w') as f:
                f.write(self.current_version)
    
    def _build_cache(self):
        """æ„å»ºæ•°æ®ç¼“å­˜"""
        start_time = time.time()
        cache_metadata_file = os.path.join(self.cache_dir, "metadata.npy")
        
        print(f"ğŸ“¦ å¼€å§‹æ„å»ºç¼“å­˜ï¼Œé¢„è®¡éœ€è¦å¤„ç† {self.original_length} ä¸ªåŸå§‹æ•°æ®...")
        
        # åˆ›å»ºå­ç›®å½•
        subdirs = ['arrays', 'masks', 'targets', 'metadata']
        for subdir in subdirs:
            subdir_path = os.path.join(self.cache_dir, subdir)
            if not os.path.exists(subdir_path):
                os.makedirs(subdir_path)
        
        # å¹¶è¡Œå¤„ç†æ•°æ®
        def cache_single_item(idx):
            try:
                data_dict = self._get_original_data_no_cache(idx)
                
                # ä¿å­˜åˆ°ç¼“å­˜
                cache_prefix = os.path.join(self.cache_dir, f"item_{idx:06d}")
                
                np.save(f"{cache_prefix}_input_data.npy", 
                       np.stack(data_dict['input_data'], axis=0).astype(np.float32))
                np.save(f"{cache_prefix}_input_target.npy", 
                       data_dict['input_target'].astype(np.float32))
                np.save(f"{cache_prefix}_id_array.npy", 
                       data_dict['id_array'].astype(np.float32))
                np.save(f"{cache_prefix}_id_mask.npy", 
                       data_dict['id_mask'].astype(np.float32))
                np.save(f"{cache_prefix}_id_target.npy", 
                       data_dict['id_target'].astype(np.float32))
                np.save(f"{cache_prefix}_metadata.npy", 
                       data_dict['metadata'].astype(np.float32))
                
                return idx, True
            except Exception as e:
                print(f"âŒ ç¼“å­˜ç¬¬ {idx} ä¸ªæ•°æ®å¤±è´¥: {e}")
                return idx, False
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        successful_items = []
        with ThreadPoolExecutor(max_workers=self.num_workers_cache) as executor:
            futures = {executor.submit(cache_single_item, idx): idx 
                      for idx in range(self.original_length)}
            
            for future in as_completed(futures):
                idx, success = future.result()
                if success:
                    successful_items.append(idx)
                
                # æ˜¾ç¤ºè¿›åº¦
                if len(successful_items) % 100 == 0:
                    elapsed = time.time() - start_time
                    progress = len(successful_items) / self.original_length
                    eta = elapsed / progress - elapsed if progress > 0 else 0
                    print(f"â³ ç¼“å­˜è¿›åº¦: {len(successful_items)}/{self.original_length} "
                          f"({progress*100:.1f}%) ETA: {eta:.1f}s")
        
        # ä¿å­˜æˆåŠŸçš„ç´¢å¼•åˆ—è¡¨
        np.save(cache_metadata_file, np.array(successful_items))
        
        elapsed = time.time() - start_time
        print(f"âœ… ç¼“å­˜æ„å»ºå®Œæˆ! è€—æ—¶: {elapsed:.1f}s, æˆåŠŸ: {len(successful_items)}/{self.original_length}")
    
    def _get_original_data_no_cache(self, idx):
        """è·å–åŸå§‹æ•°æ®ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰- ç”¨äºæ„å»ºç¼“å­˜"""
        return self._get_original_data_internal(idx)

    def _groupJsonFiles(self):
        """åŸå§‹çš„æ•°æ®åˆ†ç»„é€»è¾‘ï¼Œä¿æŒä¸å˜"""
        groups = {}
        for subfolder in os.listdir(self.jsonDir):
            subfolder_path = os.path.join(self.jsonDir, subfolder)
            print(f"processing folder: {subfolder}")
            if not os.path.isdir(subfolder_path):
                continue

            for filename in os.listdir(subfolder_path):
                if filename.endswith(".json"):
                    parts = filename.split("_")
                    if len(parts) < 4:
                        continue

                    centerx, centery, id_val, id_adj = (
                        parts[0], parts[1], parts[2], parts[3].split(".")[0]
                    )
                    key = (centerx, centery, id_val, id_adj)

                    if key not in groups:
                        groups[key] = {
                            "json_files": [],
                            "meta": {
                                "centerx": centerx,
                                "centery": centery,
                                "id": id_val,
                            },
                        }

                    json_path = os.path.join(subfolder_path, filename)
                    groups[key]["json_files"].append(json_path)

                    # è·å–å…¨å±€æ•°æ®ä¿¡æ¯
                    for filename in os.listdir(self.arrayDir):
                        if filename.endswith("_a.npy") and filename.startswith(f"{id_val}_"):
                            parts = filename.split("_")
                            if len(parts) >= 6:
                                id_val = parts[0]
                                xbegain = parts[1]
                                ybegain = parts[2]
                                xnum = parts[3]
                                ynum = parts[4]
                                if "xbegain" not in groups[key]["meta"]:
                                    groups[key]["meta"]["xbegain"] = xbegain
                                    groups[key]["meta"]["ybegain"] = ybegain
                                    groups[key]["meta"]["xnum"] = xnum
                                    groups[key]["meta"]["ynum"] = ynum
                                    groups[key]["meta"]["id"] = id_val
        return groups

    def generate_clustered_mask(self, shape=(33, 33), 
                              missing_ratio_range=(0.0, 0.2),
                              num_clusters_range=(1, 2),
                              cluster_size_range=(8, 18),
                              scattered_probability=0.2):
        """
        ç”Ÿæˆç¼ºå¤±maskï¼Œæ”¯æŒé›†ä¸­å’Œåˆ†æ•£åˆ†å¸ƒ
        
        å‚æ•°:
        - shape: maskå½¢çŠ¶
        - missing_ratio_range: ç¼ºå¤±åƒç´ æ¯”ä¾‹èŒƒå›´
        - num_clusters_range: ç¼ºå¤±åŒºåŸŸç°‡æ•°é‡èŒƒå›´ï¼ˆ1-2ä¸ªç°‡ï¼‰
        - cluster_size_range: æ¯ä¸ªç°‡çš„å¤§å°èŒƒå›´
        - scattered_probability: ç”Ÿæˆåˆ†æ•£åˆ†å¸ƒmaskçš„æ¦‚ç‡ï¼ˆ0.2è¡¨ç¤º20%æ¦‚ç‡ï¼‰
        
        è¿”å›:
        - mask: 0è¡¨ç¤ºç¼ºå¤±ï¼Œ1è¡¨ç¤ºæœ‰æ•ˆ
        """
        # å†³å®šç”Ÿæˆé›†ä¸­è¿˜æ˜¯åˆ†æ•£çš„mask
        if np.random.random() < scattered_probability:
            # ç”Ÿæˆåˆ†æ•£åˆ†å¸ƒçš„mask
            return self._generate_scattered_mask(shape, missing_ratio_range)
        else:
            # ç”Ÿæˆé›†ä¸­åˆ†å¸ƒçš„maskï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            return self._generate_concentrated_mask_pattern(
                shape, missing_ratio_range, num_clusters_range, cluster_size_range
            )
    
    def _generate_concentrated_mask_pattern(self, shape, missing_ratio_range, 
                                          num_clusters_range, cluster_size_range):
        """ç”Ÿæˆé›†ä¸­åˆ†å¸ƒçš„maskæ¨¡å¼"""
        h, w = shape
        mask = np.ones(shape, dtype=np.float32)
        
        # éšæœºç¡®å®šç¼ºå¤±æ¯”ä¾‹å’Œç°‡æ•°é‡
        target_missing_ratio = np.random.uniform(*missing_ratio_range)
        num_clusters = np.random.randint(*num_clusters_range)
        
        total_pixels = h * w
        target_missing_pixels = int(total_pixels * target_missing_ratio)
        
        for cluster_idx in range(num_clusters):
            # ä¸ºæ¯ä¸ªç°‡åˆ†é…åƒç´ æ•°
            if cluster_idx == num_clusters - 1:
                # æœ€åä¸€ä¸ªç°‡è·å¾—å‰©ä½™æ‰€æœ‰åƒç´ 
                cluster_pixels = target_missing_pixels - np.sum(mask == 0)
            else:
                # å‰é¢çš„ç°‡å¹³å‡åˆ†é…
                cluster_pixels = target_missing_pixels // num_clusters
            
            if cluster_pixels <= 0:
                continue
                
            # é€‰æ‹©ç°‡ä¸­å¿ƒï¼Œé¿å…å¤ªé è¿‘è¾¹ç¼˜
            margin = 5
            center_x = np.random.randint(margin, h - margin)
            center_y = np.random.randint(margin, w - margin)
            
            # ç”Ÿæˆé›†ä¸­çš„ç°‡
            cluster_mask = self._generate_concentrated_cluster(
                shape, center_x, center_y, 
                cluster_size_range[1], cluster_pixels
            )
            
            # åº”ç”¨å½¢æ€å­¦æ“ä½œç¡®ä¿é›†ä¸­æ€§
            cluster_mask = self._apply_morphological_operations(cluster_mask)
            
            # åº”ç”¨åˆ°ä¸»mask
            mask[cluster_mask] = 0
        
        return mask
    
    def _generate_scattered_mask(self, shape, missing_ratio_range):
        """
        ç”Ÿæˆåˆ†æ•£åˆ†å¸ƒçš„mask
        
        å‚æ•°:
        - shape: maskå½¢çŠ¶
        - missing_ratio_range: ç¼ºå¤±åƒç´ æ¯”ä¾‹èŒƒå›´
        
        è¿”å›:
        - mask: 0è¡¨ç¤ºç¼ºå¤±ï¼Œ1è¡¨ç¤ºæœ‰æ•ˆ
        """
        h, w = shape
        mask = np.ones(shape, dtype=np.float32)
        
        # éšæœºç¡®å®šç¼ºå¤±æ¯”ä¾‹
        target_missing_ratio = np.random.uniform(*missing_ratio_range)
        total_pixels = h * w
        target_missing_pixels = int(total_pixels * target_missing_ratio)
        
        # æ–¹æ³•1: éšæœºåƒç´ åˆ†æ•£ç¼ºå¤± (30%æ¦‚ç‡)
        if np.random.random() < 0.3:
            # å®Œå…¨éšæœºåˆ†æ•£
            all_positions = [(i, j) for i in range(h) for j in range(w)]
            missing_positions = np.random.choice(
                len(all_positions), target_missing_pixels, replace=False
            )
            for idx in missing_positions:
                i, j = all_positions[idx]
                mask[i, j] = 0
                
        # æ–¹æ³•2: å¤šä¸ªå°ç°‡åˆ†æ•£åˆ†å¸ƒ (40%æ¦‚ç‡)
        elif np.random.random() < 0.7:
            num_small_clusters = np.random.randint(5, 12)  # 5-11ä¸ªå°ç°‡
            pixels_per_cluster = target_missing_pixels // num_small_clusters
            remaining_pixels = target_missing_pixels % num_small_clusters
            
            for cluster_idx in range(num_small_clusters):
                cluster_pixels = pixels_per_cluster
                if cluster_idx < remaining_pixels:
                    cluster_pixels += 1
                    
                if cluster_pixels <= 0:
                    continue
                
                # éšæœºé€‰æ‹©å°ç°‡ä¸­å¿ƒ
                center_x = np.random.randint(2, h - 2)
                center_y = np.random.randint(2, w - 2)
                
                # ç”Ÿæˆå°çš„åœ†å½¢æˆ–æ–¹å½¢ç°‡
                if np.random.random() < 0.5:
                    # å°åœ†å½¢
                    radius = min(3, np.sqrt(cluster_pixels / np.pi))
                    y, x = np.ogrid[:h, :w]
                    dist = np.sqrt((x - center_y)**2 + (y - center_x)**2)
                    small_cluster = dist <= radius
                else:
                    # å°æ–¹å½¢
                    size = min(3, int(np.sqrt(cluster_pixels)))
                    x1 = max(0, center_x - size//2)
                    x2 = min(h, center_x + size//2 + 1)
                    y1 = max(0, center_y - size//2)
                    y2 = min(w, center_y + size//2 + 1)
                    small_cluster = np.zeros(shape, dtype=bool)
                    small_cluster[x1:x2, y1:y2] = True
                
                mask[small_cluster] = 0
                
        # æ–¹æ³•3: æ¡å¸¦çŠ¶åˆ†æ•£ç¼ºå¤± (30%æ¦‚ç‡)
        else:
            # ç”Ÿæˆéšæœºæ¡å¸¦
            num_stripes = np.random.randint(3, 8)  # 3-7æ¡
            pixels_per_stripe = target_missing_pixels // num_stripes
            
            for stripe_idx in range(num_stripes):
                if np.random.random() < 0.5:
                    # æ°´å¹³æ¡å¸¦
                    stripe_height = np.random.randint(1, 4)
                    stripe_y = np.random.randint(0, h - stripe_height)
                    stripe_x_start = np.random.randint(0, w // 3)
                    stripe_x_end = np.random.randint(w // 3 * 2, w)
                    mask[stripe_y:stripe_y+stripe_height, stripe_x_start:stripe_x_end] = 0
                else:
                    # å‚ç›´æ¡å¸¦
                    stripe_width = np.random.randint(1, 4)
                    stripe_x = np.random.randint(0, w - stripe_width)
                    stripe_y_start = np.random.randint(0, h // 3)
                    stripe_y_end = np.random.randint(h // 3 * 2, h)
                    mask[stripe_y_start:stripe_y_end, stripe_x:stripe_x+stripe_width] = 0
        
        # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡ç¼ºå¤±åƒç´ æ•°
        current_missing = np.sum(mask == 0)
        if current_missing > target_missing_pixels:
            # éšæœºæ¢å¤ä¸€äº›åƒç´ 
            missing_positions = np.where(mask == 0)
            excess = current_missing - target_missing_pixels
            restore_indices = np.random.choice(len(missing_positions[0]), excess, replace=False)
            mask[missing_positions[0][restore_indices], missing_positions[1][restore_indices]] = 1
        elif current_missing < target_missing_pixels:
            # éšæœºæ·»åŠ ä¸€äº›ç¼ºå¤±åƒç´ 
            valid_positions = np.where(mask == 1)
            deficit = target_missing_pixels - current_missing
            if len(valid_positions[0]) > 0:
                add_indices = np.random.choice(
                    len(valid_positions[0]), 
                    min(deficit, len(valid_positions[0])), 
                    replace=False
                )
                mask[valid_positions[0][add_indices], valid_positions[1][add_indices]] = 0
        
        return mask

    def _generate_concentrated_cluster(self, shape, center_x, center_y, max_radius, target_pixels):
        """
        ç”Ÿæˆé›†ä¸­çš„ä¸è§„åˆ™å½¢çŠ¶ç¼ºå¤±ç°‡
        
        å‚æ•°:
        - shape: æ€»å½¢çŠ¶
        - center_x, center_y: ç°‡ä¸­å¿ƒåæ ‡
        - max_radius: æœ€å¤§åŠå¾„
        - target_pixels: ç›®æ ‡åƒç´ æ•°é‡
        """
        # æ–¹æ³•1: æ¤­åœ†å½¢ç¼ºå¤±åŒºåŸŸ (40%æ¦‚ç‡)
        if np.random.random() < 0.2:
            cluster_mask = self._generate_ellipse_cluster(
                shape, center_x, center_y, max_radius, target_pixels
            )
        
        # æ–¹æ³•2: å¤šè¾¹å½¢ç¼ºå¤±åŒºåŸŸ (30%æ¦‚ç‡)
        elif np.random.random() < 0.2:
            cluster_mask = self._generate_polygon_cluster(
                shape, center_x, center_y, max_radius, target_pixels
            )
        
        # æ–¹æ³•3: è¿é€šåŒºåŸŸç”Ÿé•¿ (30%æ¦‚ç‡)
        else:
            cluster_mask = self._generate_growth_cluster(
                shape, center_x, center_y, max_radius, target_pixels
            )
        
        return cluster_mask

    def _generate_ellipse_cluster(self, shape, center_x, center_y, max_radius, target_pixels):
        """ç”Ÿæˆæ¤­åœ†å½¢ç¼ºå¤±åŒºåŸŸ"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)
        
        # ä¼°ç®—æ¤­åœ†å‚æ•°
        estimated_area = target_pixels
        
        # éšæœºæ¤­åœ†é•¿çŸ­è½´æ¯”ä¾‹
        aspect_ratio = np.random.uniform(0.4, 1.0)  # é•¿çŸ­è½´æ¯”ä¾‹
        
        # è®¡ç®—åŠé•¿è½´å’ŒåŠçŸ­è½´
        semi_major = min(max_radius * 0.8, np.sqrt(estimated_area / (np.pi * aspect_ratio)))
        semi_minor = semi_major * aspect_ratio
        
        # é™åˆ¶åœ¨å›¾åƒèŒƒå›´å†…
        semi_major = min(semi_major, min(h//3, w//3))
        semi_minor = min(semi_minor, min(h//3, w//3))
        
        try:
            # ç”Ÿæˆæ¤­åœ†
            rr, cc = ellipse(center_x, center_y, semi_major, semi_minor, shape=shape)
            cluster_mask[rr, cc] = True
        except:
            # å¦‚æœæ¤­åœ†ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°åœ†å½¢
            y, x = np.ogrid[:h, :w]
            dist_from_center = np.sqrt((x - center_y)**2 + (y - center_x)**2)
            radius = min(max_radius * 0.6, np.sqrt(target_pixels / np.pi))
            cluster_mask = dist_from_center <= radius
        
        return cluster_mask

    def _generate_polygon_cluster(self, shape, center_x, center_y, max_radius, target_pixels):
        """ç”Ÿæˆå¤šè¾¹å½¢ç¼ºå¤±åŒºåŸŸ"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)
        
        # ç”Ÿæˆä¸è§„åˆ™å¤šè¾¹å½¢é¡¶ç‚¹
        num_vertices = np.random.randint(5, 8)  # 5-7ä¸ªé¡¶ç‚¹
        
        # ä¼°ç®—åˆé€‚çš„åŠå¾„
        estimated_radius = min(max_radius * 0.7, np.sqrt(target_pixels / np.pi) * 1.1)
        
        # ç”Ÿæˆé¡¶ç‚¹
        angles = np.linspace(0, 2*np.pi, num_vertices, endpoint=False)
        vertices_x = []
        vertices_y = []
        
        for angle in angles:
            # éšæœºåŒ–åŠå¾„ï¼Œåˆ›é€ ä¸è§„åˆ™å½¢çŠ¶
            radius_variation = np.random.uniform(0.7, 1.3)
            current_radius = estimated_radius * radius_variation
            
            x = center_x + current_radius * np.cos(angle)
            y = center_y + current_radius * np.sin(angle)
            
            # ç¡®ä¿é¡¶ç‚¹åœ¨å›¾åƒèŒƒå›´å†…
            x = np.clip(x, 1, h-2)
            y = np.clip(y, 1, w-2)
            
            vertices_x.append(x)
            vertices_y.append(y)
        
        try:
            # ç”Ÿæˆå¤šè¾¹å½¢
            rr, cc = polygon(vertices_x, vertices_y, shape=shape)
            cluster_mask[rr, cc] = True
        except:
            # å¦‚æœå¤šè¾¹å½¢ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°åœ†å½¢
            y, x = np.ogrid[:h, :w]
            dist_from_center = np.sqrt((x - center_y)**2 + (y - center_x)**2)
            radius = min(max_radius * 0.6, np.sqrt(target_pixels / np.pi))
            cluster_mask = dist_from_center <= radius
        
        return cluster_mask

    def _generate_growth_cluster(self, shape, center_x, center_y, max_radius, target_pixels):
        """ä½¿ç”¨åŒºåŸŸç”Ÿé•¿æ–¹æ³•ç”Ÿæˆé›†ä¸­çš„ç¼ºå¤±åŒºåŸŸ"""
        h, w = shape
        cluster_mask = np.zeros(shape, dtype=bool)
        
        # ä»ä¸­å¿ƒç‚¹å¼€å§‹
        if 0 <= center_x < h and 0 <= center_y < w:
            cluster_mask[center_x, center_y] = True
            current_pixels = 1
        else:
            return cluster_mask
        
        # å®šä¹‰é‚»åŸŸ(8è¿é€š)
        neighbors = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]
        
        # åŒºåŸŸç”Ÿé•¿
        max_iterations = target_pixels * 2  # é˜²æ­¢æ— é™å¾ªç¯
        iteration = 0
        
        while current_pixels < target_pixels and iteration < max_iterations:
            iteration += 1
            
            # æ‰¾åˆ°å½“å‰åŒºåŸŸçš„è¾¹ç•Œç‚¹
            boundary_points = []
            
            for i in range(max(0, center_x - max_radius), min(h, center_x + max_radius + 1)):
                for j in range(max(0, center_y - max_radius), min(w, center_y + max_radius + 1)):
                    if cluster_mask[i, j]:
                        # æ£€æŸ¥8é‚»åŸŸ
                        for di, dj in neighbors:
                            ni, nj = i + di, j + dj
                            if (0 <= ni < h and 0 <= nj < w and 
                                not cluster_mask[ni, nj]):
                                
                                # è®¡ç®—è·ç¦»ä¸­å¿ƒçš„è·ç¦»
                                dist = np.sqrt((ni - center_x)**2 + (nj - center_y)**2)
                                if dist <= max_radius:
                                    boundary_points.append((ni, nj, dist))
            
            if not boundary_points:
                break
            
            # å»é‡
            boundary_points = list(set(boundary_points))
            
            # æŒ‰è·ç¦»ä¸­å¿ƒçš„è·ç¦»æ’åºï¼Œä¼˜å…ˆé€‰æ‹©è·ç¦»ä¸­å¿ƒè¿‘çš„ç‚¹
            boundary_points.sort(key=lambda x: x[2])
            
            # è®¡ç®—æœ¬æ¬¡è¦æ·»åŠ çš„ç‚¹æ•°
            remaining_pixels = target_pixels - current_pixels
            num_to_add = min(len(boundary_points), 
                           max(1, remaining_pixels // 5),
                           remaining_pixels)
            
            # é€‰æ‹©è·ç¦»ä¸­å¿ƒæœ€è¿‘çš„ç‚¹è¿›è¡Œæ‰©å±•
            selected_points = boundary_points[:num_to_add]
            
            # æ·»åŠ é€‰ä¸­çš„ç‚¹
            for ni, nj, _ in selected_points:
                if current_pixels >= target_pixels:
                    break
                cluster_mask[ni, nj] = True
                current_pixels += 1
        
        return cluster_mask

    def _apply_morphological_operations(self, mask):
        """
        åº”ç”¨å½¢æ€å­¦æ“ä½œæ¥è¿›ä¸€æ­¥é›†ä¸­mask
        """
        # é—­è¿ç®—ï¼šå¡«å……å°æ´
        kernel_close = disk(2)
        mask_closed = binary_dilation(mask, kernel_close)
        mask_closed = binary_erosion(mask_closed, kernel_close)
        
        # å»é™¤å°çš„åˆ†ç¦»åŒºåŸŸï¼Œåªä¿ç•™æœ€å¤§çš„è¿é€šåŒºåŸŸ
        labeled_mask, num_labels = label(mask_closed)
        if num_labels > 1:
            # æ‰¾åˆ°æœ€å¤§çš„è¿é€šåŒºåŸŸ
            region_sizes = []
            for i in range(1, num_labels + 1):
                size = np.sum(labeled_mask == i)
                region_sizes.append((size, i))
            
            # ä¿ç•™æœ€å¤§çš„1ä¸ªåŒºåŸŸ
            region_sizes.sort(reverse=True)
            keep_regions = 1
            
            new_mask = np.zeros_like(mask_closed)
            for i in range(min(keep_regions, len(region_sizes))):
                _, region_id = region_sizes[i]
                new_mask[labeled_mask == region_id] = True
            
            mask_closed = new_mask
        
        return mask_closed

    def apply_mask_to_data(self, target_data, mask):
        """
        å°†maskåº”ç”¨åˆ°ç›®æ ‡æ•°æ®ä¸Šç”Ÿæˆæ–°çš„è¾“å…¥æ•°æ®
        
        å‚æ•°:
        - target_data: åŸå§‹ç›®æ ‡æ•°æ® (33, 33)
        - mask: ç”Ÿæˆçš„mask (33, 33), 0è¡¨ç¤ºç¼ºå¤±
        
        è¿”å›:
        - masked_data: åº”ç”¨maskåçš„æ•°æ®
        - applied_mask: å®é™…åº”ç”¨çš„mask
        """
        masked_data = target_data.copy()
        
        # å°†maskä¸º0çš„ä½ç½®è®¾ä¸ºç©ºå€¼(100.0)
        empty_value = 100.0
        masked_data[mask == 0] = empty_value
        
        # ç”Ÿæˆå¯¹åº”çš„è¾“å…¥mask (1è¡¨ç¤ºæœ‰æ•ˆï¼Œ0è¡¨ç¤ºæ— æ•ˆ)
        applied_mask = mask.copy()
        
        return masked_data, applied_mask

    def __len__(self):
        return self.total_length

    def preprocess_data(self, data):
        """åŸå§‹çš„æ•°æ®é¢„å¤„ç†é€»è¾‘"""
        mask = data != 100
        valid_data = data[mask]
        if valid_data.size > 0:
            mean = valid_data.mean()
            data[~mask] = mean
        return data, mask.astype(np.float32)

    def _get_original_data(self, idx):
        """è·å–åŸå§‹æ•°æ®çš„é€»è¾‘ï¼Œæ”¯æŒç¼“å­˜åŠ é€Ÿ"""
        if self.use_cache:
            return self._get_original_data_cached(idx)
        else:
            return self._get_original_data_internal(idx)
    
    def _get_original_data_cached(self, idx):
        """ä»ç¼“å­˜åŠ è½½åŸå§‹æ•°æ®"""
        try:
            cache_prefix = os.path.join(self.cache_dir, f"item_{idx:06d}")
            
            # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = [
                f"{cache_prefix}_input_data.npy",
                f"{cache_prefix}_input_target.npy", 
                f"{cache_prefix}_id_array.npy",
                f"{cache_prefix}_id_mask.npy",
                f"{cache_prefix}_id_target.npy",
                f"{cache_prefix}_metadata.npy"
            ]
            
            if not all(os.path.exists(f) for f in required_files):
                # ç¼“å­˜æ–‡ä»¶ä¸å®Œæ•´ï¼Œå›é€€åˆ°ç›´æ¥åŠ è½½
                return self._get_original_data_internal(idx)
            
            # ä»ç¼“å­˜åŠ è½½
            input_data_stack = np.load(f"{cache_prefix}_input_data.npy", mmap_mode='r')
            input_data = [input_data_stack[i] for i in range(input_data_stack.shape[0])]
            
            data_dict = {
                'input_data': input_data,
                'input_target': np.load(f"{cache_prefix}_input_target.npy", mmap_mode='r'),
                'id_array': np.load(f"{cache_prefix}_id_array.npy", mmap_mode='r'),
                'id_mask': np.load(f"{cache_prefix}_id_mask.npy", mmap_mode='r'),
                'id_target': np.load(f"{cache_prefix}_id_target.npy", mmap_mode='r'),
                'metadata': np.load(f"{cache_prefix}_metadata.npy", mmap_mode='r'),
            }
            
            # è®¡ç®—å±€éƒ¨åŒºåŸŸåæ ‡
            metadata = data_dict['metadata']
            centerx, centery, xbegain, ybegain = metadata[:4]
            minx = int(centerx - xbegain) - self.kernel_size // 2
            maxx = int(centerx - xbegain) + self.kernel_size // 2 + 1
            miny = int(centery - ybegain) - self.kernel_size // 2
            maxy = int(centery - ybegain) + self.kernel_size // 2 + 1
            
            # è®¡ç®—mask_zero_values
            mask_zero_values = data_dict['id_array'][data_dict['id_mask'] == 0]
            mask_zero_values = mask_zero_values[0] if len(mask_zero_values) > 0 else 0
            
            data_dict.update({
                'minx': minx, 'maxx': maxx, 'miny': miny, 'maxy': maxy,
                'mask_zero_values': mask_zero_values
            })
            
            return data_dict
            
        except Exception as e:
            print(f"âš ï¸  ç¼“å­˜åŠ è½½å¤±è´¥ (idx={idx}): {e}, å›é€€åˆ°ç›´æ¥åŠ è½½")
            return self._get_original_data_internal(idx)
    
    def _get_original_data_internal(self, idx):
        """è·å–åŸå§‹æ•°æ®çš„å†…éƒ¨å®ç°ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        key = next(itertools.islice(self.original_data_groups.keys(), idx, idx + 1))
        group_data = self.original_data_groups[key]
        json_files = group_data["json_files"]
        meta = group_data["meta"]

        # æå–å…ƒæ•°æ®
        centerx = int(meta["centerx"])
        centery = int(meta["centery"])
        id_val = int(meta["id"])
        xbegain = int(meta.get("xbegain", 0))
        ybegain = int(meta.get("ybegain", 0))
        xnum = int(meta.get("xnum", 0))
        ynum = int(meta.get("ynum", 0))

        metadata = np.array([
            float(centerx), float(centery), float(xbegain), 
            float(ybegain), float(xnum), float(ynum), float(id_val)
        ])

        # è¯»å–è¾“å…¥æ•°æ®
        input_data = []
        for json_path in json_files:
            with open(json_path, "r", encoding="utf-8") as f:
                json_content = json.load(f)
                input_data.append(np.array(json_content["data"]))

        # è¯»å–å…¨å±€æ•°æ®
        array_filename = f"{id_val}_{xbegain}_{ybegain}_{xnum}_{ynum}_a.npy"
        array_path = os.path.join(self.arrayDir, array_filename)
        if os.path.exists(array_path):
            id_array = np.load(array_path, mmap_mode='r').astype(np.float32)
        else:
            print(f"Warning: Array file {array_path} not found.")
            id_array = np.zeros((xnum, ynum))

        # è¯»å–å…¨å±€mask
        mask_filename = f"{id_val}_{xbegain}_{ybegain}_{xnum}_{ynum}_m.npy"
        mask_path = os.path.join(self.maskDir, mask_filename)
        if os.path.exists(mask_path):
            id_mask = np.load(mask_path).astype(np.float32)
        else:
            print(f"Warning: Mask file {mask_path} not found.")
            id_mask = np.zeros((xnum, ynum))

        # è¯»å–ç›®æ ‡æ•°æ®
        target_filename = None
        for filename in os.listdir(self.targetDir):
            if filename.startswith(f"rs_{id_val}_") and filename.endswith(".npy"):
                target_filename = filename
                break
        
        if target_filename:
            target_path = os.path.join(self.targetDir, target_filename)
            id_target = np.load(target_path, mmap_mode='r').astype(np.float32)
        else:
            print(f"Warning: Target file not found for id {id_val}")
            id_target = np.zeros((int(xnum), int(ynum)))

        # æå–å±€éƒ¨target
        minx = int(centerx - xbegain) - self.kernel_size // 2
        maxx = int(centerx - xbegain) + self.kernel_size // 2 + 1
        miny = int(centery - ybegain) - self.kernel_size // 2
        maxy = int(centery - ybegain) + self.kernel_size // 2 + 1
        
        input_target = id_target[minx:maxx, miny:maxy].copy()

        # å‰”é™¤å±€éƒ¨åŒºåŸŸä»å…¨å±€æ•°æ®
        mask_zero_values = id_array[id_mask == 0][0] if np.any(id_mask == 0) else 0
        id_array[minx:maxx, miny:maxy] = mask_zero_values
        id_mask[minx:maxx, miny:maxy] = 0

        return {
            'input_data': input_data,
            'input_target': input_target,
            'id_array': id_array,
            'id_mask': id_mask,
            'id_target': id_target,
            'metadata': metadata,
            'minx': minx, 'maxx': maxx, 'miny': miny, 'maxy': maxy,
            'mask_zero_values': mask_zero_values
        }

    def __getitem__(self, idx):
        if idx < self.original_length:
            # è¿”å›åŸå§‹æ•°æ®
            return self._get_original_item(idx)
        else:
            # è¿”å›ç”Ÿæˆå¼maskæ•°æ®
            return self._get_synthetic_item(idx)

    def _get_original_item(self, idx):
        """è·å–åŸå§‹æ•°æ®é¡¹"""
        data_dict = self._get_original_data(idx)
        
        # å¤„ç†è¾“å…¥æ•°æ®
        input_data = np.stack(data_dict['input_data'], axis=0)
        merged_input_data, _ = merge_input_channels(input_data, empty_value=100)
        input_data, input_masks = self.preprocess_data(merged_input_data)

        return (
            torch.tensor(input_data, dtype=torch.float32),
            torch.tensor(input_masks, dtype=torch.int),
            torch.tensor(data_dict['input_target'], dtype=torch.float32),
            torch.tensor(data_dict['id_array'], dtype=torch.float32),
            torch.tensor(data_dict['id_mask'], dtype=torch.int),
            torch.tensor(data_dict['id_target'], dtype=torch.float32),
            data_dict['metadata'].astype(int),
        )

    def _get_synthetic_item(self, idx):
        """ç”ŸæˆåŸºäºç”Ÿæˆå¼maskçš„æ•°æ®é¡¹"""
        # ä»åŸå§‹æ•°æ®ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªä½œä¸ºåŸºç¡€
        original_idx = (idx - self.original_length) % self.original_length
        data_dict = self._get_original_data(original_idx)
        
        # ç”Ÿæˆæ··åˆåˆ†å¸ƒçš„mask
        synthetic_mask = self.generate_clustered_mask(
            shape=(self.kernel_size, self.kernel_size),
            missing_ratio_range=(0.25, 0.75),  # 25%-75%çš„ç¼ºå¤±æ¯”ä¾‹
            num_clusters_range=(1, 2),         # 1-2ä¸ªç¼ºå¤±ç°‡
            cluster_size_range=(6, 15),        # ç°‡å¤§å°èŒƒå›´
            scattered_probability=0.2          # 20%æ¦‚ç‡ç”Ÿæˆåˆ†æ•£mask
        )
        
        # å°†maskåº”ç”¨åˆ°ç›®æ ‡æ•°æ®ä¸Šç”Ÿæˆæ–°çš„è¾“å…¥
        synthetic_input_data, synthetic_input_mask = self.apply_mask_to_data(
            data_dict['input_target'], synthetic_mask
        )
        
        # é¢„å¤„ç†ç”Ÿæˆçš„è¾“å…¥æ•°æ®
        synthetic_input_data = synthetic_input_data[np.newaxis, :]  # æ·»åŠ é€šé“ç»´åº¦
        processed_input, processed_mask = self.preprocess_data(synthetic_input_data)
        
        # ä¿®æ”¹å…¨å±€æ•°æ®ä»¥åæ˜ æ–°çš„å±€éƒ¨è¾“å…¥
        modified_id_array = data_dict['id_array'].copy()
        modified_id_mask = data_dict['id_mask'].copy()
        
        # åœ¨å…¨å±€æ•°ç»„ä¸­åµŒå…¥ç”Ÿæˆçš„å±€éƒ¨æ•°æ®
        minx, maxx = data_dict['minx'], data_dict['maxx']
        miny, maxy = data_dict['miny'], data_dict['maxy']
        
        # å°†ç”Ÿæˆçš„è¾“å…¥æ•°æ®åµŒå…¥åˆ°å…¨å±€æ•°ç»„ä¸­
        modified_id_array[minx:maxx, miny:maxy] = synthetic_input_data.squeeze()
        modified_id_mask[minx:maxx, miny:maxy] = synthetic_input_mask

        return (
            torch.tensor(processed_input, dtype=torch.float32),
            torch.tensor(processed_mask, dtype=torch.int),
            torch.tensor(data_dict['input_target'], dtype=torch.float32),  # ç›®æ ‡ä¿æŒä¸å˜
            torch.tensor(modified_id_array, dtype=torch.float32),
            torch.tensor(modified_id_mask, dtype=torch.int),
            torch.tensor(data_dict['id_target'], dtype=torch.float32),
            data_dict['metadata'].astype(int),
        )

    def visualize_synthetic_mask(self, idx_list=None, save_dir=None):
        """
        å¯è§†åŒ–ç”Ÿæˆçš„maskæ•ˆæœ
        
        å‚æ•°:
        - idx_list: è¦å¯è§†åŒ–çš„ç´¢å¼•åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™éšæœºé€‰æ‹©
        - save_dir: ä¿å­˜å›¾åƒçš„ç›®å½•
        """
        if idx_list is None:
            # éšæœºé€‰æ‹©ä¸€äº›åˆæˆæ•°æ®è¿›è¡Œå¯è§†åŒ–
            synthetic_indices = list(range(self.original_length, min(self.original_length + 8, self.total_length)))
        else:
            synthetic_indices = [idx for idx in idx_list if idx >= self.original_length]
        
        if not synthetic_indices:
            print("æ²¡æœ‰åˆæˆæ•°æ®å¯ä¾›å¯è§†åŒ–")
            return
            
        fig, axes = plt.subplots(len(synthetic_indices), 4, figsize=(16, 4 * len(synthetic_indices)))
        if len(synthetic_indices) == 1:
            axes = axes.reshape(1, -1)
        
        for i, idx in enumerate(synthetic_indices):
            # è·å–åˆæˆæ•°æ®
            (input_data, input_mask, target_data, 
             global_array, global_mask, global_target, metadata) = self[idx]
            
            # è·å–å¯¹åº”çš„åŸå§‹æ•°æ®è¿›è¡Œæ¯”è¾ƒ
            original_idx = (idx - self.original_length) % self.original_length
            (orig_input_data, orig_input_mask, orig_target_data, 
             orig_global_array, orig_global_mask, orig_global_target, orig_metadata) = self[original_idx]
            
            # è½¬æ¢ä¸ºnumpyç”¨äºå¯è§†åŒ–
            input_data_np = input_data.squeeze().numpy()
            input_mask_np = input_mask.squeeze().numpy()
            target_data_np = target_data.squeeze().numpy()
            orig_input_data_np = orig_input_data.squeeze().numpy()
            
            # è®¡ç®—ç»Ÿä¸€çš„é¢œè‰²èŒƒå›´
            all_data = [input_data_np, target_data_np, orig_input_data_np]
            valid_data = []
            for d in all_data:
                valid_values = d[d != 100]
                if len(valid_values) > 0:
                    valid_data.extend(valid_values)
            
            if valid_data:
                vmin = np.min(valid_data)
                vmax = np.max(valid_data)
            else:
                vmin, vmax = 0, 1
            
            # 1. åŸå§‹è¾“å…¥
            im1 = axes[i, 0].imshow(orig_input_data_np, cmap='terrain', vmin=vmin, vmax=vmax)
            axes[i, 0].set_title(f'åŸå§‹è¾“å…¥ (idx {original_idx})')
            
            # 2. ç”Ÿæˆçš„è¾“å…¥ï¼ˆå¸¦maskï¼‰
            masked_display = input_data_np.copy()
            masked_display[input_mask_np == 0] = np.nan  # å°†ç¼ºå¤±åŒºåŸŸè®¾ä¸ºNaNä»¥åœ¨å¯è§†åŒ–ä¸­æ˜¾ç¤º
            im2 = axes[i, 1].imshow(masked_display, cmap='terrain', vmin=vmin, vmax=vmax)
            axes[i, 1].set_title(f'ç”Ÿæˆè¾“å…¥ (idx {idx})')
            
            # 3. ç”Ÿæˆçš„mask
            im3 = axes[i, 2].imshow(input_mask_np, cmap='binary', vmin=0, vmax=1)
            axes[i, 2].set_title(f'ç”Ÿæˆmask (ç¼ºå¤±: {1-input_mask_np.mean():.1%})')
            
            # 4. ç›®æ ‡æ•°æ®ï¼ˆä¿æŒä¸å˜ï¼‰
            im4 = axes[i, 3].imshow(target_data_np, cmap='terrain', vmin=vmin, vmax=vmax)
            axes[i, 3].set_title('ç›®æ ‡æ•°æ®')
            
            # æ·»åŠ colorbar
            if i == 0:
                plt.colorbar(im1, ax=axes[i, 0])
                plt.colorbar(im2, ax=axes[i, 1])
                plt.colorbar(im3, ax=axes[i, 2])
                plt.colorbar(im4, ax=axes[i, 3])
        
        plt.tight_layout()
        
        if save_dir:
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)
            save_path = os.path.join(save_dir, "synthetic_mask_visualization.png")
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
        else:
            plt.show()
        
        plt.close()


def merge_input_channels(input_data, empty_value=100.0, visualize=False):
    """
    åˆå¹¶å¤šä¸ªè¾“å…¥é€šé“ï¼Œé‡‡ç”¨éç©ºä¼˜å…ˆç­–ç•¥

    å‚æ•°:
    input_data: å½¢çŠ¶ä¸º[N, H, W]çš„æ•°ç»„ï¼Œå…¶ä¸­Næ˜¯é€šé“æ•°
    empty_value: è¡¨ç¤ºç©ºå€¼çš„æ•°å€¼

    è¿”å›:
    merged_data: å½¢çŠ¶ä¸º[1, H, W]çš„æ•°ç»„
    """
    # åˆ›å»ºç©ºå€¼æ©ç  (Trueè¡¨ç¤ºæœ‰æ•ˆå€¼ï¼ŒFalseè¡¨ç¤ºç©ºå€¼)
    valid_mask = (input_data != empty_value).astype(np.uint8)

    # ç»Ÿè®¡æ¯ä¸ªä½ç½®çš„æœ‰æ•ˆå€¼æ•°é‡
    valid_count = np.sum(valid_mask, axis=0, keepdims=True)

    # å°†ç©ºå€¼æ›¿æ¢ä¸º0ï¼Œä»¥ä¾¿äºè®¡ç®—æ€»å’Œ
    temp_data = np.where(valid_mask, input_data, 0)

    # è®¡ç®—æœ‰æ•ˆå€¼çš„æ€»å’Œ
    valid_sum = np.sum(temp_data, axis=0, keepdims=True)

    # è®¡ç®—æœ‰æ•ˆå€¼çš„å‡å€¼ (é¿å…é™¤ä»¥é›¶)
    merged_data = np.zeros_like(valid_sum)
    non_empty_positions = valid_count > 0
    merged_data[non_empty_positions] = (
        valid_sum[non_empty_positions] / valid_count[non_empty_positions]
    )

    # å°†æ²¡æœ‰æœ‰æ•ˆå€¼çš„ä½ç½®è®¾ä¸ºç©ºå€¼
    merged_data[valid_count == 0] = empty_value

    return merged_data, False


# è‡ªå®šä¹‰ collate_fnï¼Œå¤„ç†å˜é•¿å¼ é‡
def custom_collate_fn(batch):
    # batch æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª7å…ƒç´ çš„å…ƒç»„
    # å°†æ¯ä¸ªå…ƒç»„æ‹†åˆ†æˆ7ä¸ªåˆ—è¡¨
    inputs = []
    input_masks = []
    input_targets = []
    id_arrays = []
    id_masks = []
    id_targets = []
    metadata = []

    for item in batch:
        inputs.append(item[0])
        input_masks.append(item[1])
        input_targets.append(item[2])
        id_arrays.append(item[3])
        id_masks.append(item[4])
        id_targets.append(item[5])
        metadata.append(item[6])

    # ç°åœ¨å°†åˆ—è¡¨è½¬æ¢ä¸ºæ‰¹æ¬¡å¼ é‡
    inputs = torch.stack(inputs)
    input_masks = torch.stack(input_masks)
    input_targets = torch.stack(input_targets).unsqueeze(1)
    id_arrays = torch.stack(id_arrays).unsqueeze(1)
    id_masks = torch.stack(id_masks).unsqueeze(1)
    id_targets = torch.stack(id_targets).unsqueeze(1)

    return inputs, input_masks, input_targets, id_arrays, id_masks, id_targets, metadata


def test_mask_generation():
    """æµ‹è¯•æ··åˆå¼maskç”Ÿæˆæ•ˆæœ"""
    print("ğŸ§ª æµ‹è¯•æ··åˆå¼maskç”Ÿæˆç®—æ³•...")
    
    # åˆ›å»ºä¸´æ—¶æ•°æ®é›†å®ä¾‹è¿›è¡Œæµ‹è¯•
    class TestDataset:
        def __init__(self):
            self.kernel_size = 33
            
        def generate_clustered_mask(self, *args, **kwargs):
            # åˆ›å»ºä¸€ä¸ªEnhancedDemDatasetçš„ä¸´æ—¶å®ä¾‹
            temp_dataset = type('temp', (), {})()
            temp_dataset.kernel_size = 33
            
            # ç»‘å®šæ‰€æœ‰æ–¹æ³•
            temp_dataset.generate_clustered_mask = EnhancedDemDataset.generate_clustered_mask.__get__(temp_dataset)
            temp_dataset._generate_concentrated_mask_pattern = EnhancedDemDataset._generate_concentrated_mask_pattern.__get__(temp_dataset)
            temp_dataset._generate_scattered_mask = EnhancedDemDataset._generate_scattered_mask.__get__(temp_dataset)
            temp_dataset._generate_concentrated_cluster = EnhancedDemDataset._generate_concentrated_cluster.__get__(temp_dataset)
            temp_dataset._generate_ellipse_cluster = EnhancedDemDataset._generate_ellipse_cluster.__get__(temp_dataset)
            temp_dataset._generate_polygon_cluster = EnhancedDemDataset._generate_polygon_cluster.__get__(temp_dataset)
            temp_dataset._generate_growth_cluster = EnhancedDemDataset._generate_growth_cluster.__get__(temp_dataset)
            temp_dataset._apply_morphological_operations = EnhancedDemDataset._apply_morphological_operations.__get__(temp_dataset)
            
            return temp_dataset.generate_clustered_mask(*args, **kwargs)
    
    test_dataset = TestDataset()
    
    # ç”Ÿæˆæµ‹è¯•mask - åˆ†åˆ«æµ‹è¯•é›†ä¸­å’Œåˆ†æ•£æ¨¡å¼
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    
    for i in range(12):
        row = i // 4
        col = i % 4
        
        # ç”Ÿæˆmask - è®¾ç½®ä¸åŒçš„åˆ†æ•£æ¦‚ç‡æ¥æ¼”ç¤ºä¸¤ç§æ¨¡å¼
        if i < 6:
            # å‰6ä¸ªï¼šä¸»è¦é›†ä¸­æ¨¡å¼ï¼ˆ10%åˆ†æ•£æ¦‚ç‡ï¼‰
            mask = test_dataset.generate_clustered_mask(
                shape=(33, 33),
                missing_ratio_range=(0.25, 0.75),
                num_clusters_range=(1, 2),
                cluster_size_range=(6, 15),
                scattered_probability=0.1
            )
            mask_type = "é›†ä¸­æ¨¡å¼"
        else:
            # å6ä¸ªï¼šä¸»è¦åˆ†æ•£æ¨¡å¼ï¼ˆ90%åˆ†æ•£æ¦‚ç‡ï¼‰
            mask = test_dataset.generate_clustered_mask(
                shape=(33, 33),
                missing_ratio_range=(0.25, 0.75),
                num_clusters_range=(1, 2),
                cluster_size_range=(6, 15),
                scattered_probability=0.9
            )
            mask_type = "åˆ†æ•£æ¨¡å¼"
        
        # è®¡ç®—ç¼ºå¤±æ¯”ä¾‹
        missing_ratio = 1 - (mask.sum() / mask.size)
        
        # è®¡ç®—è¿é€šåŒºåŸŸæ•°
        labeled_mask, num_regions = label(mask == 0)
        
        # å¯è§†åŒ–
        axes[row, col].imshow(mask, cmap='binary', vmin=0, vmax=1)
        axes[row, col].set_title(f'{mask_type} {i%6+1}\nç¼ºå¤±: {missing_ratio:.1%}, åŒºåŸŸ: {num_regions}')
        axes[row, col].axis('off')
    
    plt.tight_layout()
    plt.suptitle('æ··åˆå¼Maskç”Ÿæˆæµ‹è¯•ç»“æœ (é›†ä¸­ vs åˆ†æ•£)', fontsize=16, y=0.98)
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    plt.savefig('mixed_masks_test.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("âœ… æ··åˆå¼maskç”Ÿæˆæµ‹è¯•å®Œæˆï¼")
    print("ğŸ“Š æµ‹è¯•ç»“æœå·²ä¿å­˜ä¸º mixed_masks_test.png")
    print("ğŸ¯ æ–°ç®—æ³•ç‰¹ç‚¹:")
    print("   - æ”¯æŒé›†ä¸­å’Œåˆ†æ•£ä¸¤ç§åˆ†å¸ƒæ¨¡å¼")
    print("   - å¯æ§çš„åˆ†æ•£æ¦‚ç‡ï¼ˆscattered_probabilityå‚æ•°ï¼‰")
    print("   - é›†ä¸­æ¨¡å¼: 1-2ä¸ªå¤§å‹è¿ç»­åŒºåŸŸ")
    print("   - åˆ†æ•£æ¨¡å¼: å¤šç§åˆ†æ•£ç­–ç•¥ï¼ˆéšæœºã€å°ç°‡ã€æ¡å¸¦ï¼‰")
    print("   - æ™ºèƒ½ç¼“å­˜ç³»ç»ŸåŠ é€Ÿæ•°æ®åŠ è½½")


def benchmark_dataset_loading(dataset, num_samples=50):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print(f"ğŸ“Š å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• (æ ·æœ¬æ•°: {num_samples})")
    
    # æµ‹è¯•åŸå§‹æ•°æ®åŠ è½½
    start_time = time.time()
    for i in range(min(num_samples, dataset.original_length)):
        _ = dataset._get_original_item(i)
    original_time = time.time() - start_time
    
    # æµ‹è¯•åˆæˆæ•°æ®ç”Ÿæˆ
    if dataset.enable_synthetic_masks and dataset.total_length > dataset.original_length:
        start_time = time.time()
        for i in range(dataset.original_length, min(dataset.original_length + num_samples, dataset.total_length)):
            _ = dataset._get_synthetic_item(i)
        synthetic_time = time.time() - start_time
    else:
        synthetic_time = 0
    
    print(f"â±ï¸  æ€§èƒ½ç»“æœ:")
    print(f"   åŸå§‹æ•°æ®åŠ è½½: {original_time:.2f}s ({original_time/num_samples*1000:.1f}ms/æ ·æœ¬)")
    if synthetic_time > 0:
        print(f"   åˆæˆæ•°æ®ç”Ÿæˆ: {synthetic_time:.2f}s ({synthetic_time/num_samples*1000:.1f}ms/æ ·æœ¬)")
    
    return original_time, synthetic_time


def create_fast_dataset(jsonDir, arrayDir, maskDir, targetDir, **kwargs):
    """åˆ›å»ºä¼˜åŒ–çš„å¿«é€Ÿæ•°æ®é›†"""
    # é»˜è®¤çš„å¿«é€Ÿé…ç½®
    fast_config = {
        'enable_synthetic_masks': True,
        'synthetic_ratio': 1.0,
        'use_cache': True,
        'num_workers_cache': 6,  # å¢åŠ ç¼“å­˜ç”Ÿæˆçš„å¹¶è¡Œåº¦
    }
    fast_config.update(kwargs)
    
    print("ğŸš€ åˆ›å»ºé«˜æ€§èƒ½æ•°æ®é›†é…ç½®:")
    for key, value in fast_config.items():
        print(f"   {key}: {value}")
    
    return EnhancedDemDataset(jsonDir, arrayDir, maskDir, targetDir, **fast_config)


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # æµ‹è¯•maskç”Ÿæˆ
    print("ğŸ§ª æµ‹è¯•æ··åˆå¼maskç”Ÿæˆç®—æ³•...")
    test_mask_generation()
    
    # æ€§èƒ½æµ‹è¯•
    print("\n" + "="*50)
    print("âš¡ æ€§èƒ½æµ‹è¯•")
    print("="*50)
    
    # æµ‹è¯•ç¼“å­˜æ€§èƒ½
    jsonDir = r"E:\KingCrimson Dataset\Simulate\data0\json"
    arrayDir = r"E:\KingCrimson Dataset\Simulate\data0\arraynmask\array"
    maskDir = r"E:\KingCrimson Dataset\Simulate\data0\arraynmask\mask"
    targetDir = r"E:\KingCrimson Dataset\Simulate\data0\groundtruthstatus\statusarray"
    
    print("æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½...")
    
    # åˆ›å»ºå°è§„æ¨¡æµ‹è¯•æ•°æ®é›†
    if os.path.exists(jsonDir):
        print("ğŸ“Š æ— ç¼“å­˜åŠ è½½æµ‹è¯•...")
        start_time = time.time()
        dataset_no_cache = EnhancedDemDataset(
            jsonDir, arrayDir, maskDir, targetDir,
            enable_synthetic_masks=False,
            use_cache=False
        )
        # æµ‹è¯•åŠ è½½å‰10ä¸ªæ•°æ®
        for i in range(10):
            _ = dataset_no_cache[i]
        no_cache_time = time.time() - start_time
        print(f"æ— ç¼“å­˜åŠ è½½10ä¸ªæ•°æ®è€—æ—¶: {no_cache_time:.2f}s")
        
        print("ğŸ“Š æœ‰ç¼“å­˜åŠ è½½æµ‹è¯•...")
        start_time = time.time()
        dataset_with_cache = EnhancedDemDataset(
            jsonDir, arrayDir, maskDir, targetDir,
            enable_synthetic_masks=False,
            use_cache=True
        )
        # æµ‹è¯•åŠ è½½å‰10ä¸ªæ•°æ®
        for i in range(10):
            _ = dataset_with_cache[i]
        cache_time = time.time() - start_time
        print(f"æœ‰ç¼“å­˜åŠ è½½10ä¸ªæ•°æ®è€—æ—¶: {cache_time:.2f}s")
        
        if no_cache_time > 0:
            speedup = no_cache_time / cache_time
            print(f"ğŸš€ ç¼“å­˜åŠ é€Ÿæ¯”: {speedup:.1f}x")
    
    print("\n" + "="*50)
    print("ğŸš€ æ•°æ®é›†ä½¿ç”¨ç¤ºä¾‹")
    print("="*50)

    # åˆ›å»ºå¢å¼ºæ•°æ®é›† - ä½¿ç”¨å¿«é€Ÿé…ç½®
    dataset = create_fast_dataset(
        jsonDir, arrayDir, maskDir, targetDir,
        synthetic_ratio=1.0,           # æ¯ä¸ªåŸå§‹æ•°æ®ç”Ÿæˆ1ä¸ªåˆæˆæ•°æ®
        use_cache=True,                # å¯ç”¨ç¼“å­˜åŠ é€Ÿ
        num_workers_cache=6            # ä½¿ç”¨6ä¸ªçº¿ç¨‹å¹¶è¡Œæ„å»ºç¼“å­˜
    )
    
    print(f"ğŸ“ˆ å¢å¼ºåæ•°æ®é›†å¤§å°: {len(dataset)}")
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    if len(dataset) > 0:
        benchmark_dataset_loading(dataset, num_samples=20)
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    dataloader = DataLoader(
        dataset, batch_size=4, shuffle=True, 
        collate_fn=custom_collate_fn, num_workers=2
    )
    
    # å¯è§†åŒ–ç”Ÿæˆå¼maskæ•ˆæœ
    if len(dataset) > dataset.original_length:
        print("ğŸ¨ ç”Ÿæˆæ•°æ®å¢å¼ºå¯è§†åŒ–...")
        dataset.visualize_synthetic_mask(save_dir="./mask_visualization_results")
    
    # æµ‹è¯•è®­ç»ƒå¾ªç¯
    print("ğŸ”„ æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
    for i, batch in enumerate(dataloader):
        inputs, input_masks, input_targets, id_arrays, id_masks, id_targets, metadata = batch
        print(f"æ‰¹æ¬¡ {i+1}:")
        print(f"  è¾“å…¥å½¢çŠ¶: {inputs.shape}")
        print(f"  æ©ç å½¢çŠ¶: {input_masks.shape}")
        print(f"  ç›®æ ‡å½¢çŠ¶: {input_targets.shape}")
        
        # åˆ†æmaskç‰¹å¾
        concentrated_count = 0
        scattered_count = 0
        
        for j in range(inputs.shape[0]):
            mask = input_masks[j].cpu().numpy().squeeze()
            missing_ratio = 1.0 - (mask.sum() / mask.size)
            labeled_mask, num_regions = label(mask == 0)
            
            # ç®€å•åˆ¤æ–­maskç±»å‹ï¼ˆåŸºäºè¿é€šåŒºåŸŸæ•°é‡ï¼‰
            mask_type = "é›†ä¸­" if num_regions <= 2 else "åˆ†æ•£"
            if mask_type == "é›†ä¸­":
                concentrated_count += 1
            else:
                scattered_count += 1
                
            print(f"    æ ·æœ¬{j+1}: ç¼ºå¤±{missing_ratio:.1%}, {num_regions}ä¸ªåŒºåŸŸ ({mask_type})")
        
        print(f"  æ‰¹æ¬¡ç»Ÿè®¡: {concentrated_count}ä¸ªé›†ä¸­, {scattered_count}ä¸ªåˆ†æ•£")
        
        if i >= 1:  # åªæµ‹è¯•2ä¸ªæ‰¹æ¬¡
            break
    
    print("âœ… æµ‹è¯•å®Œæˆï¼æ•°æ®é›†å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
    print("ğŸ¯ ä¼˜åŒ–æ€»ç»“:")
    print("   - æ”¯æŒé›†ä¸­(80%)å’Œåˆ†æ•£(20%)æ··åˆmaskæ¨¡å¼")
    print("   - æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿæ˜¾è‘—åŠ é€Ÿæ•°æ®åŠ è½½")
    print("   - å¹¶è¡Œç¼“å­˜æ„å»ºæé«˜åˆå§‹åŒ–é€Ÿåº¦") 
    print("   - å†…å­˜æ˜ å°„å‡å°‘RAMå ç”¨")
    print("   - çº¿ç¨‹æ± åŠ é€ŸI/Oå¯†é›†å‹æ“ä½œ")