import os

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split, Subset
from torch.nn.utils import spectral_norm
from torch.optim import Adadelta, Adam
import torchvision.transforms as transforms
from torchvision.utils import make_grid
import numpy as np
from tqdm import tqdm
from PIL import Image, ImageDraw
import visdom
import matplotlib.pyplot as plt
from torchvision.transforms.functional import to_pil_image
from losses.loss import completion_network_loss
import torch.nn.functional as F

# Import your models
from models.CompletionNet import CompletionNetwork
from models.ContextDis import ContextDiscriminator

# Import your custom dataset
## from DemDataset1 import DemDataset
from data_manage.fastdatageneration import PreGeneratedMaskDataset

# from DEMData import DEMData

# Import the custom_collate_fn function from your dataset module
## from DemDataset import custom_collate_fn
# from DEMData import custom_collate_fn
from torch.amp import GradScaler, autocast
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.optim.lr_scheduler import CosineAnnealingLR
from models.SurfaceDis import BoundaryQualityDiscriminator

import random
import gc
import copy
import glob
import psutil
import traceback
import tempfile
import shutil
import os
import re
import subprocess
import time
from contextlib import contextmanager
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler


@contextmanager
def visdom_server():
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨ç®¡ç†VisdomæœåŠ¡å™¨çš„å¯åŠ¨å’Œå…³é—­"""
    process = None
    # try:
    # å¯åŠ¨æœåŠ¡å™¨
    process = subprocess.Popen(
        ["python", "-m", "visdom.server"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    print("æ­£åœ¨å¯åŠ¨VisdomæœåŠ¡å™¨...")
    time.sleep(3)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    yield process
    """finally:
        # ç¡®ä¿æœåŠ¡å™¨è¢«å…³é—­
        if process:
            process.terminate()
            process.wait()
            print("VisdomæœåŠ¡å™¨å·²å…³é—­")"""


# ç¨³å®šæ€§è¿½è¸ªå™¨
class StabilityTracker:
    def __init__(self):
        self.real_acc_ema = 0.5
        self.fake_acc_ema = 0.5
        self.recon_loss_ema = float("inf")
        self.alpha = 0.95
        self.collapse_threshold = 0.15
        self.recovery_steps = 0

    def update(self, real_acc, fake_acc, recon_loss):
        self.real_acc_ema = self.alpha * self.real_acc_ema + (1 - self.alpha) * real_acc
        self.fake_acc_ema = self.alpha * self.fake_acc_ema + (1 - self.alpha) * fake_acc
        self.recon_loss_ema = (
            self.alpha * self.recon_loss_ema + (1 - self.alpha) * recon_loss
        )

    def is_collapsed(self):
        return (
            self.real_acc_ema < self.collapse_threshold
            or self.fake_acc_ema < self.collapse_threshold
        )

    def get_loss_weights(self):
        if (
            self.real_acc_ema < 0.25
            and self.fake_acc_ema > 0.75
            or self.fake_acc_ema - self.real_acc_ema >= 0.4
        ):
            print("strenthening real sample weights")
            return 3.0, 0.3  # åŠ å¼ºçœŸå®æ ·æœ¬æƒé‡
        elif (
            self.fake_acc_ema < 0.25
            and self.real_acc_ema > 0.75
            or self.real_acc_ema - self.fake_acc_ema >= 0.4
        ):
            print("strenthening fake sample weights")
            return 0.3, 3.0  # åŠ å¼ºå‡æ ·æœ¬æƒé‡
        else:
            return 1.0, 1.0


# å†…å­˜ç®¡ç†å·¥å…·å‡½æ•°
def cleanup_memory():
    """å¼ºåˆ¶æ¸…ç†å†…å­˜"""
    gc.collect()
    if torch.cuda.device_count() > 1:
        for i in range(torch.cuda.device_count()):
            torch.cuda.empty_cache()
            torch.cuda.synchronize(i)
    else:
        torch.cuda.empty_cache()
        if torch.cuda.is_available():
            torch.cuda.synchronize()


def get_gpu_memory_usage():
    """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3  # GB
        return allocated, reserved
    return 0, 0


def check_memory_and_cleanup(threshold_gb=10.0):
    """æ£€æŸ¥å†…å­˜ä½¿ç”¨å¹¶åœ¨è¶…è¿‡é˜ˆå€¼æ—¶æ¸…ç†"""
    if torch.cuda.device_count() > 1:
        total_allocated = 0
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            total_allocated += allocated
            if allocated > threshold_gb:
                print(f"GPU {i} å†…å­˜ä½¿ç”¨è¿‡é«˜: {allocated:.2f}GB")

        if total_allocated > threshold_gb * torch.cuda.device_count():
            print(f"æ€»GPUå†…å­˜ä½¿ç”¨è¿‡é«˜: {total_allocated:.2f}GB, è¿›è¡Œæ¸…ç†...")
            cleanup_memory()
    else:
        # åŸæœ‰çš„å•GPUé€»è¾‘
        allocated, reserved = get_gpu_memory_usage()
        if allocated > threshold_gb:
            print(f"GPUå†…å­˜ä½¿ç”¨è¿‡é«˜: {allocated:.2f}GB, è¿›è¡Œæ¸…ç†...")
            cleanup_memory()


def safe_tensor_operation(func, *args, **kwargs):
    """å®‰å…¨çš„tensoræ“ä½œï¼Œè‡ªåŠ¨å¤„ç†å†…å­˜ä¸è¶³"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"å†…å­˜ä¸è¶³ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•...")
                cleanup_memory()
                if attempt == max_retries - 1:
                    raise
            else:
                raise


# setx PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:98


def save_checkpoint(
    model_cn,
    model_cd,
    model_bqd,
    opt_cn,
    opt_cd,
    opt_bqd,
    step,
    phase,
    result_dir,
    best_val_loss=None,
    best_acc=None,
    cn_lr=None,
    cd_lr=None,
    bqd_lr=None,
    schedulers=None,  # æ–°å¢
    scalers=None,  # æ–°å¢
    stability_tracker=None,  # æ–°å¢
):
    """å®Œæ•´çš„æ£€æŸ¥ç‚¹ä¿å­˜"""
    try:
        # å¤„ç†å¤šGPUæ¨¡å‹çš„state_dict
        def get_model_state_dict(model):
            if model is None:
                return None
            # å¦‚æœæ˜¯DataParallelåŒ…è£…çš„æ¨¡å‹ï¼Œä½¿ç”¨.moduleè·å–åŸå§‹æ¨¡å‹
            if hasattr(model, "module"):
                return model.module.state_dict()
            else:
                return model.state_dict()

        checkpoint = {
            # æ¨¡å‹çŠ¶æ€
            "model_cn_state_dict": get_model_state_dict(model_cn),
            "model_cd_state_dict": get_model_state_dict(model_cd),
            "model_bqd_state_dict": get_model_state_dict(model_bqd),
            # ä¼˜åŒ–å™¨çŠ¶æ€
            "opt_cn_state_dict": opt_cn.state_dict() if opt_cn is not None else None,
            "opt_cd_state_dict": opt_cd.state_dict() if opt_cd is not None else None,
            "opt_bqd_state_dict": opt_bqd.state_dict() if opt_bqd is not None else None,
            # è®­ç»ƒçŠ¶æ€
            "step": step,
            "phase": phase,
            "best_val_loss": best_val_loss,
            "best_acc": best_acc,
            "cn_lr": cn_lr,
            "cd_lr": cd_lr,
            "bqd_lr": bqd_lr,
            # éšæœºæ•°çŠ¶æ€
            "torch_rng_state": torch.get_rng_state(),
            "numpy_rng_state": np.random.get_state(),
            "python_rng_state": random.getstate(),  # æ·»åŠ Python randomçŠ¶æ€
            # CUDAéšæœºæ•°çŠ¶æ€
            "cuda_rng_state": (
                torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None
            ),
            # å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
            "schedulers": {},
            "scalers": {},
            "stability_tracker": None,
        }

        # ä¿å­˜è°ƒåº¦å™¨çŠ¶æ€
        if schedulers:
            for name, scheduler in schedulers.items():
                if scheduler is not None:
                    checkpoint["schedulers"][name] = scheduler.state_dict()

        # ä¿å­˜ç¼©æ”¾å™¨çŠ¶æ€
        if scalers:
            for name, scaler in scalers.items():
                if scaler is not None:
                    checkpoint["scalers"][name] = scaler.state_dict()

        # ä¿å­˜ç¨³å®šæ€§è¿½è¸ªå™¨
        if stability_tracker is not None:
            checkpoint["stability_tracker"] = {
                "real_acc_ema": stability_tracker.real_acc_ema,
                "fake_acc_ema": stability_tracker.fake_acc_ema,
                "recon_loss_ema": stability_tracker.recon_loss_ema,
                "alpha": stability_tracker.alpha,
                "recovery_steps": stability_tracker.recovery_steps,
            }

        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs(result_dir, exist_ok=True)

        checkpoint_filename = f"checkpoint_phase{phase}_step{step}.pth"
        checkpoint_path = os.path.join(result_dir, checkpoint_filename)

        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å®‰å…¨ä¿å­˜
        with tempfile.NamedTemporaryFile(
            delete=False, suffix=".pth.tmp", dir=result_dir
        ) as tmp_file:
            temp_path = tmp_file.name

        try:
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            print(f"æ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_path}")
            torch.save(checkpoint, temp_path)

            # éªŒè¯ä¸´æ—¶æ–‡ä»¶å®Œæ•´æ€§
            print("éªŒè¯æ£€æŸ¥ç‚¹æ–‡ä»¶å®Œæ•´æ€§...")
            test_checkpoint = torch.load(
                temp_path, map_location="cpu", weights_only=False
            )
            required_keys = ["model_cn_state_dict", "step", "phase"]
            for key in required_keys:
                if key not in test_checkpoint:
                    raise ValueError(f"æ£€æŸ¥ç‚¹éªŒè¯å¤±è´¥ï¼šç¼ºå°‘é”® {key}")
            del test_checkpoint  # é‡Šæ”¾å†…å­˜

            # å¦‚æœéªŒè¯é€šè¿‡ï¼Œç§»åŠ¨ä¸´æ—¶æ–‡ä»¶åˆ°æœ€ç»ˆä½ç½®
            if os.path.exists(checkpoint_path):
                # å¤‡ä»½ç°æœ‰æ–‡ä»¶
                backup_path = checkpoint_path + ".backup"
                shutil.move(checkpoint_path, backup_path)
                print(f"å¤‡ä»½ç°æœ‰æ£€æŸ¥ç‚¹åˆ°: {backup_path}")

            shutil.move(temp_path, checkpoint_path)
            print(f"æˆåŠŸä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {checkpoint_path}")

            # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹çš„å‰¯æœ¬
            latest_checkpoint_path = os.path.join(result_dir, "latest_checkpoint.pth")
            shutil.copy2(checkpoint_path, latest_checkpoint_path)
            print(f"ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹åˆ°: {latest_checkpoint_path}")

            # ç°åœ¨å®‰å…¨åœ°åˆ é™¤æ—§æ–‡ä»¶ï¼ˆä¿ç•™æœ€è¿‘çš„3ä¸ªï¼‰
            cleanup_old_checkpoints(result_dir, phase, step, keep_count=3)

        except Exception as e:
            # å¦‚æœä¿å­˜å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_path):
                os.remove(temp_path)
            raise e

        # æ¸…ç†checkpointå˜é‡
        del checkpoint
        cleanup_memory()

        return checkpoint_path

    except Exception as e:
        print(f"ä¿å­˜æ£€æŸ¥ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        cleanup_memory()
        raise


def cleanup_old_checkpoints(result_dir, current_phase, current_step, keep_count=3):
    """
    æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä¿ç•™æœ€è¿‘çš„å‡ ä¸ª
    """
    try:
        # æ‰¾åˆ°åŒä¸€é˜¶æ®µçš„æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶
        pattern = os.path.join(result_dir, f"checkpoint_phase{current_phase}_step*.pth")
        checkpoint_files = glob.glob(pattern)

        # æ’é™¤å½“å‰æ­¥æ•°çš„æ–‡ä»¶
        current_filename = f"checkpoint_phase{current_phase}_step{current_step}.pth"
        checkpoint_files = [
            f for f in checkpoint_files if not f.endswith(current_filename)
        ]

        # æŒ‰æ­¥æ•°æ’åºï¼ˆä»æ–‡ä»¶åæå–æ­¥æ•°ï¼‰
        def extract_step(filepath):
            try:
                filename = os.path.basename(filepath)
                # æå–stepæ•°å­—
                step_part = filename.split("_step")[1].split(".pth")[0]
                return int(step_part)
            except:
                return 0

        checkpoint_files.sort(key=extract_step, reverse=True)

        # åˆ é™¤å¤šä½™çš„æ–‡ä»¶
        files_to_delete = checkpoint_files[keep_count:]
        for file_path in files_to_delete:
            try:
                os.remove(file_path)
                print(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {os.path.basename(file_path)}")
            except Exception as e:
                print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        # åŒæ—¶æ¸…ç†å¤‡ä»½æ–‡ä»¶
        backup_files = glob.glob(os.path.join(result_dir, "*.backup"))
        backup_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        for backup_file in backup_files[keep_count:]:
            try:
                os.remove(backup_file)
                print(f"åˆ é™¤æ—§å¤‡ä»½: {os.path.basename(backup_file)}")
            except Exception as e:
                print(f"åˆ é™¤å¤‡ä»½æ–‡ä»¶å¤±è´¥ {backup_file}: {e}")

    except Exception as e:
        print(f"æ¸…ç†æ—§æ£€æŸ¥ç‚¹æ—¶å‡ºé”™: {e}")


def load_checkpoint(
    checkpoint_path,
    model_cn,
    model_cd,
    model_bqd,
    opt_cn,
    opt_cd,
    opt_bqd,
    device,
    schedulers=None,
    scalers=None,
):
    """å®Œæ•´çš„æ£€æŸ¥ç‚¹åŠ è½½"""
    try:
        checkpoint = torch.load(
            checkpoint_path, map_location=device, weights_only=False
        )

        # å¤„ç†å¤šGPUæ¨¡å‹çš„state_dictåŠ è½½
        def load_model_state_dict(model, state_dict_key):
            if model is None or checkpoint.get(state_dict_key) is None:
                return

            state_dict = checkpoint[state_dict_key]

            # å¤„ç†DataParallelæ¨¡å‹
            if hasattr(model, "module"):
                # å½“å‰æ˜¯DataParallelï¼Œæ£€æŸ¥state_dictæ˜¯å¦æœ‰'module.'å‰ç¼€
                sample_key = list(state_dict.keys())[0]
                if not sample_key.startswith("module."):
                    # state_dictæ²¡æœ‰'module.'å‰ç¼€ï¼Œéœ€è¦æ·»åŠ 
                    new_state_dict = {}
                    for k, v in state_dict.items():
                        new_state_dict["module." + k] = v
                    model.load_state_dict(new_state_dict)
                else:
                    model.load_state_dict(state_dict)
            else:
                # å½“å‰ä¸æ˜¯DataParallelï¼Œæ£€æŸ¥state_dictæ˜¯å¦æœ‰'module.'å‰ç¼€
                sample_key = list(state_dict.keys())[0]
                if sample_key.startswith("module."):
                    # state_dictæœ‰'module.'å‰ç¼€ï¼Œéœ€è¦ç§»é™¤
                    new_state_dict = {}
                    for k, v in state_dict.items():
                        new_state_dict[k[7:]] = v  # ç§»é™¤'module.'å‰ç¼€
                    model.load_state_dict(new_state_dict)
                else:
                    model.load_state_dict(state_dict)

        # åŠ è½½æ¨¡å‹
        load_model_state_dict(model_cn, "model_cn_state_dict")
        # load_model_state_dict(model_cd, "model_cd_state_dict")
        load_model_state_dict(model_bqd, "model_bqd_state_dict")

        # åŠ è½½ä¼˜åŒ–å™¨
        if opt_cn and checkpoint.get("opt_cn_state_dict"):
            opt_cn.load_state_dict(checkpoint["opt_cn_state_dict"])
        if opt_cd and checkpoint.get("opt_cd_state_dict") and False:
            opt_cd.load_state_dict(checkpoint["opt_cd_state_dict"])
        if opt_bqd and checkpoint.get("opt_bqd_state_dict"):
            opt_bqd.load_state_dict(checkpoint["opt_bqd_state_dict"])

        # æ¢å¤éšæœºæ•°çŠ¶æ€
        """if "torch_rng_state" in checkpoint:
            torch.set_rng_state(checkpoint["torch_rng_state"])
        if "numpy_rng_state" in checkpoint:
            np.random.set_state(checkpoint["numpy_rng_state"])
        if "python_rng_state" in checkpoint:
            random.setstate(checkpoint["python_rng_state"])
        if "cuda_rng_state" in checkpoint and torch.cuda.is_available():
            torch.cuda.set_rng_state_all(checkpoint["cuda_rng_state"])"""

        # æ¢å¤è°ƒåº¦å™¨çŠ¶æ€
        if schedulers and "schedulers" in checkpoint:
            for name, scheduler in schedulers.items():
                if scheduler is not None and name in checkpoint["schedulers"]:
                    scheduler.load_state_dict(checkpoint["schedulers"][name])

        # æ¢å¤ç¼©æ”¾å™¨çŠ¶æ€
        if scalers and "scalers" in checkpoint:
            for name, scaler in scalers.items():
                if scaler is not None and name in checkpoint["scalers"]:
                    scaler.load_state_dict(checkpoint["scalers"][name])

        # æ¢å¤ç¨³å®šæ€§è¿½è¸ªå™¨
        stability_tracker = None
        if "stability_tracker" in checkpoint and checkpoint["stability_tracker"]:
            stability_tracker = StabilityTracker()
            st_data = checkpoint["stability_tracker"]
            stability_tracker.real_acc_ema = st_data["real_acc_ema"]
            stability_tracker.fake_acc_ema = st_data["fake_acc_ema"]
            stability_tracker.recon_loss_ema = st_data["recon_loss_ema"]
            stability_tracker.alpha = st_data.get("alpha", 0.95)
            stability_tracker.recovery_steps = st_data.get("recovery_steps", 0)

        # æå–å…¶ä»–ä¿¡æ¯
        step = checkpoint["step"]
        phase = checkpoint["phase"]
        best_val_loss = checkpoint.get("best_val_loss", float("inf"))
        best_acc = checkpoint.get("best_acc", float("-inf"))
        cn_lr = checkpoint.get("cn_lr", None)
        cd_lr = checkpoint.get("cd_lr", None)
        bqd_lr = checkpoint.get("bqd_lr", None)

        print(f"æˆåŠŸæ¢å¤æ£€æŸ¥ç‚¹ï¼šé˜¶æ®µ {phase}ï¼Œæ­¥éª¤ {step}")
        print(f"éšæœºæ•°çŠ¶æ€å·²æ¢å¤")

        return (
            step,
            phase,
            best_val_loss,
            best_acc,
            cn_lr,
            cd_lr,
            bqd_lr,
            stability_tracker,
        )

    except Exception as e:
        print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        raise


def ensure_scalar_loss(loss):
    """
    ç¡®ä¿lossæ˜¯æ ‡é‡ï¼Œå¤„ç†å¤šGPUæƒ…å†µ
    """
    if isinstance(loss, torch.Tensor):
        if loss.dim() > 0:
            return loss.mean()
        else:
            return loss
    else:
        return loss


def prepare_batch_data(batch, device):
    """
    Prepare batch data by moving tensors to the specified device.
    ä¼˜åŒ–ï¼šä½¿ç”¨non_blockingè½¬ç§»å’Œæ›´å¥½çš„å†…å­˜ç®¡ç†
    """
    try:
        (
            local_inputs,
            local_masks,
            local_targets,
            global_inputs,
            global_masks,
            global_targets,
            metadata,
        ) = batch

        # ä½¿ç”¨non_blockingè½¬ç§»æé«˜æ•ˆç‡
        local_inputs = local_inputs.to(device, dtype=torch.float32, non_blocking=True)
        local_masks = local_masks.to(device, dtype=torch.int, non_blocking=True)
        local_targets = local_targets.to(device, dtype=torch.float32, non_blocking=True)
        global_inputs = global_inputs.to(device, dtype=torch.float32, non_blocking=True)
        global_masks = global_masks.to(device, dtype=torch.int, non_blocking=True)
        global_targets = global_targets.to(
            device, dtype=torch.float32, non_blocking=True
        )

        return (
            local_inputs,
            local_masks,
            local_targets,
            global_inputs,
            global_masks,
            global_targets,
            metadata,
        )
    except Exception as e:
        print(f"å‡†å¤‡æ‰¹æ¬¡æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        cleanup_memory()
        raise


def simple_dataset_split(dataset, test_ratio=0.1, val_ratio=0.2, seed=42):
    """
    ç®€æ´çš„æ•°æ®é›†åˆ†å‰²æ–¹æ³•

    Args:
        dataset: å®Œæ•´æ•°æ®é›†
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆä»å‰é¢å–ï¼‰
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ï¼ˆä»å‰©ä½™æ•°æ®ä¸­å–ï¼‰
        seed: éšæœºç§å­ï¼Œç¡®ä¿å¯é‡ç°

    Returns:
        train_dataset, val_dataset, test_dataset
    """
    total_size = len(dataset)
    test_size = int(total_size * test_ratio)

    # Step 1: å‰test_ratio%ä½œä¸ºæµ‹è¯•é›†ï¼ˆå›ºå®šä¸å˜ï¼‰
    test_indices = list(range(test_size))
    remaining_indices = list(range(test_size, total_size))

    # Step 2: å‰©ä½™æ•°æ®shuffleååˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    np.random.seed(seed)
    np.random.shuffle(remaining_indices)

    remaining_size = len(remaining_indices)
    val_size = int(remaining_size * val_ratio)

    val_indices = remaining_indices[:val_size]
    train_indices = remaining_indices[val_size:]

    # åˆ›å»ºå­æ•°æ®é›†
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    test_dataset = Subset(dataset, test_indices)

    print(f"æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
    print(f"  æ€»æ•°æ®: {total_size}")
    print(
        f"  æµ‹è¯•é›†: {len(test_indices)} ({len(test_indices)/total_size:.1%}) - ç´¢å¼• 0 åˆ° {test_size-1}"
    )
    print(f"  éªŒè¯é›†: {len(val_indices)} ({len(val_indices)/total_size:.1%})")
    print(f"  è®­ç»ƒé›†: {len(train_indices)} ({len(train_indices)/total_size:.1%})")

    return train_dataset, val_dataset, test_dataset


def generate_target_mask(batch_global_masks, metadata):
    """
    ç”Ÿæˆtarget maskï¼Œå¹¶éªŒè¯æ˜¯å¦åŒ…å«global_masksçš„æ‰€æœ‰1å€¼

    Args:
        batch_global_masks: å…¨å±€æ©ç å¼ é‡ [batch_size, channels, height, width], dtype=torch.int
        metadata: numpyæ•°ç»„æˆ–tupleåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ ¼å¼ä¸º [centerx, centery, xbegain, ybegain, xnum, ynum, id_val]

    Returns:
        target_masks: ç”Ÿæˆçš„target maskå¼ é‡ [batch_size, channels, height, width], dtypeä¸global_masksç›¸åŒ
        success_flags: æ¯ä¸ªæ ·æœ¬æ˜¯å¦æˆåŠŸç”Ÿæˆçš„æ ‡å¿—åˆ—è¡¨
    """
    batch_size = batch_global_masks.shape[0]
    device = batch_global_masks.device
    dtype = batch_global_masks.dtype  # ä¿æŒä¸global_masksç›¸åŒçš„æ•°æ®ç±»å‹

    # åˆ›å»ºä¸batch_global_masksç›¸åŒshapeå’Œdtypeçš„target_masks
    target_masks = torch.zeros_like(batch_global_masks, dtype=dtype, device=device)
    success_flags = []

    for i in range(batch_size):
        try:
            meta = metadata[i]

            # å¤„ç†ä¸åŒç±»å‹çš„metadata (numpyæ•°ç»„ã€tupleã€listç­‰)
            if hasattr(meta, "__getitem__"):  # å¯ç´¢å¼•å¯¹è±¡
                # metadataæ•°ç»„ç´¢å¼•: [centerx, centery, xbegain, ybegain, xnum, ynum, id_val]
                #                    [0,       1,       2,       3,       4,     5,     6]

                # å®‰å…¨åœ°æå–xnumå’Œynumï¼Œå¹¶è½¬æ¢ä¸ºæ•´æ•°
                try:
                    xnum_raw = meta[4]
                    ynum_raw = meta[5]

                    # å¤„ç†å¯èƒ½çš„tensorã€numpyæˆ–å…¶ä»–æ•°å€¼ç±»å‹
                    if hasattr(xnum_raw, "item"):  # tensoræˆ–numpyæ ‡é‡
                        xnum = int(xnum_raw.item())
                    else:
                        xnum = int(float(xnum_raw))  # å…ˆè½¬floatå†è½¬intï¼Œå¤„ç†å­—ç¬¦ä¸²æƒ…å†µ

                    if hasattr(ynum_raw, "item"):
                        ynum = int(ynum_raw.item())
                    else:
                        ynum = int(float(ynum_raw))

                except (ValueError, TypeError, IndexError) as e:
                    print(f"ERROR: Batch {i}: æ— æ³•è§£æmetadataä¸­çš„xnum/ynum: {e}")
                    print(f"  Metadata: {meta}")
                    print(f"  Metadata type: {type(meta)}")
                    success_flags.append(False)
                    continue
            else:
                print(f"ERROR: Batch {i}: metadataæ ¼å¼ä¸æ”¯æŒ: {type(meta)}")
                success_flags.append(False)
                continue

            # è·å–å›¾åƒå°ºå¯¸
            batch_size_dim, channels, height, width = batch_global_masks.shape

            # ç¬¬ä¸€æ¬¡å°è¯•ï¼šä½¿ç”¨åŸå§‹xnumå’Œynum
            target_mask = create_mask(
                height, width, ynum, xnum, device, dtype, channels
            )

            # ç¡®ä¿target_maskæ˜¯tensorè€Œä¸æ˜¯tuple - å…³é”®ä¿®å¤ç‚¹1
            if isinstance(target_mask, tuple):
                target_mask = target_mask[0]  # å¦‚æœæ˜¯tupleï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 

            # éªŒè¯æ˜¯å¦åŒ…å«global_masksçš„æ‰€æœ‰1å€¼
            global_mask = batch_global_masks[i]
            if validate_mask_inclusion(global_mask, target_mask):
                target_masks[i] = target_mask
                success_flags.append(True)
                # print(f"Batch {i}: æˆåŠŸç”Ÿæˆtarget mask (xnum={xnum}, ynum={ynum})")
            else:
                print(f"Batch {i}: ç¬¬ä¸€æ¬¡éªŒè¯å¤±è´¥ï¼Œäº¤æ¢xnumå’Œynumé‡è¯•...")

                # ç¬¬äºŒæ¬¡å°è¯•ï¼šäº¤æ¢xnumå’Œynum
                target_mask = create_mask(
                    height, width, xnum, ynum, device, dtype, channels
                )

                # ç¡®ä¿target_maskæ˜¯tensorè€Œä¸æ˜¯tuple - å…³é”®ä¿®å¤ç‚¹2
                if isinstance(target_mask, tuple):
                    target_mask = target_mask[0]

                if validate_mask_inclusion(global_mask, target_mask):
                    target_masks[i] = target_mask
                    success_flags.append(True)
                    print(
                        f"Batch {i}: äº¤æ¢åæˆåŠŸç”Ÿæˆtarget mask (xnum={ynum}, ynum={xnum})"
                    )
                else:
                    print(f"ERROR: Batch {i}: äº¤æ¢xnumå’Œynumåä»ç„¶éªŒè¯å¤±è´¥!")
                    print(f"  åŸå§‹å‚æ•°: xnum={xnum}, ynum={ynum}")
                    print(f"  å›¾åƒå°ºå¯¸: {height}x{width}")
                    print(f"  Global maskä¸­1çš„æ•°é‡: {global_mask.sum().item()}")
                    print(f"  Target maskå°ºå¯¸: {target_mask.shape}")
                    success_flags.append(False)

        except Exception as e:
            print(f"ERROR: Batch {i}: å¤„ç†metadataæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            print(f"  Metadata: {metadata[i] if i < len(metadata) else 'N/A'}")
            print(
                f"  Metadata type: {type(metadata[i]) if i < len(metadata) else 'N/A'}"
            )
            success_flags.append(False)

    # ç¡®ä¿è¿”å›çš„æ˜¯çº¯tensorï¼Œä¸æ˜¯tuple - å…³é”®ä¿®å¤ç‚¹3
    if isinstance(target_masks, tuple):
        target_masks = target_masks[0]

    assert isinstance(
        target_masks, torch.Tensor
    ), f"target_masksåº”è¯¥æ˜¯tensorï¼Œä½†å¾—åˆ°äº†{type(target_masks)}"
    assert (
        target_masks.shape == batch_global_masks.shape
    ), f"shapeä¸åŒ¹é…: {target_masks.shape} vs {batch_global_masks.shape}"

    return target_masks, success_flags


def create_mask(height, width, x_size, y_size, device, dtype, channels):
    """
    åœ¨å·¦ä¸Šè§’åˆ›å»ºæŒ‡å®šå°ºå¯¸çš„mask

    Args:
        height, width: å›¾åƒå°ºå¯¸
        x_size, y_size: maskçš„å°ºå¯¸
        device: è®¾å¤‡ç±»å‹
        dtype: æ•°æ®ç±»å‹ (torch.int)
        channels: é€šé“æ•°

    Returns:
        mask: ç”Ÿæˆçš„maskå¼ é‡ [channels, height, width] - çº¯tensorï¼Œä¸æ˜¯tuple
    """
    # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ˜¯æ•´æ•°
    height = int(height)
    width = int(width)
    x_size = int(x_size)
    y_size = int(y_size)
    channels = int(channels)

    # ç¡®ä¿maskå°ºå¯¸ä¸è¶…è¿‡å›¾åƒå°ºå¯¸
    x_size = min(x_size, width)
    y_size = min(y_size, height)

    # åˆ›å»ºä¸global_masksç›¸åŒæ ¼å¼çš„mask
    mask = torch.zeros(channels, height, width, device=device, dtype=dtype)
    mask[:, :y_size, :x_size] = 1

    # ç¡®ä¿è¿”å›çš„æ˜¯tensorï¼Œä¸æ˜¯tuple - é¢å¤–ä¿æŠ¤
    if isinstance(mask, tuple):
        mask = mask[0]

    assert isinstance(mask, torch.Tensor), f"maskåº”è¯¥æ˜¯tensorï¼Œä½†å¾—åˆ°äº†{type(mask)}"

    return mask


def validate_mask_inclusion(global_mask, target_mask):
    """
    éªŒè¯target_maskæ˜¯å¦åŒ…å«global_maskçš„æ‰€æœ‰1å€¼

    Args:
        global_mask: å…¨å±€æ©ç 
        target_mask: ç›®æ ‡æ©ç 

    Returns:
        bool: æ˜¯å¦åŒ…å«æ‰€æœ‰1å€¼
    """
    # ç¡®ä¿è¾“å…¥éƒ½æ˜¯tensor
    if isinstance(global_mask, tuple):
        global_mask = global_mask[0]
    if isinstance(target_mask, tuple):
        target_mask = target_mask[0]

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨global_maskä¸º1ä½†target_maskä¸º0çš„ä½ç½®
    invalid_positions = (global_mask == 1) & (target_mask == 0)
    return not invalid_positions.any().item()


def safe_extract_metadata(meta, index, default=0):
    """
    å®‰å…¨åœ°ä»metadataä¸­æå–æ•°å€¼

    Args:
        meta: metadataå¯¹è±¡ (å¯èƒ½æ˜¯numpyæ•°ç»„ã€tupleã€listç­‰)
        index: ç´¢å¼•ä½ç½®
        default: é»˜è®¤å€¼

    Returns:
        æå–å¹¶è½¬æ¢åçš„æ•°å€¼
    """
    try:
        if hasattr(meta, "__getitem__") and len(meta) > index:
            value = meta[index]

            # å¤„ç†ä¸åŒçš„æ•°å€¼ç±»å‹
            if hasattr(value, "item"):  # tensoræˆ–numpyæ ‡é‡
                return value.item()
            elif isinstance(value, (int, float)):
                return float(value)
            elif isinstance(value, str):
                return float(value)
            else:
                return float(value)
        else:
            return default
    except Exception as e:
        print(f"Warning: æå–metadata[{index}]æ—¶å‡ºé”™: {e}, ä½¿ç”¨é»˜è®¤å€¼ {default}")
        return default


# å®‰å…¨çš„ç±»å‹è½¬æ¢å‡½æ•°
def safe_tensor_float(tensor_or_tuple):
    """
    å®‰å…¨åœ°å°†tensoræˆ–tupleè½¬æ¢ä¸ºfloat tensor

    Args:
        tensor_or_tuple: å¯èƒ½æ˜¯tensoræˆ–åŒ…å«tensorçš„tuple

    Returns:
        floatç±»å‹çš„tensor
    """
    if isinstance(tensor_or_tuple, tuple):
        # å¦‚æœæ˜¯tupleï¼Œå‡è®¾ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯tensor
        tensor = tensor_or_tuple[0]
    else:
        tensor = tensor_or_tuple

    if isinstance(tensor, torch.Tensor):
        return tensor.float()
    else:
        raise TypeError(
            f"Expected tensor or tuple containing tensor, got {type(tensor)}"
        )


def nomarlize(local_inputs, local_targets, global_inputs, global_targets):
    ## å½’ä¸€åŒ–
    # åŸºäºglobal_inputçš„æ•°æ®èŒƒå›´è¿›è¡Œå½’ä¸€åŒ–ï¼ˆå·²ç»ç”¨å‡å€¼å¡«å……ï¼Œæ— éœ€è€ƒè™‘maskï¼‰
    global_min = global_inputs.min()
    global_max = global_inputs.max()

    # é¿å…é™¤é›¶é”™è¯¯
    if global_max - global_min > 1e-8:
        # å½’ä¸€åŒ–åˆ°[0, 1]
        local_inputs = (local_inputs - global_min) / (global_max - global_min)
        local_targets = (local_targets - global_min) / (global_max - global_min)
        global_inputs = (global_inputs - global_min) / (global_max - global_min)
        global_targets = (global_targets - global_min) / (global_max - global_min)
        return local_inputs, local_targets, global_inputs, global_targets
    else:
        print("Warning: global_inputsçš„æœ€å¤§å€¼å’Œæœ€å°å€¼ç›¸ç­‰ï¼Œæ— æ³•è¿›è¡Œå½’ä¸€åŒ–ã€‚")
        return local_inputs, local_targets, global_inputs, global_targets


def train(dir, envi, cuda, batch, test=False, resume_from=None, Phase=1):
    try:
        # =================================================
        # Configuration settings (replace with your settings)
        # =================================================
        if test:
            json_dir = r"E:\KingCrimson Dataset\Simulate\data0\testjson"
        else:
            json_dir = r"E:\KingCrimson Dataset\Simulate\data0\json"  # å±€éƒ¨json
        array_dir = (
            r"E:\KingCrimson Dataset\Simulate\data0\arraynmask_a\array"  # å…¨å±€array
        )
        mask_dir = (
            r"E:\KingCrimson Dataset\Simulate\data0\arraynmask_a\mask"  # å…¨å±€mask
        )
        target_dir = r"E:\KingCrimson Dataset\Simulate\data0\groundtruthstatus\statusarray"  # å…¨å±€target
        result_dir = rf"E:\KingCrimson Dataset\Simulate\data0\{dir}"  # ç»“æœä¿å­˜è·¯å¾„

        # Training parameters
        steps_1 = 50000  # 160000  # Phase 1 training steps
        steps_2 = 50000  # Phase 2 training steps
        steps_3 = 200000  # Phase 3 training steps

        snaperiod_1 = 500  # How often to save snapshots in phase 1
        snaperiod_2 = 500  # How often to save snapshots in phase 2
        snaperiod_3 = 500  # How often to save snapshots in phase 3

        batch_size = batch  # Batch size
        alpha = 4e-1  # Alpha parameter for loss weighting
        validation_split = 0.1  # Percentage of data to use for validation

        # åˆå§‹å†…å­˜æ¸…ç†
        cleanup_memory()

        # Hardware setup
        if not torch.cuda.is_available():
            raise Exception("At least one GPU must be available.")
        # è‡ªåŠ¨æ£€æµ‹å’Œè®¾ç½®GPU
        if torch.cuda.device_count() > 1 and cuda == "multi":
            print(f"æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
            use_multi_gpu = True
            # è‡ªåŠ¨è®¾ç½®ä¸»è®¾å¤‡ä¸ºcuda:0
            device = torch.device("cuda:0")
            print(f"ä½¿ç”¨å¤šGPUè®­ç»ƒï¼Œä¸»è®¾å¤‡: {device}")
            print(f"å¯ç”¨GPU: {[f'cuda:{i}' for i in range(torch.cuda.device_count())]}")
        else:
            use_multi_gpu = False
            # ä½¿ç”¨ä¼ å…¥çš„cudaå‚æ•°
            device = torch.device(cuda)
            print(f"ä½¿ç”¨å•GPUè®­ç»ƒ: {device}")

        torch.cuda.set_device(device)

        # Initialize Visdom with error handling
        try:
            viz = visdom.Visdom(env=envi)
            viz.close(env=envi)
        except Exception as e:
            print(f"Visdomåˆå§‹åŒ–å¤±è´¥: {e}")
            viz = None

        # =================================================
        # Preparation
        # =================================================
        # Create result directories
        if not os.path.exists(result_dir):
            os.makedirs(result_dir)
        for phase in ["phase_1", "phase_2", "phase_3", "validation", "bqd"]:
            if not os.path.exists(os.path.join(result_dir, phase)):
                os.makedirs(os.path.join(result_dir, phase))

        cleanup_memory()

        size = 65

        # æ•°æ®é›†å‚æ•°é…ç½®
        reference_dir = (
            r"e:\KingCrimson Dataset\Simulate\data0\ST2\statusarray"  # å‚è€ƒæ•°æ®ç›®å½•
        )
        mask_cache_dir = f"./mask_cache_{size}"  # maskç¼“å­˜ç›®å½•
        local_sizes = [size]  # æ”¯æŒå¤šç§å±€éƒ¨å°ºå¯¸
        samples_per_file = 20  # æ¯ä¸ªæ–‡ä»¶ç”Ÿæˆçš„æ ·æœ¬æ•°
        global_coverage_range = (0.5, 1.0)  # å…¨å±€è¦†ç›–ç‡èŒƒå›´
        local_missing_range = (0.3, 0.7)  # å±€éƒ¨ç¼ºå¤±ç‡èŒƒå›´
        num_global_masks = 500  # å…¨å±€maskæ¨¡æ¿æ•°é‡
        num_local_masks = 500  # å±€éƒ¨maskæ¨¡æ¿æ•°é‡
        batch_size = 8  # æ‰¹æ¬¡å¤§å°
        validation_split = 0.1  # éªŒè¯é›†æ¯”ä¾‹

        # åˆ›å»ºæ•°æ®é›†
        print("ğŸš€ åŠ è½½é¢„ç”Ÿæˆmaskæ•°æ®é›†...")
        try:
            dataset = PreGeneratedMaskDataset(
                reference_dir=reference_dir,
                mask_cache_dir=mask_cache_dir,
                local_sizes=local_sizes,
                samples_per_file=samples_per_file,
                global_coverage_range=global_coverage_range,
                local_missing_range=local_missing_range,
                num_global_masks=num_global_masks,
                num_local_masks=num_local_masks,
                regenerate_masks=False,  # ä½¿ç”¨å·²æœ‰çš„é¢„ç”Ÿæˆmask
                seed=42,
                cache_reference_files=True,
                visualization_samples=0,
                enable_edge_connection=True,  # å¯ç”¨è¾¹ç¼˜è¿æ¥
            )
            full_dataset = dataset
            print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ!")
            print(f"   æ€»æ ·æœ¬æ•°: {len(full_dataset)}")
            print(f"   Global masks: {len(dataset.global_masks)}")
            print(f"   Local masksæ•°é‡:")
            for size in local_sizes:
                if size in dataset.local_masks:
                    print(f"     {size}x{size}: {len(dataset.local_masks[size])}")
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            raise

        # åˆ†å‰²æ•°æ®é›†
        dataset_size = len(full_dataset)
        val_size = int(validation_split * dataset_size)
        train_size = dataset_size - val_size

        print(f"\nğŸ“Š æ•°æ®é›†åˆ†å‰²:")
        print(f"   æ€»æ•°æ®é‡: {dataset_size}")
        print(f"   è®­ç»ƒé›†: {train_size} ({100*(1-validation_split):.0f}%)")
        print(f"   éªŒè¯é›†: {val_size} ({100*validation_split:.0f}%)")

        # ä½¿ç”¨å›ºå®šç§å­ç¡®ä¿å¯é‡ç°æ€§
        generator = torch.Generator().manual_seed(42)
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size], generator=generator
        )

        # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,  # å¯ç”¨shuffle
            num_workers=2,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2,
            drop_last=True,  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡
        )

        # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=True,  # å¯ç”¨shuffle
            num_workers=2,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2,
            drop_last=False,
        )

        # åˆ›å»ºéªŒè¯å­é›†ï¼ˆç”¨äºå¿«é€ŸéªŒè¯ï¼‰
        val_subset_size = min(100, len(val_dataset))
        val_indices = np.random.choice(
            len(val_dataset), size=val_subset_size, replace=False
        )
        val_subset = torch.utils.data.Subset(val_dataset, val_indices)

        val_subset_loader = DataLoader(
            val_subset,
            batch_size=batch_size,
            shuffle=True,  # å¯ç”¨shuffle
            num_workers=2,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2,
            drop_last=False,
        )

        # =================================================
        # Setup Models
        # =================================================
        # Initialize models with error handling
        try:
            model_cn = CompletionNetwork(input_channels=1, local_size=size).to(device)
            model_cd = ContextDiscriminator(
                local_input_channels=1,
                local_input_size=size,
                global_input_channels=1,
                global_input_size=600,
            ).to(device)
            model_bqd = BoundaryQualityDiscriminator(
                input_channels=1, input_size=size
            ).to(device)
            model_cn = model_cn.float()
            model_cd = model_cd.float()
            model_bqd = model_bqd.float()

            # åŒ…è£…ä¸ºå¤šGPUæ¨¡å‹
            if use_multi_gpu:
                model_cn = nn.DataParallel(model_cn)
                model_cd = nn.DataParallel(model_cd)
                model_bqd = nn.DataParallel(model_bqd)
                print("æ¨¡å‹å·²åŒ…è£…ä¸ºDataParallel")
        except Exception as e:
            print(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            cleanup_memory()
            raise

        # Setup optimizers
        opt_cn_p1 = Adadelta(model_cn.parameters())
        opt_bqd = torch.optim.Adam(
            model_bqd.parameters(), lr=5e-4, betas=(0.5, 0.999), eps=1e-8
        )
        # opt_cd_p1 = Adadelta(model_cd.parameters())
        opt_cd_p2 = torch.optim.Adam(
            model_cd.parameters(), lr=1e-5, betas=(0.5, 0.999), eps=1e-8
        )
        opt_cn_p3 = torch.optim.Adam(
            model_cn.parameters(), lr=1e-4, betas=(0.5, 0.999), eps=1e-8
        )
        opt_cd_p3 = torch.optim.Adam(
            model_cd.parameters(), lr=1e-5, betas=(0.5, 0.999), eps=1e-8
        )

        # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€å˜é‡
        phase = Phase  # ä»ç¬¬äºŒé˜¶æ®µå¼€å§‹
        step_phase1 = 0
        step_phase2 = 0
        step_phase3 = 0
        best_val_loss = float("inf")
        best_bqd_loss = float("inf")
        best_acc = float("-inf")
        best_val_loss_joint = float("inf")
        cn_lr = None
        cd_lr = None
        bqd_lr = None

        # è·å–å½“å‰æœ¬åœ°æ—¶é—´
        current_time = time.localtime()

        # è·å–å¹´æœˆæ—¥
        year = current_time.tm_year
        month = current_time.tm_mon
        day = current_time.tm_mday
        hour = current_time.tm_hour  # 24å°æ—¶åˆ¶ (0-23)
        minute = current_time.tm_min  # åˆ†é’Ÿ (0-59)
        # å¦‚æœæŒ‡å®šäº†æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œåˆ™åŠ è½½å®ƒæ¢å¤è®­ç»ƒ
        interrupt = f"_{year}_{month}_{day}_{hour}_{minute}"
        if resume_from:
            try:
                # æ ¹æ®ä¸åŒé˜¶æ®µå‡†å¤‡ç›¸åº”çš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
                schedulers_dict = None
                scalers_dict = None
                loaded_stability_tracker = None

                # é¦–æ¬¡åŠ è½½è·å–åŸºæœ¬ä¿¡æ¯
                (
                    step,
                    phase,
                    best_val_loss,
                    best_acc,
                    cn_lr,
                    cd_lr,
                    bqd_lr,
                    loaded_stability_tracker,
                ) = load_checkpoint(
                    resume_from,
                    model_cn,
                    model_cd,
                    model_bqd,
                    None,  # å…ˆä¸åŠ è½½ä¼˜åŒ–å™¨ï¼Œæ ¹æ®phaseå†³å®š
                    None,
                    None,
                    device,
                )

                print(f"æ£€æŸ¥ç‚¹ä¿¡æ¯: é˜¶æ®µ={phase}, æ­¥éª¤={step}")

                # æ ¹æ®åŠ è½½çš„é˜¶æ®µè®¾ç½®ç›¸åº”çš„è®­ç»ƒçŠ¶æ€
                if phase == 1:
                    print("æ¢å¤Phase 1è®­ç»ƒçŠ¶æ€...")
                    # é‡æ–°åŠ è½½ï¼Œä½¿ç”¨æ­£ç¡®çš„ä¼˜åŒ–å™¨
                    (
                        step,
                        phase,
                        best_val_loss,
                        best_acc,
                        cn_lr,
                        cd_lr,
                        bqd_lr,
                        loaded_stability_tracker,
                    ) = load_checkpoint(
                        resume_from,
                        model_cn,
                        model_cd,
                        model_bqd,
                        opt_cn_p1,  # Phase 1ä½¿ç”¨çš„ä¼˜åŒ–å™¨
                        None,  # Phase 1ä¸ä½¿ç”¨CDä¼˜åŒ–å™¨
                        opt_bqd,
                        device,
                        schedulers=None,  # Phase 1é€šå¸¸ä¸ä½¿ç”¨è°ƒåº¦å™¨
                        scalers=None,
                    )

                    step_phase1 = step
                    step_phase2 = 0
                    step_phase3 = 0
                    # best_val_lossä¿æŒåŠ è½½çš„å€¼

                elif phase == 2:
                    print("æ¢å¤Phase 2è®­ç»ƒçŠ¶æ€...")
                    # Phase 2å®Œæˆååˆ‡æ¢åˆ°Phase 3çš„ä¼˜åŒ–å™¨è®¾ç½®
                    (
                        step,
                        phase,
                        best_val_loss,
                        best_acc,
                        cn_lr,
                        cd_lr,
                        bqd_lr,
                        loaded_stability_tracker,
                    ) = load_checkpoint(
                        resume_from,
                        model_cn,
                        model_cd,
                        model_bqd,
                        opt_cn_p1,  # ä¿æŒPhase 1çš„CNä¼˜åŒ–å™¨
                        opt_cd_p2,  # Phase 2çš„CDä¼˜åŒ–å™¨
                        opt_bqd,
                        device,
                        schedulers=None,  # Phase 2çš„è°ƒåº¦å™¨
                        scalers=None,
                    )

                    step_phase1 = steps_1
                    step_phase2 = step
                    step_phase3 = 0
                    # best_accä¿æŒåŠ è½½çš„å€¼

                elif phase == 3:
                    print("æ¢å¤Phase 3è®­ç»ƒçŠ¶æ€...")

                    # Phase 3éœ€è¦åˆå§‹åŒ–è°ƒåº¦å™¨å’Œç¼©æ”¾å™¨
                    # æ³¨æ„ï¼šè¿™äº›å˜é‡åœ¨Phase 3ä»£ç ä¸­æ‰ä¼šåˆ›å»ºï¼Œè¿™é‡Œéœ€è¦å…ˆåˆ›å»º
                    scheduler_cn = torch.optim.lr_scheduler.ReduceLROnPlateau(
                        opt_cn_p3, mode="min", factor=0.7, patience=8, min_lr=1e-6
                    )
                    scheduler_cd = torch.optim.lr_scheduler.ReduceLROnPlateau(
                        opt_cd_p3, mode="max", factor=0.7, patience=5, min_lr=1e-7
                    )

                    scaler_cn = GradScaler(
                        enabled=True, init_scale=2**10, growth_interval=2000
                    )
                    scaler_cd = GradScaler(
                        enabled=True, init_scale=2**10, growth_interval=2000
                    )
                    scaler_bqd_p3 = GradScaler()

                    # å‡†å¤‡è°ƒåº¦å™¨å’Œç¼©æ”¾å™¨å­—å…¸
                    schedulers_dict = {"cn": scheduler_cn, "cd": scheduler_cd}
                    scalers_dict = {
                        "cn": scaler_cn,
                        "cd": scaler_cd,
                        "bqd": scaler_bqd_p3,
                    }

                    # é‡æ–°åŠ è½½ï¼ŒåŒ…å«å®Œæ•´çš„çŠ¶æ€
                    (
                        step,
                        phase,
                        best_val_loss,
                        best_acc,
                        cn_lr,
                        cd_lr,
                        bqd_lr,
                        loaded_stability_tracker,
                    ) = load_checkpoint(
                        resume_from,
                        model_cn,
                        model_cd,
                        model_bqd,
                        opt_cn_p3,  # Phase 3çš„CNä¼˜åŒ–å™¨
                        opt_cd_p3,  # Phase 3çš„CDä¼˜åŒ–å™¨
                        opt_bqd,
                        device,
                        schedulers=schedulers_dict,
                        scalers=scalers_dict,
                    )

                    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!
                    pretrained_weights_path_cd = r"e:\KingCrimson Dataset\Simulate\data0\results37\phase_2\model_cd_step63000"
                    if os.path.exists(pretrained_weights_path_cd):
                        try:
                            model_cd.load_state_dict(
                                torch.load(
                                    pretrained_weights_path_cd,
                                    map_location=device,
                                    weights_only=True,
                                )
                            )
                            print(
                                f"Phase3: Loaded pre-trained weights from {pretrained_weights_path_cd}"
                            )
                        except Exception as e:
                            print(f"åŠ è½½CDé¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")

                    step_phase1 = steps_1
                    step_phase2 = steps_2
                    step_phase3 = step
                    best_val_loss_joint = best_val_loss

                    # æ¢å¤ç¨³å®šæ€§è¿½è¸ªå™¨
                    if loaded_stability_tracker:
                        stability_tracker = loaded_stability_tracker
                        print(
                            f"æ¢å¤ç¨³å®šæ€§è¿½è¸ªå™¨: çœŸå®å‡†ç¡®ç‡EMA={stability_tracker.real_acc_ema:.3f}, "
                            f"å‡æ ·æœ¬å‡†ç¡®ç‡EMA={stability_tracker.fake_acc_ema:.3f}"
                        )
                    else:
                        # å¦‚æœæ²¡æœ‰ä¿å­˜çš„è¿½è¸ªå™¨ï¼Œåˆ›å»ºæ–°çš„
                        stability_tracker = StabilityTracker()
                        print("åˆ›å»ºæ–°çš„ç¨³å®šæ€§è¿½è¸ªå™¨")

                else:
                    raise ValueError(f"æœªçŸ¥çš„è®­ç»ƒé˜¶æ®µ: {phase}")

                # åº”ç”¨å­¦ä¹ ç‡ï¼ˆå¦‚æœæ£€æŸ¥ç‚¹ä¸­ä¿å­˜äº†çš„è¯ï¼‰
                if cn_lr is not None and phase in [1, 3]:
                    if phase == 1:
                        opt_cn_p1.param_groups[0]["lr"] = cn_lr
                        print(f"æ¢å¤Phase 1 CNå­¦ä¹ ç‡: {cn_lr}")
                    elif phase == 3:
                        opt_cn_p3.param_groups[0]["lr"] = cn_lr
                        print(f"æ¢å¤Phase 3 CNå­¦ä¹ ç‡: {cn_lr}")

                if cd_lr is not None and phase in [2, 3]:
                    if phase == 2:
                        opt_cd_p2.param_groups[0]["lr"] = cd_lr
                        print(f"æ¢å¤Phase 2 CDå­¦ä¹ ç‡: {cd_lr}")
                    elif phase == 3:
                        opt_cd_p3.param_groups[0]["lr"] = cd_lr
                        print(f"æ¢å¤Phase 3 CDå­¦ä¹ ç‡: {cd_lr}")

                if bqd_lr is not None:
                    opt_bqd.param_groups[0]["lr"] = bqd_lr
                    print(f"æ¢å¤BQDå­¦ä¹ ç‡: {bqd_lr}")

                print(f"âœ… æˆåŠŸæ¢å¤è®­ç»ƒçŠ¶æ€:")
                print(f"   å½“å‰é˜¶æ®µ: {phase}")
                print(f"   Phase 1 æ­¥éª¤: {step_phase1}")
                print(f"   Phase 2 æ­¥éª¤: {step_phase2}")
                print(f"   Phase 3 æ­¥éª¤: {step_phase3}")
                # å®‰å…¨åœ°æ˜¾ç¤ºæœ€ä½³å€¼
                if best_val_loss is not None and best_val_loss != float("inf"):
                    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                else:
                    print(f"   æœ€ä½³éªŒè¯æŸå¤±: æœªè®¾ç½®")

                if best_acc is not None and best_acc != float("-inf"):
                    print(f"   æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
                else:
                    print(f"   æœ€ä½³å‡†ç¡®ç‡: æœªè®¾ç½®")

                # æ ¹æ®é˜¶æ®µæ˜¾ç¤ºç‰¹å®šä¿¡æ¯
                if phase == 1:
                    print("   -> å°†ç»§ç»­Phase 1è®­ç»ƒï¼ˆç”Ÿæˆå™¨+BQDé¢„è®­ç»ƒï¼‰")
                elif phase == 2:
                    print("   -> å°†ç»§ç»­Phase 2è®­ç»ƒï¼ˆåˆ¤åˆ«å™¨è®­ç»ƒï¼‰")
                elif phase == 3:
                    print("   -> å°†ç»§ç»­Phase 3è®­ç»ƒï¼ˆè”åˆå¯¹æŠ—è®­ç»ƒï¼‰")
                    if loaded_stability_tracker:
                        print(f"   -> ç¨³å®šæ€§è¿½è¸ªå™¨å·²æ¢å¤")

            except Exception as e:
                print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ...")
                # é‡ç½®æ‰€æœ‰è®­ç»ƒçŠ¶æ€
                phase = Phase  # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šçš„é˜¶æ®µ
                step_phase1 = 0
                step_phase2 = 0
                step_phase3 = 0
                best_val_loss = float("inf")
                best_bqd_loss = float("inf")
                best_acc = float("-inf")
                best_val_loss_joint = float("inf")
                cn_lr = None
                cd_lr = None
                bqd_lr = None
                cleanup_memory()
                raise
        else:
            step = 0
            if phase == 1:
                """pretrained_weights_path = r"E:\KingCrimson Dataset\Simulate\data0\results18\phase_1\model_cn_step26500"
                if os.path.exists(pretrained_weights_path):
                    try:
                        model_cn.load_state_dict(torch.load(pretrained_weights_path, map_location=device, weights_only=True))
                        print(f"Phase1: Loaded pre-trained weights from {pretrained_weights_path}")
                    except Exception as e:
                        print(f"åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")"""

            if phase == 2:
                # Load pre-trained weights if available
                pretrained_weights_path = r"e:\KingCrimson Dataset\Simulate\data0\results46\phase_1\model_cn_step50000"
                if os.path.exists(pretrained_weights_path):
                    try:
                        model_cn.load_state_dict(
                            torch.load(
                                pretrained_weights_path,
                                map_location=device,
                                weights_only=True,
                            )
                        )
                        print(
                            f"Phase2: Loaded pre-trained weights from {pretrained_weights_path}"
                        )
                    except Exception as e:
                        print(f"åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
                pretrained_weights_path_bqd = r"e:\KingCrimson Dataset\Simulate\data0\results46\bqd\model_bqd_best"
                if os.path.exists(pretrained_weights_path_bqd):
                    try:
                        model_bqd.load_state_dict(
                            torch.load(
                                pretrained_weights_path_bqd,
                                map_location=device,
                                weights_only=True,
                            )
                        )
                        print(
                            f"Phase3: Loaded pre-trained weights from {pretrained_weights_path_bqd}"
                        )
                    except Exception as e:
                        print(f"åŠ è½½CDé¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
            elif phase == 3:
                # Load pre-trained weights if available
                pretrained_weights_path_cn = r"e:\KingCrimson Dataset\Simulate\data0\results46\phase_1\model_cn_step50000"
                if os.path.exists(pretrained_weights_path_cn):
                    try:
                        model_cn.load_state_dict(
                            torch.load(
                                pretrained_weights_path_cn,
                                map_location=device,
                                weights_only=True,
                            )
                        )
                        print(
                            f"Phase3: Loaded pre-trained weights from {pretrained_weights_path_cn}"
                        )
                    except Exception as e:
                        print(f"åŠ è½½CNé¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")

                pretrained_weights_path_cd = r"e:\KingCrimson Dataset\Simulate\data0\results46\phase_2\model_cd_step50000"
                if os.path.exists(pretrained_weights_path_cd):
                    try:
                        model_cd.load_state_dict(
                            torch.load(
                                pretrained_weights_path_cd,
                                map_location=device,
                                weights_only=True,
                            )
                        )
                        print(
                            f"Phase3: Loaded pre-trained weights from {pretrained_weights_path_cd}"
                        )
                    except Exception as e:
                        print(f"åŠ è½½CDé¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")

                pretrained_weights_path_bqd = r"e:\KingCrimson Dataset\Simulate\data0\results46\bqd\model_bqd_best"
                if os.path.exists(pretrained_weights_path_bqd):
                    try:
                        model_bqd.load_state_dict(
                            torch.load(
                                pretrained_weights_path_bqd,
                                map_location=device,
                                weights_only=True,
                            )
                        )
                        print(
                            f"Phase3: Loaded pre-trained weights from {pretrained_weights_path_bqd}"
                        )
                    except Exception as e:
                        print(f"åŠ è½½CDé¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")

        # Create windows for the different phases and metrics
        loss_windows = {}
        if viz is not None:
            try:
                loss_windows = {
                    "phase1_train": viz.line(
                        Y=torch.zeros((1, 3)),
                        X=torch.tensor([step]),
                        opts=dict(
                            title="Phase 1 Training Loss",
                            legend=["Generator Loss", "Din Loss", "Dout Loss"],
                            linecolor=np.array([[255, 0, 0], [0, 255, 0], [0, 0, 255]]),
                        ),
                    ),
                    "phase1_val": viz.line(
                        Y=torch.zeros((1, 2)),
                        X=torch.tensor([step]),
                        opts=dict(
                            title="Phase 1 Validation Loss",
                            legend=["G loss", "D Loss"],
                            linecolor=np.array([[255, 0, 0], [0, 255, 0]]),
                        ),
                    ),
                    "phase2_disc": viz.line(
                        Y=torch.zeros(1),
                        X=torch.tensor([step]),
                        opts=dict(title="Phase 2 Discriminator Loss"),
                    ),
                    "phase2_acc": viz.line(
                        Y=torch.zeros((1, 3)),
                        X=torch.tensor([step]),
                        opts=dict(
                            title="Phase 2 Discriminator Accuracy",
                            legend=["avg acc", "fake acc", "real acc"],
                            linecolor=np.array([[255, 0, 0], [0, 255, 0], [0, 0, 255]]),
                        ),
                    ),
                    "phase3_gen": viz.line(
                        Y=torch.zeros((1, 3)),
                        X=torch.tensor([step]),
                        opts=dict(
                            title="Phase 3 Generator Loss",
                            legend=[
                                "gen loss",
                                "bqd loss",
                                "boundary loss",
                            ],  # ç§»åˆ°optså†…éƒ¨
                            linecolor=np.array([[255, 0, 0], [0, 255, 0], [0, 0, 255]]),
                        ),
                    ),
                    "phase3_disc": viz.line(
                        Y=torch.zeros(1),
                        X=torch.tensor([step]),
                        opts=dict(title="Phase 3 Discriminator Loss"),
                    ),
                    "phase3_val": viz.line(
                        Y=torch.zeros((1, 6)),
                        X=torch.tensor([step]),
                        opts=dict(
                            title="Phase 3 Validation Loss",
                            legend=[
                                "Recon Loss",
                                "Adv Loss",
                                "Real Acc",
                                "Fake Acc",
                                "Disc Acc",
                                "BQD Loss",
                            ],
                            linecolor=np.array(
                                [
                                    [255, 0, 0],
                                    [0, 255, 0],
                                    [0, 0, 255],
                                    [255, 255, 0],
                                    [0, 255, 255],
                                    [177, 177, 0],
                                ]
                            ),
                        ),
                    ),
                    "phase3_fake_acc": viz.line(
                        Y=torch.zeros((1, 2)),
                        X=torch.tensor([step]),
                        opts=dict(
                            title="Phase 3 Discriminator Accuracy",
                            legend=["Fake Acc", "Real Acc"],
                            linecolor=np.array([[255, 0, 0], [0, 0, 255]]),
                        ),
                    ),
                }
            except Exception as e:
                print(f"åˆ›å»ºVisdomçª—å£å¤±è´¥: {e}")
                traceback.print_exc()  # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
                loss_windows = {}

        # åœ¨æ‰¹æ¬¡å¤§å°é…ç½®éƒ¨åˆ†
        if use_multi_gpu:
            # æ¯ä¸ªGPUçš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
            per_gpu_batch_size = batch_size
            # æ€»çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
            effective_batch_size = batch_size * torch.cuda.device_count()
            print(
                f"å¤šGPUè®­ç»ƒ: æ¯GPUæ‰¹æ¬¡={per_gpu_batch_size}, æ€»æ‰¹æ¬¡={effective_batch_size}"
            )
        else:
            effective_batch_size = batch_size
        target_batch_size = 32  # ç›®æ ‡æœ‰æ•ˆbatch size
        actual_batch_size = batch_size  # å½“å‰çš„batch_size (8)
        if use_multi_gpu:
            # å¤šGPUæ—¶ï¼Œå®é™…çš„æœ‰æ•ˆbatch sizeæ˜¯ actual_batch_size * num_gpus
            effective_batch_size = actual_batch_size * torch.cuda.device_count()
        else:
            effective_batch_size = actual_batch_size

        accumulation_steps = max(1, target_batch_size // effective_batch_size)
        print(
            f"é…ç½®æ¢¯åº¦ç´¯ç§¯: {accumulation_steps} steps, æœ‰æ•ˆbatch size: {target_batch_size}"
        )

        def visualize_results(
            local_targets,
            local_inputs,
            outputs,
            edge_inputs=None,
            edge_outputs=None,
            edge_targets=None,
            completed=None,
            step=0,
            phase="phase_1",
            num_test_completions=8,
        ):
            """
            åœ¨Visdomä¸­å¯è§†åŒ–è¾“å…¥ã€ç›®æ ‡ã€è¾“å‡ºå’Œå®Œæˆçš„å›¾åƒï¼Œä½¿ç”¨é«˜ç¨‹å›¾ç€è‰²æ–¹æ¡ˆã€‚
            å¢åŠ edge_inputså’Œedge_outputsçš„å¯¹æ¯”å¯è§†åŒ–ã€‚
            """
            if viz is None:
                return

            try:

                def apply_colormap(tensor, grayscale=False):
                    """å°†å¼ é‡è½¬æ¢ä¸ºå¸¦æœ‰é«˜ç¨‹å›¾ç€è‰²çš„PILå›¾åƒ"""
                    # ç¡®ä¿è¾“å…¥æ˜¯2Då¼ é‡(å•é€šé“å›¾åƒ)æˆ–å–ç¬¬ä¸€ä¸ªé€šé“
                    if tensor.dim() > 2:
                        if tensor.size(0) == 1:
                            tensor = tensor.squeeze(0)
                        else:
                            tensor = tensor[0]

                    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
                    np_array = tensor.detach().cpu().numpy()
                    min_val = np_array.min()
                    max_val = np_array.max()
                    norm_array = (np_array - min_val) / (max_val - min_val + 1e-8)

                    if grayscale:
                        # ç›´æ¥ä½¿ç”¨å½’ä¸€åŒ–çš„æ•°ç»„ä½œä¸ºç°åº¦å›¾
                        gray_array = (norm_array * 255).astype(np.uint8)
                        colored_image = Image.fromarray(gray_array, mode="L")
                    else:
                        # åº”ç”¨matplotlibçš„jetè‰²å½©æ˜ å°„
                        colored_array = plt.cm.jet(norm_array)
                        # è½¬æ¢ä¸ºPILå›¾åƒ (colored_arrayæ˜¯RGBAæ ¼å¼)
                        colored_array_rgb = (colored_array[:, :, :3] * 255).astype(
                            np.uint8
                        )
                        colored_image = Image.fromarray(colored_array_rgb)

                    return colored_image

                # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨çš„æ ·æœ¬æ•°é‡
                num_samples = min(num_test_completions, len(local_targets))

                # å¤„ç†å¹¶ç€è‰²å°å—å›¾åƒ
                colored_patches = []
                labels = ["ç›®æ ‡", "è¾“å…¥", "è¾“å‡º"]
                patch_tensors = [
                    local_targets[:num_samples],
                    local_inputs[:num_samples],
                    outputs[:num_samples],
                ]

                for i, tensor_batch in enumerate(patch_tensors):
                    for j in range(len(tensor_batch)):
                        colored_img = apply_colormap(tensor_batch[j])
                        colored_patches.append(colored_img)

                # åˆ›å»ºç½‘æ ¼å¸ƒå±€
                cols = num_samples
                rows = 3
                grid_width = (colored_patches[0].width + 5) * cols
                grid_height = (colored_patches[0].height + 5) * rows
                grid_img = Image.new("RGB", (grid_width, grid_height))

                # å¡«å……ç½‘æ ¼
                for i in range(rows):
                    for j in range(cols):
                        idx = i * cols + j
                        if idx < len(colored_patches):
                            x = j * (colored_patches[idx].width + 5)
                            y = i * (colored_patches[idx].height + 5)
                            grid_img.paste(colored_patches[idx], (x, y))

                # æ”¾å¤§å›¾åƒ3å€
                grid_img = grid_img.resize(
                    (grid_img.width * 3, grid_img.height * 3), Image.NEAREST
                )

                viz.image(
                    np.array(grid_img).transpose(2, 0, 1),
                    opts=dict(
                        caption=f"{phase} local patch step {step}",
                        title=f"{phase} local patch",
                    ),
                    win=f"{phase}_patches",
                )

                # å¤„ç†completedå›¾åƒ
                if completed is not None:
                    completed_samples = min(num_samples, len(completed))
                    colored_completed = []

                    for i in range(completed_samples):
                        colored_img = apply_colormap(completed[i])
                        colored_completed.append(colored_img)

                    # åˆ›å»ºcompletedå›¾åƒç½‘æ ¼
                    cols_c = min(2, completed_samples)
                    rows_c = (completed_samples + cols_c - 1) // cols_c
                    completed_grid_width = (colored_completed[0].width + 5) * cols_c
                    completed_grid_height = (colored_completed[0].height + 5) * rows_c
                    completed_grid_img = Image.new(
                        "RGB", (completed_grid_width, completed_grid_height)
                    )

                    # å¡«å……ç½‘æ ¼
                    for i in range(completed_samples):
                        x = (i % cols_c) * (colored_completed[i].width + 5)
                        y = (i // cols_c) * (colored_completed[i].height + 5)
                        completed_grid_img.paste(colored_completed[i], (x, y))

                    # ç¼©å°å›¾åƒä¸ºåŸæ¥çš„ä¸€åŠ
                    completed_grid_img = completed_grid_img.resize(
                        (completed_grid_img.width // 2, completed_grid_img.height // 2),
                        Image.Resampling.LANCZOS,
                    )

                    viz.image(
                        np.array(completed_grid_img).transpose(2, 0, 1),
                        opts=dict(
                            caption=f"{phase} global patch step {step}",
                            title=f"{phase} global patch",
                        ),
                        win=f"{phase}_completed",
                    )

                # ========== å¢åŠ edge_inputsã€edge_outputså’Œedge_targetsä¸‰è¡Œå¯¹æ¯”å¯è§†åŒ–ï¼ˆç°åº¦å›¾ï¼‰ ==========
                if edge_inputs is not None and edge_outputs is not None:
                    num_edge = min(num_samples, len(edge_inputs), len(edge_outputs))
                    # æ£€æŸ¥æ˜¯å¦æœ‰edge_targets
                    has_edge_targets = (
                        edge_targets is not None and len(edge_targets) >= num_edge
                    )

                    colored_edge_in = []
                    colored_edge_out = []
                    colored_edge_target = []

                    # ç”Ÿæˆç°åº¦å›¾åƒ
                    for i in range(num_edge):
                        colored_edge_in.append(
                            apply_colormap(edge_inputs[i], grayscale=True)
                        )
                        colored_edge_out.append(
                            apply_colormap(edge_outputs[i], grayscale=True)
                        )
                        if has_edge_targets:
                            colored_edge_target.append(
                                apply_colormap(edge_targets[i], grayscale=True)
                            )

                    # åˆ›å»º3è¡Œnum_edgeåˆ—çš„ç½‘æ ¼ï¼Œç¬¬ä¸€è¡Œä¸ºinï¼Œç¬¬äºŒè¡Œä¸ºoutï¼Œç¬¬ä¸‰è¡Œä¸ºtarget
                    edge_width = colored_edge_in[0].width
                    edge_height = colored_edge_in[0].height
                    gap_size = 5
                    gray_value = 128  # ä¸­ç­‰ç°åº¦å€¼ (0-255èŒƒå›´)

                    edge_grid_width = (edge_width + gap_size) * num_edge
                    edge_grid_height = (edge_height + gap_size) * (
                        3 if has_edge_targets else 2
                    )

                    # åˆ›å»ºç°åº¦å›¾ç½‘æ ¼ï¼Œä½¿ç”¨ç°è‰²èƒŒæ™¯
                    edge_grid_img = Image.new(
                        "L", (edge_grid_width, edge_grid_height), color=gray_value
                    )

                    # ç¬¬ä¸€è¡Œ: edge_inputs
                    for i in range(num_edge):
                        x = i * (edge_width + gap_size)
                        y = 0
                        edge_grid_img.paste(colored_edge_target[i], (x, y))

                    # ç¬¬äºŒè¡Œ: edge_outputs
                    for i in range(num_edge):
                        x = i * (edge_width + gap_size)
                        y = edge_height + gap_size
                        edge_grid_img.paste(colored_edge_in[i], (x, y))

                    # ç¬¬ä¸‰è¡Œ: edge_targets (å¦‚æœå­˜åœ¨)
                    if has_edge_targets:
                        for i in range(num_edge):
                            x = i * (edge_width + gap_size)
                            y = (edge_height + gap_size) * 2
                            edge_grid_img.paste(colored_edge_out[i], (x, y))

                    # æ”¾å¤§å’Œlocal_inputsä¸€æ ·
                    edge_grid_img = edge_grid_img.resize(
                        (edge_grid_img.width * 3, edge_grid_img.height * 3),
                        Image.NEAREST,
                    )

                    # è½¬æ¢ä¸ºnumpyæ•°ç»„æ—¶éœ€è¦æ³¨æ„ç°åº¦å›¾çš„å¤„ç†
                    edge_array = np.array(edge_grid_img)
                    # ç°åº¦å›¾éœ€è¦æ·»åŠ é€šé“ç»´åº¦ç»™Visdom
                    if edge_array.ndim == 2:
                        edge_array = edge_array[np.newaxis, :, :]  # æ·»åŠ é€šé“ç»´åº¦

                    # æ„å»ºcaptionï¼Œæ ¹æ®æ˜¯å¦æœ‰targetsè°ƒæ•´
                    caption_text = f"{phase} edge extraction step {step}"
                    if has_edge_targets:
                        caption_text += " (Input/Output/Target)"
                    else:
                        caption_text += " (Input/Output)"

                    viz.image(
                        edge_array,
                        opts=dict(
                            caption=caption_text,
                            title=f"{phase} edge extraction",
                        ),
                        win=f"{phase}_edge_compare",
                    )

                    # æ¸…ç†edgeç›¸å…³å˜é‡
                    del colored_edge_in, colored_edge_out
                    if has_edge_targets:
                        del colored_edge_target
                    del edge_grid_img

                # æ¸…ç†å˜é‡
                del colored_patches, grid_img
                if "colored_completed" in locals():
                    del colored_completed, completed_grid_img
                cleanup_memory()

            except Exception as e:
                print(f"å¯è§†åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                traceback.print_exc()  # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
                cleanup_memory()

        # Helper function to evaluate model on validation set
        def validate(model_cn, model_bqd, val_subset_loader, device):
            """ä¼˜åŒ–çš„éªŒè¯å‡½æ•°ï¼Œå¢åŠ é”™è¯¯å¤„ç†å’Œå†…å­˜ç®¡ç†"""
            try:
                model_cn.eval()
                val_loss = 0.0
                val_bqd_loss = 0.0
                num_batches = 0

                with torch.no_grad():
                    for batch in val_subset_loader:
                        try:
                            # Prepare batch data
                            batch_data = prepare_batch_data(batch, device)
                            (
                                batch_local_inputs,
                                batch_local_masks,
                                batch_local_targets,
                                batch_global_inputs,
                                batch_global_masks,
                                batch_global_targets,
                                metadata,
                            ) = batch_data

                            # Forward pass with memory management
                            outputs = safe_tensor_operation(
                                model_cn,
                                batch_local_inputs,
                                batch_local_masks,
                                batch_global_inputs,
                                batch_global_masks,
                            )

                            completed, completed_mask, pos = merge_local_to_global(
                                outputs,
                                batch_global_inputs,
                                batch_global_masks,
                                metadata,
                            )

                            # Compute loss
                            loss = completion_network_loss(
                                outputs,
                                batch_local_targets,
                                batch_local_masks,
                                batch_global_targets,
                                completed,
                                completed_mask,
                                pos,
                            )
                            val_loss += loss.item()

                            nld_li = normalization(
                                batch_local_inputs, batch_global_inputs
                            )
                            nld_lo = normalization(outputs, batch_global_inputs)
                            nld_lt = normalization(
                                batch_local_targets, batch_global_inputs
                            )

                            output1, loss1, _ = model_bqd(nld_li, batch_local_masks)
                            output2, loss2, loss2_mask = model_bqd(
                                nld_lo, batch_local_masks
                            )
                            nld_lt = nld_lt.unsqueeze(1)
                            # print(f"local size: {nld_li.size()}, mask size: {torch.zeros_like(outputs).size()}")
                            # print(f"nld_lt size: {nld_lt.size()}, nld_lo size: {nld_lo.size()}")
                            output3, _, _ = model_bqd(nld_lt, torch.zeros_like(nld_lt))
                            loss_bqd = (loss1 + loss2) / 2
                            val_bqd_loss += loss_bqd.item()
                            val_loss += loss2_mask.item()

                            num_batches += 1

                            # æ¸…ç†ä¸´æ—¶å˜é‡
                            del (
                                # outputs,
                                # completed,
                                completed_mask,
                                pos,
                                loss,
                                # output1,
                                # output2,
                                loss1,
                                loss2,
                                loss_bqd,
                            )
                            del batch_data

                        except Exception as e:
                            print(f"éªŒè¯æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                            traceback.print_exc()  # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
                            cleanup_memory()
                            continue

                # Calculate average validation loss
                avg_val_loss = val_loss / max(num_batches, 1)
                avg_bqd_val_loss = val_bqd_loss / max(num_batches, 1)
                model_cn.train()
                model_bqd.train()

                return (
                    avg_val_loss,
                    avg_bqd_val_loss,
                    batch_local_targets,
                    batch_local_inputs,
                    outputs,
                    output1,
                    output2,
                    output3,
                    completed,
                )

            except Exception as e:
                print(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                cleanup_memory()
                model_cn.train()
                return float("inf")

        def validateAll(model_cn, model_cd, model_bqd, val_subset_loader, device):
            """å…¨é¢éªŒè¯ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æ€§èƒ½ï¼Œä¼˜åŒ–å†…å­˜ç®¡ç†"""
            try:
                model_cn.eval()
                model_cd.eval()
                model_bqd.eval()

                # åˆå§‹åŒ–å„ç§æŸå¤±å’ŒæŒ‡æ ‡
                recon_loss_sum = 0.0
                adv_loss_sum = 0.0
                bqd_loss_sum = 0.0
                real_acc_sum = 0.0
                fake_acc_sum = 0.0
                total_batches = 0

                with torch.no_grad():
                    for batch in val_subset_loader:
                        try:
                            total_batches += 1

                            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                            batch_data = prepare_batch_data(batch, device)
                            (
                                batch_local_inputs,
                                batch_local_masks,
                                batch_local_targets,
                                batch_global_inputs,
                                batch_global_masks,
                                batch_global_targets,
                                metadata,
                            ) = batch_data

                            # 1. è¯„ä¼°ç”Ÿæˆå™¨é‡å»ºæ€§èƒ½
                            outputs = safe_tensor_operation(
                                model_cn,
                                batch_local_inputs,
                                batch_local_masks,
                                batch_global_inputs,
                                batch_global_masks,
                            )

                            # è®¡ç®—å…¨å±€å®Œæˆç»“æœ
                            completed, completed_mask, pos = merge_local_to_global(
                                outputs,
                                batch_global_inputs,
                                batch_global_masks,
                                metadata,
                            )

                            # è®¡ç®—é‡å»ºæŸå¤±
                            recon_loss = completion_network_loss(
                                outputs,
                                batch_local_targets,
                                batch_local_masks,
                                batch_global_targets,
                                completed,
                                completed_mask,
                                pos,
                            )
                            recon_loss_sum += recon_loss.item()

                            # 2. è¯„ä¼°å¯¹æŠ—æ€§æ€§èƒ½
                            batch_size = batch_local_targets.size(0)
                            real_labels = torch.ones(batch_size, 1, device=device)
                            fake_labels = torch.zeros(batch_size, 1, device=device)

                            nld_o = normalization(outputs, batch_global_inputs)
                            _, _, mask_loss = model_bqd(nld_o, batch_local_masks)
                            mask_loss = ensure_scalar_loss(mask_loss)
                            bqd_loss_sum += mask_loss.item()
                            # recon_loss_sum += mask_loss.item()

                            # å°†ç”Ÿæˆå™¨è¾“å‡ºä¼ é€’ç»™åˆ¤åˆ«å™¨
                            fake_global_embedded, fake_global_mask_embedded, _ = (
                                merge_local_to_global(
                                    outputs,
                                    batch_global_inputs,
                                    batch_global_masks,
                                    metadata,
                                )
                            )

                            fake_predictions, _, _ = safe_tensor_operation(
                                model_cd,
                                outputs,
                                batch_local_masks,
                                fake_global_embedded,
                                fake_global_mask_embedded,
                            )

                            # å°†çœŸå®æ ·æœ¬ä¼ é€’ç»™åˆ¤åˆ«å™¨
                            real_global_embedded, real_global_mask_embedded, _ = (
                                merge_local_to_global(
                                    batch_local_targets,
                                    batch_global_inputs,
                                    batch_global_masks,
                                    metadata,
                                )
                            )

                            real_predictions, _, _ = safe_tensor_operation(
                                model_cd,
                                batch_local_targets,
                                batch_local_masks,
                                real_global_embedded,
                                real_global_mask_embedded,
                            )

                            # è®¡ç®—å¯¹æŠ—æ€§æŸå¤± - ç”Ÿæˆå™¨è§†è§’ï¼ˆå¸Œæœ›åˆ¤åˆ«å™¨å°†ç”Ÿæˆæ ·æœ¬è¯†åˆ«ä¸ºçœŸï¼‰
                            adv_loss = F.binary_cross_entropy_with_logits(
                                fake_predictions, real_labels
                            )
                            adv_loss_sum += adv_loss.item()

                            # è®¡ç®—åˆ¤åˆ«å™¨å‡†ç¡®ç‡
                            real_acc = (
                                (torch.sigmoid(real_predictions) >= 0.5)
                                .float()
                                .mean()
                                .item()
                            )
                            fake_acc = (
                                (torch.sigmoid(fake_predictions) < 0.5)
                                .float()
                                .mean()
                                .item()
                            )

                            real_acc_sum += real_acc
                            fake_acc_sum += fake_acc

                            # æ¸…ç†ä¸´æ—¶å˜é‡
                            del outputs, completed, completed_mask, pos, recon_loss
                            del fake_global_embedded, fake_global_mask_embedded
                            del real_global_embedded, real_global_mask_embedded
                            del fake_predictions, real_predictions, adv_loss
                            del batch_data

                        except Exception as e:
                            print(f"éªŒè¯æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                            cleanup_memory()
                            continue

                # è®¡ç®—å¹³å‡å€¼
                avg_recon_loss = (
                    recon_loss_sum + 0.1 * adv_loss_sum + bqd_loss_sum * 0.1
                ) / max(total_batches, 1)
                avg_adv_loss = adv_loss_sum / max(total_batches, 1)
                avg_real_acc = real_acc_sum / max(total_batches, 1)
                avg_fake_acc = fake_acc_sum / max(total_batches, 1)
                avg_disc_acc = (avg_real_acc + avg_fake_acc) / 2
                bqd_loss = bqd_loss_sum / max(total_batches, 1)

                # å°†æ¨¡å‹è®¾å›è®­ç»ƒæ¨¡å¼
                model_cn.train()
                model_cd.train()
                model_bqd.train()

                return {
                    "recon_loss": avg_recon_loss,
                    "adv_loss": avg_adv_loss,
                    "real_acc": avg_real_acc,
                    "fake_acc": avg_fake_acc,
                    "disc_acc": avg_disc_acc,
                    "bqd_loss": bqd_loss,
                }

            except Exception as e:
                print(f"å…¨é¢éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                cleanup_memory()
                model_cn.train()
                model_cd.train()
                model_bqd.train()
                return {
                    "recon_loss": float("inf"),
                    "adv_loss": float("inf"),
                    "real_acc": 0.0,
                    "fake_acc": 0.0,
                    "disc_acc": 0.0,
                    "bqd_loss": 0.0,
                }

        def merge_local_to_global(local_data, global_data, global_mask, metadata):
            """
            Merge local patch data into global data based on metadata.

            metadataæ ¼å¼: [center_y, center_x, local_size, sample_idx, coverage_ratio, missing_ratio, local_size]
            å…¶ä¸­center_y, center_xæ˜¯localä¸­å¿ƒåœ¨globalä¸­çš„ä½ç½®
            """
            try:
                local_data = local_data.unsqueeze(1)  # ç¡®ä¿local_dataæ˜¯4Då¼ é‡
                merged_data = global_data.clone()
                merged_mask = global_mask.clone()
                pos = []

                for i in range(len(metadata)):
                    # æå–metadataä¿¡æ¯
                    center_y = int(metadata[i][0])  # localä¸­å¿ƒåœ¨globalä¸­çš„yåæ ‡
                    center_x = int(metadata[i][1])  # localä¸­å¿ƒåœ¨globalä¸­çš„xåæ ‡
                    local_size = int(metadata[i][2])  # localå°ºå¯¸

                    # è®¡ç®—localåŒºåŸŸåœ¨globalä¸­çš„è¾¹ç•Œ
                    half_size = local_size // 2
                    min_y = center_y - half_size
                    max_y = center_y + half_size + 1  # +1å› ä¸ºPythonåˆ‡ç‰‡æ˜¯å·¦é—­å³å¼€
                    min_x = center_x - half_size
                    max_x = center_x + half_size + 1

                    # è¾¹ç•Œæ£€æŸ¥
                    if (
                        min_y < 0
                        or min_x < 0
                        or max_y > global_data.size(2)
                        or max_x > global_data.size(3)
                    ):
                        print(
                            f"è­¦å‘Š: æ ·æœ¬{i}ä½ç½®è¶Šç•Œ - center:({center_y},{center_x}), size:{local_size}"
                        )
                        print(f"       èŒƒå›´: y[{min_y},{max_y}), x[{min_x},{max_x})")
                        print(f"       global size: {global_data.shape}")
                        continue

                    # åµŒå…¥localæ•°æ®åˆ°globalä¸­
                    merged_data[i, :, min_y:max_y, min_x:max_x] = local_data[i, :, :, :]
                    merged_mask[i, :, min_y:max_y, min_x:max_x] = 1

                    pos.append([min_y, max_y, min_x, max_x])

                return merged_data, merged_mask, pos

            except Exception as e:
                print(f"åˆå¹¶é”™è¯¯: {e}")
                print(f"local_data shape: {local_data.shape}")
                print(f"global_data shape: {global_data.shape}")
                for i, meta in enumerate(metadata):
                    print(
                        f"metadata[{i}]: center=({meta[0]},{meta[1]}), size={meta[2]}"
                    )
                raise

        def normalization(inputs, global_inputs):
            # print(f"normalization inputs shape: {inputs.shape}, global_inputs shape: {global_inputs.shape}")
            gmin = global_inputs.min()
            gmax = global_inputs.max()

            # é¿å…é™¤é›¶é”™è¯¯
            if gmax - gmin > 1e-8:
                # å½’ä¸€åŒ–åˆ°[0, 1]
                inputs = (inputs - gmin) / (gmax - gmin)
            return inputs

        # =================================================
        # Training Phase 1: Train Completion Network only
        # =================================================

        if step_phase1 < steps_1 and phase == 1:
            print("å¼€å§‹/ç»§ç»­ç¬¬1é˜¶æ®µè®­ç»ƒ...")
            try:
                model_cn.train()
                model_bqd.train()
                pbar = tqdm(total=steps_1, initial=step_phase1)
                step = step_phase1
                scaler = GradScaler()
                scaler_bqd = GradScaler()
                opt_cn_p1 = Adadelta(model_cn.parameters())

                if bqd_lr is not None:
                    opt_bqd.param_groups[0]["lr"] = bqd_lr

                # æ¢¯åº¦ç´¯ç§¯è®¡æ•°å™¨
                accumulated_step = 0

                while step < steps_1:
                    for batch in train_loader:
                        try:
                            # å®šæœŸæ£€æŸ¥å†…å­˜
                            if step % 100 == 0:
                                check_memory_and_cleanup(threshold_gb=6.0)

                            # Prepare batch data
                            batch_data = prepare_batch_data(batch, device)
                            (
                                batch_local_inputs,
                                batch_local_masks,
                                batch_local_targets,
                                batch_global_inputs,
                                batch_global_masks,
                                batch_global_targets,
                                metadata,
                            ) = batch_data

                            # ==================== ä¿®å¤ï¼šæ­£ç¡®å®ç°æ¢¯åº¦ç´¯ç§¯ ====================

                            #### ç¬¬1æ¬¡backward - BQDç½‘ç»œå­¦ä¹ è¯†åˆ«ç”Ÿæˆè¾“å‡ºè¾¹ç¼˜ ####
                            for param in model_cn.parameters():
                                param.requires_grad = False
                            for param in model_bqd.parameters():
                                param.requires_grad = True

                            with torch.no_grad():
                                outputs = safe_tensor_operation(
                                    model_cn,
                                    batch_local_inputs,
                                    batch_local_masks,
                                    batch_global_inputs,
                                    batch_global_masks,
                                )
                            nld_o = normalization(outputs, batch_global_inputs)
                            bqd_outputs_out, bqd_loss_out, _ = model_bqd(
                                nld_o.detach(),
                                batch_local_masks,
                            )
                            bqd_loss_out = ensure_scalar_loss(bqd_loss_out)
                            bqd_loss_out_scaled = bqd_loss_out / accumulation_steps
                            scaler_bqd.scale(bqd_loss_out_scaled).backward(
                                retain_graph=True
                            )
                            # bqd_loss_out_scaled.backward(retain_graph=True)

                            #### ç¬¬2æ¬¡backward - BQDç½‘ç»œå­¦ä¹ è¯†åˆ«åŸå§‹è¾“å…¥è¾¹ç¼˜ ####\
                            nld_li = normalization(
                                batch_local_inputs, batch_global_inputs
                            )
                            bqd_outputs_in, bqd_loss_in, _ = model_bqd(
                                nld_li, batch_local_masks
                            )
                            bqd_loss_in = ensure_scalar_loss(bqd_loss_in)
                            bqd_loss_in_scaled = bqd_loss_in / accumulation_steps
                            scaler_bqd.scale(bqd_loss_in_scaled).backward(
                                retain_graph=True
                            )
                            # bqd_loss_in_scaled.backward(retain_graph=True)

                            #### ç¬¬3æ¬¡backward - è¡¥å…¨ç½‘ç»œçš„é‡å»º+å¯¹æŠ—æŸå¤± ####
                            for param in model_bqd.parameters():
                                param.requires_grad = False
                            for param in model_cn.parameters():
                                param.requires_grad = True

                            # Forward pass for completion network
                            outputs = safe_tensor_operation(
                                model_cn,
                                batch_local_inputs,
                                batch_local_masks,
                                batch_global_inputs,
                                batch_global_masks,
                            )

                            completed, completed_mask, pos = merge_local_to_global(
                                outputs,
                                batch_global_inputs,
                                batch_global_masks,
                                metadata,
                            )

                            # é‡å»ºæŸå¤±
                            recon_loss = completion_network_loss(
                                outputs,
                                batch_local_targets,
                                batch_local_masks,
                                batch_global_targets,
                                completed,
                                completed_mask,
                                pos,
                            )

                            # BQDå¯¹æŠ—æŸå¤±ï¼ˆè®©ç”Ÿæˆç»“æœæ¬ºéª—BQDï¼‰
                            nld_lo = normalization(outputs, batch_global_inputs)
                            _, _, bqd_loss_mask_adversarial = model_bqd(
                                nld_lo, batch_local_masks
                            )
                            bqd_loss_mask_adversarial = ensure_scalar_loss(
                                bqd_loss_mask_adversarial
                            )

                            # ç»„åˆæŸå¤±å¹¶åº”ç”¨æ¢¯åº¦ç´¯ç§¯ç¼©æ”¾
                            total_cn_loss = (
                                recon_loss + bqd_loss_mask_adversarial
                            ) / accumulation_steps
                            # scaler.scale(total_cn_loss).backward()
                            total_cn_loss.backward()

                            accumulated_step += 1

                            # ==================== ä¿®å¤ï¼šæ­£ç¡®çš„æ¢¯åº¦ç´¯ç§¯æ›´æ–° ====================
                            if accumulated_step % accumulation_steps == 0:
                                # æ›´æ–°è¡¥å…¨ç½‘ç»œ
                                # scaler.unscale_(opt_cn_p1)
                                torch.nn.utils.clip_grad_norm_(
                                    model_cn.parameters(), max_norm=1.0
                                )
                                # scaler.step(opt_cn_p1)
                                opt_cn_p1.step()
                                # scaler.update()
                                opt_cn_p1.zero_grad(set_to_none=True)
                            if accumulated_step % (accumulation_steps * 2) == 0:
                                # æ›´æ–°BQDç½‘ç»œ
                                scaler_bqd.unscale_(opt_bqd)
                                torch.nn.utils.clip_grad_norm_(
                                    model_bqd.parameters(), max_norm=1.0
                                )
                                scaler_bqd.step(opt_bqd)
                                # opt_bqd.step()
                                scaler_bqd.update()
                                opt_bqd.zero_grad(set_to_none=True)

                            # Update Visdom plot for training loss
                            if (
                                viz is not None
                                and "phase1_train" in loss_windows
                                and step % 100 == 0
                            ):
                                try:
                                    y0 = (
                                        recon_loss.item()
                                        if recon_loss.item() < 1000
                                        else -1
                                    )
                                    y = torch.tensor(
                                        [y0, bqd_loss_in.item(), bqd_loss_out.item()]
                                    ).float()
                                    viz.line(
                                        Y=y.unsqueeze(0),  # shape (1, 3)
                                        X=torch.tensor([step]).float(),
                                        win=loss_windows["phase1_train"],
                                        update="append",
                                    )
                                except Exception as e:
                                    print(f"Visdom phase1_train plot error: {e}")
                                    pass

                            # åœ¨æ¯ä¸ªsnaperiod_1æ­¥ä¿å­˜æ£€æŸ¥ç‚¹
                            if step % snaperiod_1 == 0:
                                try:
                                    # å‡†å¤‡Phase 1çš„çŠ¶æ€ä¿¡æ¯
                                    phase1_schedulers = None  # Phase 1é€šå¸¸ä¸ä½¿ç”¨è°ƒåº¦å™¨
                                    phase1_scalers = {
                                        "cn": scaler,  # Phase 1çš„CNç¼©æ”¾å™¨
                                        "bqd": scaler_bqd,  # Phase 1çš„BQDç¼©æ”¾å™¨
                                    }

                                    """save_checkpoint(
                                        model_cn,
                                        None,  # Phase 1ä¸ä¿å­˜CDæ¨¡å‹
                                        model_bqd,
                                        opt_cn_p1,  # Phase 1çš„CNä¼˜åŒ–å™¨
                                        None,  # Phase 1ä¸ä½¿ç”¨CDä¼˜åŒ–å™¨
                                        opt_bqd,
                                        step,
                                        1,  # phase = 1
                                        result_dir,
                                        best_val_loss=best_val_loss,
                                        best_acc=None,  # Phase 1ä¸è¿½è¸ªå‡†ç¡®ç‡
                                        cn_lr=opt_cn_p1.param_groups[0]["lr"],
                                        cd_lr=None,  # Phase 1ä¸ä½¿ç”¨CD
                                        bqd_lr=opt_bqd.param_groups[0]["lr"],
                                        schedulers=phase1_schedulers,
                                        scalers=phase1_scalers,
                                        stability_tracker=None,  # Phase 1ä¸ä½¿ç”¨ç¨³å®šæ€§è¿½è¸ªå™¨
                                    )"""

                                    print(f"âœ… Phase 1 æ£€æŸ¥ç‚¹å·²ä¿å­˜ (step {step})")

                                except Exception as e:
                                    print(f"âŒ Phase 1 ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

                            step += 1
                            pbar.set_description(
                                f"Phase 1 | train loss: {recon_loss.item():.5f}, adv loss: {bqd_loss_mask_adversarial.item():.5f}"
                            )
                            pbar.update(1)

                            # æ¸…ç†ä¸´æ—¶å˜é‡
                            del (
                                outputs,
                                completed,
                                completed_mask,
                                pos,
                                recon_loss,
                                bqd_outputs_in,
                                bqd_loss_in,
                                bqd_outputs_out,
                                bqd_loss_out,
                            )
                            del batch_data

                            # Testing and saving snapshots
                            if step % snaperiod_1 == 0:
                                try:

                                    (
                                        val_loss,
                                        val_bqd_loss,
                                        val_local_targets,
                                        val_local_inputs,
                                        test_output,
                                        val_in,
                                        val_out,
                                        val_target,
                                        completed,
                                    ) = validate(
                                        model_cn, model_bqd, val_subset_loader, device
                                    )
                                    print(
                                        f"Step {step}, Validation Loss: {val_loss:.5f}"
                                    )

                                    # Update Visdom plot for validation loss
                                    if viz is not None and "phase1_val" in loss_windows:
                                        try:
                                            viz.line(
                                                Y=torch.tensor(
                                                    [[val_loss, val_bqd_loss]]
                                                ),
                                                X=torch.tensor([step]),
                                                win=loss_windows["phase1_val"],
                                                update="append",
                                            )
                                        except:
                                            pass

                                    # Save if best model
                                    if val_loss < best_val_loss:
                                        best_val_loss = val_loss
                                        best_model_path = os.path.join(
                                            result_dir, "phase_1", "model_cn_best"
                                        )
                                        torch.save(
                                            model_cn.state_dict(), best_model_path
                                        )

                                    if val_bqd_loss < best_bqd_loss:
                                        best_bqd_loss = val_bqd_loss
                                        best_bqd_model_path = os.path.join(
                                            result_dir, "bqd", "model_bqd_best"
                                        )
                                        torch.save(
                                            model_bqd.state_dict(), best_bqd_model_path
                                        )

                                    """# Generate some test completions
                                    model_cn.eval()
                                    model_bqd.eval()
                                    with torch.no_grad():
                                        try:
                                            # Get a batch from validation set
                                            val_batch = next(iter(val_subset_loader))

                                            # Prepare validation batch
                                            val_data = prepare_batch_data(
                                                val_batch, device
                                            )
                                            (
                                                val_local_inputs,
                                                val_local_masks,
                                                val_local_targets,
                                                val_global_inputs,
                                                val_global_masks,
                                                val_global_targets,
                                                metadata,
                                            ) = val_data

                                            # Generate completions
                                            test_output = safe_tensor_operation(
                                                model_cn,
                                                val_local_inputs,
                                                val_local_masks,
                                                val_global_inputs,
                                                val_global_masks,
                                            )

                                            # Merge local output to global for visualization
                                            completed, completed_mask, _ = (
                                                merge_local_to_global(
                                                    test_output,
                                                    val_global_inputs,
                                                    val_global_masks,
                                                    metadata,
                                                )
                                            )

                                            val_in, loss1, _ = model_bqd(
                                                val_local_inputs, val_local_masks
                                            )
                                            val_out, loss2, loss2_mask = model_bqd(
                                                test_output, val_local_masks
                                            )"""
                                    with torch.no_grad():
                                        try:
                                            # Visualize results in Visdom
                                            visualize_results(
                                                val_local_targets,
                                                val_local_inputs,
                                                test_output,
                                                val_in,
                                                val_out,
                                                val_target,
                                                completed,
                                                step,
                                                "phase_1",
                                            )

                                            # ä¿å­˜æ¨¡å‹
                                            folder = os.path.join(result_dir, "phase_1")

                                            # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆä¸è¦æ±‚.pthæ‰©å±•åï¼‰
                                            pattern = os.path.join(
                                                folder, "model_cn_step*"
                                            )
                                            old_files = glob.glob(pattern)

                                            # åˆ é™¤stepå°äºå½“å‰step-1000çš„æ–‡ä»¶
                                            for file in old_files:
                                                try:
                                                    # ä»æ–‡ä»¶åä¸­æå–stepæ•°å­—
                                                    basename = os.path.basename(file)
                                                    # ä¿®æ”¹æ­£åˆ™è¡¨è¾¾å¼ï¼Œä½¿.pthæ‰©å±•åå¯é€‰
                                                    match = re.search(
                                                        r"model_cn_step(\d+)(?:\.pth)?$",
                                                        basename,
                                                    )
                                                    if match:
                                                        file_step = int(match.group(1))
                                                        # åªåˆ é™¤stepå°äºå½“å‰step-1000çš„æ–‡ä»¶
                                                        if file_step < step - 1000:
                                                            os.remove(file)
                                                            print(
                                                                f"åˆ é™¤æ—§æ–‡ä»¶: {basename} (step={file_step})"
                                                            )
                                                    else:
                                                        print(
                                                            f"æ— æ³•è§£ææ–‡ä»¶å: {basename}"
                                                        )
                                                except Exception as e:
                                                    print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")

                                            bqd_folder = os.path.join(result_dir, "bqd")
                                            bqd_pattern = os.path.join(
                                                bqd_folder, "model_bqd_step*"
                                            )
                                            old_bqd_files = glob.glob(bqd_pattern)
                                            # åˆ é™¤stepå°äºå½“å‰step-1000çš„æ–‡ä»¶
                                            for file in old_bqd_files:
                                                try:
                                                    # ä»æ–‡ä»¶åä¸­æå–stepæ•°å­—
                                                    basename = os.path.basename(file)
                                                    # ä¿®æ”¹æ­£åˆ™è¡¨è¾¾å¼ï¼Œä½¿.pthæ‰©å±•åå¯é€‰
                                                    match = re.search(
                                                        r"model_bqd_step(\d+)(?:\.pth)?$",
                                                        basename,
                                                    )
                                                    if match:
                                                        file_step = int(match.group(1))
                                                        # åªåˆ é™¤stepå°äºå½“å‰step-1000çš„æ–‡ä»¶
                                                        if file_step < step - 1000:
                                                            os.remove(file)
                                                            print(
                                                                f"åˆ é™¤æ—§æ–‡ä»¶: {basename} (step={file_step})"
                                                            )
                                                    else:
                                                        print(
                                                            f"æ— æ³•è§£ææ–‡ä»¶å: {basename}"
                                                        )
                                                except Exception as e:
                                                    print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")

                                            # ä¿å­˜å½“å‰æ¨¡å‹
                                            model_path = os.path.join(
                                                result_dir,
                                                "phase_1",
                                                f"model_cn_step{step}",
                                            )
                                            torch.save(
                                                model_cn.state_dict(), model_path
                                            )
                                            print(f"ä¿å­˜æ¨¡å‹: model_cn_step{step}")

                                            # ä¿å­˜å½“å‰BQDæ¨¡å‹
                                            bqd_model_path = os.path.join(
                                                result_dir,
                                                "bqd",
                                                f"model_bqd_step{step}",
                                            )
                                            torch.save(
                                                model_bqd.state_dict(), bqd_model_path
                                            )
                                            print(f"ä¿å­˜æ¨¡å‹: model_bqd_step{step}")

                                            # æ¸…ç†å˜é‡
                                            del test_output, completed

                                            del (
                                                # val_batch,
                                                val_local_inputs,
                                                # val_local_masks,
                                                val_local_targets,
                                            )
                                            """del (
                                                val_global_inputs,
                                                val_global_masks,
                                                val_global_targets,
                                            )
"""
                                        except Exception as e:
                                            print(f"ç”Ÿæˆæµ‹è¯•å®Œæˆå›¾åƒæ—¶å¤±è´¥: {e}")
                                            cleanup_memory()

                                    model_cn.train()
                                    model_bqd.train()

                                except Exception as e:
                                    print(f"éªŒè¯æ­¥éª¤å¤±è´¥: {e}")
                                    cleanup_memory()

                            if step >= steps_1:
                                break

                        except Exception as e:
                            print(f"è®­ç»ƒæ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                            traceback.print_exc()  # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
                            cleanup_memory()
                            continue

                pbar.close()
                phase = 2  # åˆ‡æ¢åˆ°ç¬¬äºŒé˜¶æ®µ

            except Exception as e:
                print(f"Phase 1è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                cleanup_memory()
                raise

        # =================================================
        # Training Phase 2: Train Context Discriminator only
        # =================================================

        def balanced_discriminator_loss(
            real_preds, fake_preds, real_labels, fake_labels
        ):
            """
            å¹³è¡¡çš„åˆ¤åˆ«å™¨æŸå¤±ï¼Œç‰¹åˆ«å¤„ç†ä¸å¹³è¡¡æƒ…å†µ
            """
            # åŸºç¡€MSEæŸå¤±
            real_loss = F.mse_loss(real_preds, real_labels)
            fake_loss = F.mse_loss(fake_preds, fake_labels)

            # è®¡ç®—çœŸå‡åˆ†ç±»æƒ…å†µ
            real_accuracy = (real_preds > 0.5).float().mean()
            fake_accuracy = (fake_preds < 0.5).float().mean()

            # å¦‚æœåˆ†ç±»ä¸¥é‡ä¸å¹³è¡¡
            if fake_accuracy > 0.8 and real_accuracy < 0.2:
                # å¤§å¹…å¢åŠ çœŸå®æ ·æœ¬æƒé‡ï¼Œå‡å°‘å‡æ ·æœ¬æƒé‡
                return real_loss * 5.0 + fake_loss * 0.1
            elif real_accuracy > 0.8 and fake_accuracy < 0.2:
                # åä¹‹äº¦ç„¶
                return real_loss * 0.1 + fake_loss * 5.0
            else:
                # ç›¸å¯¹å¹³è¡¡æƒ…å†µ
                return (real_loss + fake_loss) / 2.0

        def weighted_bce_loss(predictions, targets, pos_weight=2.0, neg_weight=2.0):
            """æ”¹è¿›çš„åŠ æƒäºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼Œé˜²æ­¢æ•°å€¼é—®é¢˜"""
            # ä½¿ç”¨æ›´å®‰å…¨çš„epsilonå€¼
            eps = 1e-6

            # ä½¿ç”¨æ›´ä¸¥æ ¼çš„clampç¡®ä¿é¢„æµ‹å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
            predictions = torch.clamp(predictions, eps, 1.0 - eps)

            # ä½¿ç”¨æ›´ç¨³å®šçš„å½¢å¼è®¡ç®—æŸå¤±
            pos_part = -targets * torch.log(predictions) * pos_weight
            neg_part = -(1.0 - targets) * torch.log(1.0 - predictions) * neg_weight

            # æ£€æŸ¥æ— æ•ˆå€¼å¹¶æ›¿æ¢
            pos_part = torch.where(
                torch.isnan(pos_part), torch.zeros_like(pos_part), pos_part
            )
            neg_part = torch.where(
                torch.isnan(neg_part), torch.zeros_like(neg_part), neg_part
            )

            # è®¡ç®—å‡å€¼å‰æ£€æŸ¥æ€»æŸå¤±
            loss = pos_part + neg_part
            loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)

            return loss.mean()

        def feature_contrastive_loss(
            real_local_feat,
            real_global_feat,
            fake_local_feat,
            fake_global_feat,
        ):
            """ç‰¹å¾å¯¹æ¯”æŸå¤± - é¼“åŠ±çœŸå‡æ ·æœ¬ç‰¹å¾åˆ†ç¦»"""

            # è®¡ç®—çœŸå‡ç‰¹å¾å‡å€¼
            real_local_mean = real_local_feat.mean(0)
            fake_local_mean = fake_local_feat.mean(0)
            real_global_mean = real_global_feat.mean(0)
            fake_global_mean = fake_global_feat.mean(0)

            # è®¡ç®—æ¬§æ°è·ç¦»
            local_distance = torch.sqrt(
                torch.sum((real_local_mean - fake_local_mean) ** 2) + 1e-6
            )
            global_distance = torch.sqrt(
                torch.sum((real_global_mean - fake_global_mean) ** 2) + 1e-6
            )
            # boundary_distance = torch.sqrt(torch.sum((real_boudnary_mean - fake_boundary_mean) ** 2) + 1e-6)

            # å¯¹æ¯”æŸå¤± - æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è·ç¦»ï¼ˆæ‰€ä»¥ä½¿ç”¨è´Ÿå·ï¼‰
            contrastive_loss = (
                1.0 / local_distance + 1.0 / global_distance
            )  #  + 1.0 / boundary_distance

            return contrastive_loss

        def enhanced_feature_contrastive_loss(
            real_local_feat,
            real_global_feat,
            fake_local_feat,
            fake_global_feat,
            alpha=1.0,
            beta=0.6,
            gamma=0.6,
        ):
            """
            å¢å¼ºç‰ˆç‰¹å¾å¯¹æ¯”æŸå¤± - åŸºäºä½ çš„åŸå§‹è®¾è®¡ï¼ŒåŠ å…¥ä½™å¼¦åˆ†ç¦»å’Œä¸­å¿ƒåˆ†ç¦»

            Args:
                real_local_feat: [B, D] çœŸå®æ ·æœ¬å±€éƒ¨ç‰¹å¾
                real_global_feat: [B, D] çœŸå®æ ·æœ¬å…¨å±€ç‰¹å¾
                fake_local_feat: [B, D] ç”Ÿæˆæ ·æœ¬å±€éƒ¨ç‰¹å¾
                fake_global_feat: [B, D] ç”Ÿæˆæ ·æœ¬å…¨å±€ç‰¹å¾
                alpha: åŸå§‹æ¬§å‡ é‡Œå¾—è·ç¦»æƒé‡
                beta: ä½™å¼¦åˆ†ç¦»æƒé‡
                gamma: ä¸­å¿ƒåˆ†ç¦»æƒé‡

            Returns:
                total_loss: ç»„åˆæŸå¤±
                loss_info: å„é¡¹æŸå¤±çš„è¯¦ç»†ä¿¡æ¯
            """

            # ========== 1. åŸå§‹çš„æ¬§å‡ é‡Œå¾—è·ç¦»åˆ†ç¦» (ä½ çš„åŸå§‹è®¾è®¡) ==========
            # è®¡ç®—çœŸå‡ç‰¹å¾å‡å€¼
            real_local_mean = real_local_feat.mean(0)
            fake_local_mean = fake_local_feat.mean(0)
            real_global_mean = real_global_feat.mean(0)
            fake_global_mean = fake_global_feat.mean(0)

            # è®¡ç®—æ¬§æ°è·ç¦»
            local_distance = torch.sqrt(
                torch.sum((real_local_mean - fake_local_mean) ** 2) + 1e-6
            )
            global_distance = torch.sqrt(
                torch.sum((real_global_mean - fake_global_mean) ** 2) + 1e-6
            )

            # åŸå§‹å¯¹æ¯”æŸå¤± - æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è·ç¦»ï¼ˆæ‰€ä»¥ä½¿ç”¨è´Ÿå·ï¼‰
            euclidean_loss = 1.0 / (local_distance + 1e-6) + 1.0 / (
                global_distance + 1e-6
            )

            # ========== 2. ä½™å¼¦åˆ†ç¦»æŸå¤± ==========
            # å½’ä¸€åŒ–ç‰¹å¾åˆ°å•ä½çƒé¢
            real_local_norm = F.normalize(real_local_feat, p=2, dim=1)
            real_global_norm = F.normalize(real_global_feat, p=2, dim=1)
            fake_local_norm = F.normalize(fake_local_feat, p=2, dim=1)
            fake_global_norm = F.normalize(fake_global_feat, p=2, dim=1)

            # è®¡ç®—çœŸå‡æ ·æœ¬ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦
            local_cosine_sim = F.cosine_similarity(
                real_local_norm, fake_local_norm, dim=1
            )
            global_cosine_sim = F.cosine_similarity(
                real_global_norm, fake_global_norm, dim=1
            )

            # ä½™å¼¦åˆ†ç¦»æŸå¤± - å¸Œæœ›ç›¸ä¼¼åº¦æ¥è¿‘0ï¼ˆæ­£äº¤ï¼‰
            cosine_separation_loss = (
                local_cosine_sim.abs().mean() + global_cosine_sim.abs().mean()
            ) / 2.0

            # ========== 3. ä¸­å¿ƒåˆ†ç¦»æŸå¤± ==========
            # è®¡ç®—å½’ä¸€åŒ–åçš„ä¸­å¿ƒ
            real_local_center = real_local_norm.mean(0, keepdim=True)  # [1, D]
            real_global_center = real_global_norm.mean(0, keepdim=True)  # [1, D]
            fake_local_center = fake_local_norm.mean(0, keepdim=True)  # [1, D]
            fake_global_center = fake_global_norm.mean(0, keepdim=True)  # [1, D]

            # å½’ä¸€åŒ–ä¸­å¿ƒå‘é‡
            real_local_center_norm = F.normalize(real_local_center, p=2, dim=1)
            real_global_center_norm = F.normalize(real_global_center, p=2, dim=1)
            fake_local_center_norm = F.normalize(fake_local_center, p=2, dim=1)
            fake_global_center_norm = F.normalize(fake_global_center, p=2, dim=1)

            # è®¡ç®—ä¸­å¿ƒé—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
            local_center_sim = F.cosine_similarity(
                real_local_center_norm, fake_local_center_norm, dim=1
            )
            global_center_sim = F.cosine_similarity(
                real_global_center_norm, fake_global_center_norm, dim=1
            )

            # ä¸­å¿ƒåˆ†ç¦»æŸå¤± - å¸Œæœ›ä¸­å¿ƒç›¸ä¼¼åº¦å°½å¯èƒ½å°
            center_separation_loss = (
                local_center_sim.abs() + global_center_sim.abs()
            ) / 2.0

            # ========== 4. ç»„åˆæŸå¤± ==========
            total_loss = (
                alpha * euclidean_loss  # åŸå§‹æ¬§å‡ é‡Œå¾—è·ç¦»
                + beta * cosine_separation_loss  # ä½™å¼¦åˆ†ç¦»
                + gamma * center_separation_loss  # ä¸­å¿ƒåˆ†ç¦»
            )
            # print(f"euclidean_loss: {euclidean_loss.item()}, cosine_separation: {cosine_separation_loss.item()}, center_separation: {center_separation_loss.item()}")

            # è¿”å›è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
            loss_info = {
                "total_loss": total_loss.item(),
                "euclidean_loss": euclidean_loss.item(),
                "cosine_separation": cosine_separation_loss.item(),
                "center_separation": center_separation_loss.item(),
                "local_euclidean_distance": local_distance.item(),
                "global_euclidean_distance": global_distance.item(),
                "local_cosine_similarity": local_cosine_sim.mean().item(),
                "global_cosine_similarity": global_cosine_sim.mean().item(),
                "local_center_similarity": local_center_sim.item(),
                "global_center_similarity": global_center_sim.item(),
            }

            return total_loss  # , loss_info

        def gradient_penalty(
            discriminator,
            real_local_data,
            real_local_mask,
            real_global_data,
            real_global_mask,
            fake_local_data,
            fake_local_mask,
            fake_global_data,
            fake_global_mask,
        ):
            """å†…å­˜æ•ˆç‡æ›´é«˜çš„æ¢¯åº¦æƒ©ç½šè®¡ç®—"""
            try:
                batch_size = real_local_data.size(0)

                # åªä½¿ç”¨è¾ƒå°çš„æ‰¹é‡è®¡ç®—æ¢¯åº¦æƒ©ç½š
                max_samples = min(batch_size, 4)  # ä¸€æ¬¡æœ€å¤šå¤„ç†4ä¸ªæ ·æœ¬

                # éšæœºé€‰æ‹©æ ·æœ¬
                indices = torch.randperm(batch_size)[:max_samples]

                real_local_sample = real_local_data[indices]
                real_local_mask_sample = real_local_mask[indices]
                real_global_sample = real_global_data[indices]
                real_global_mask_sample = real_global_mask[indices]

                fake_local_sample = fake_local_data[indices]
                fake_local_mask_sample = fake_local_mask[indices]
                fake_global_sample = fake_global_data[indices]
                fake_global_mask_sample = fake_global_mask[indices]

                # ä»¥ä¸‹è®¡ç®—ä¸åŸå§‹å‡½æ•°ç›¸åŒï¼Œä½†ä½¿ç”¨è¾ƒå°çš„æ ·æœ¬
                alpha = torch.rand(max_samples, 1, 1, 1, device=real_local_data.device)

                interpolates_local = (
                    alpha * real_local_sample + (1 - alpha) * fake_local_sample
                )
                interpolates_global = (
                    alpha * real_global_sample + (1 - alpha) * fake_global_sample
                )

                interpolates_local_mask = (
                    alpha * real_local_mask_sample
                    + (1 - alpha) * fake_local_mask_sample
                ).round()
                interpolates_global_mask = (
                    alpha * real_global_mask_sample
                    + (1 - alpha) * fake_global_mask_sample
                ).round()

                interpolates_local.requires_grad_(True)
                interpolates_global.requires_grad_(True)

                d_interpolates, _, _ = discriminator(
                    interpolates_local,
                    interpolates_local_mask,
                    interpolates_global,
                    interpolates_global_mask,
                )

                fake = torch.ones(d_interpolates.size(), device=real_local_data.device)

                gradients = torch.autograd.grad(
                    outputs=d_interpolates,
                    inputs=[interpolates_local, interpolates_global],
                    grad_outputs=fake,
                    create_graph=True,
                    retain_graph=True,
                    only_inputs=True,
                )

                gradients_local = gradients[0].view(max_samples, -1)
                gradients_global = gradients[1].view(max_samples, -1)

                gradients_all = torch.cat([gradients_local, gradients_global], dim=1)
                gradient_norm = gradients_all.norm(2, dim=1)

                gradient_penalty = ((gradient_norm - 1) ** 2).mean()

                # æ¸…ç†ä¸´æ—¶å˜é‡
                del interpolates_local, interpolates_global, d_interpolates
                del gradients, gradients_local, gradients_global, gradients_all

                return gradient_penalty

            except Exception as e:
                print(f"è®¡ç®—æ¢¯åº¦æƒ©ç½šæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return torch.tensor(0.0, device=real_local_data.device)

        def log_scale_balance_loss(loss_real, loss_fake, lambda_balance=1.0):
            """
            å¯¹æ•°æ¯”ä¾‹å¹³è¡¡æŸå¤±ï¼Œä¸“é—¨å¤„ç†æ•°é‡çº§å·®å¼‚
            """
            # æ·»åŠ å°å¸¸æ•°é˜²æ­¢log(0)
            epsilon = 1e-8

            # è®¡ç®—å¯¹æ•°æ¯”ä¾‹
            log_ratio = torch.log(loss_real + epsilon) - torch.log(loss_fake + epsilon)

            # å¹³è¡¡æŸå¤±æ˜¯å¯¹æ•°æ¯”ä¾‹çš„å¹³æ–¹ï¼Œå½“æ¯”ä¾‹ä¸º1(å¯¹æ•°ä¸º0)æ—¶æœ€å°
            balance_loss = lambda_balance * log_ratio**2

            return balance_loss

        def r1_regularization(
            discriminator,
            real_data,
            real_mask,
            real_global,
            real_global_mask,
            weight=1e3,
        ):
            """R1æ­£åˆ™åŒ– - åœ¨çœŸå®æ•°æ®ä¸Šçš„æ¢¯åº¦æƒ©ç½š"""
            try:
                # éœ€è¦æ¢¯åº¦
                real_data.requires_grad_(True)

                # å‰å‘ä¼ æ’­
                real_pred, _, _ = discriminator(
                    real_data, real_mask, real_global, real_global_mask
                )

                # å¯¹æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹æ±‚å’Œ
                pred_sum = real_pred.sum()

                # è®¡ç®—æ¢¯åº¦
                gradients = torch.autograd.grad(
                    outputs=pred_sum,
                    inputs=real_data,
                    create_graph=True,
                    retain_graph=True,
                )[0]

                # æ¢¯åº¦å¹³æ–¹èŒƒæ•°
                grad_norm_squared = (
                    gradients.pow(2).reshape(gradients.shape[0], -1).sum(1)
                )

                # R1æ­£åˆ™åŒ–
                r1_penalty = weight * grad_norm_squared.mean() / 2

                # æ¸…ç†æ¢¯åº¦å˜é‡
                del gradients, grad_norm_squared

                return r1_penalty

            except Exception as e:
                print(f"è®¡ç®—R1æ­£åˆ™åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return torch.tensor(0.0, device=real_data.device)

        best_acc = float("-inf")

        if step_phase2 < steps_2 and phase == 2:
            print("å¼€å§‹/ç»§ç»­ç¬¬2é˜¶æ®µè®­ç»ƒ...")
            try:
                model_cd.load_state_dict(
                    torch.load(
                        r"e:\KingCrimson Dataset\Simulate\data0\results46\phase_2\model_cd_step42500",
                        map_location=device,
                        weights_only=True,
                    )
                )
                step_phase2 = 42500
                # ä½¿ç”¨Adamä¼˜åŒ–å™¨
                opt_cd = torch.optim.Adam(
                    model_cd.parameters(), lr=1e-5, betas=(0.5, 0.999), eps=1e-8
                )

                # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨ï¼Œç”¨äºè‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
                scaler2 = GradScaler(enabled=True)

                # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ - åªä½¿ç”¨ä¸€ç§è°ƒåº¦å™¨
                scheduler = CosineAnnealingLR(opt_cd_p2, T_max=steps_2, eta_min=1e-6)

                # å®ä¾‹å™ªå£°åˆå§‹å€¼å’Œæœ€å°å€¼
                start_noise = 0.1
                min_noise = 0.001

                # è·Ÿè¸ªæ€§èƒ½æŒ‡æ ‡
                running_loss = 0.0
                running_real_acc = 0.0
                running_fake_acc = 0.0
                patience_counter = 0
                max_patience = 10  # å¢åŠ è€å¿ƒå€¼ï¼Œé¿å…è¿‡æ—©é™ä½å­¦ä¹ ç‡

                pbar = tqdm(total=steps_2, initial=step_phase2)
                step = step_phase2
                while step < steps_2:
                    for batch in train_loader:
                        try:
                            # å®šæœŸæ£€æŸ¥å†…å­˜
                            if step % 50 == 0:
                                check_memory_and_cleanup(threshold_gb=8.0)

                            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                            batch_data = prepare_batch_data(batch, device)
                            (
                                batch_local_inputs,
                                batch_local_masks,
                                batch_local_targets,
                                batch_global_inputs,
                                batch_global_masks,
                                batch_global_targets,
                                metadata,
                            ) = batch_data

                            # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
                            with autocast(
                                device_type=device.type,
                                dtype=torch.float32,
                                enabled=True,
                            ):
                                # nld_li, nldlt, nld_gi, nld_gt = nomarlize(batch_local_inputs, batch_local_targets, batch_global_inputs, batch_global_targets)
                                # ç”Ÿæˆå‡æ ·æœ¬
                                with torch.no_grad():
                                    fake_outputs = safe_tensor_operation(
                                        model_cn,
                                        batch_local_inputs,
                                        batch_local_masks,
                                        batch_global_inputs,
                                        batch_global_masks,
                                    )

                                # è®¡ç®—å½“å‰å™ªå£°æ°´å¹³ - éšç€è®­ç»ƒè¿›å±•å‡å°‘
                                noise_level = max(
                                    min_noise, start_noise * (1.0 - step / steps_2)
                                )

                                # åº”ç”¨å®ä¾‹å™ªå£°
                                # é™åˆ¶å™ªå£°æ°´å¹³åœ¨0-1ä¹‹é—´
                                noise_level = max(0.0, min(noise_level, 1.0))
                                batch_local_targets_noisy = (
                                    batch_local_targets
                                    + torch.randn_like(batch_local_targets)
                                    * noise_level
                                )
                                fake_outputs_noisy = (
                                    fake_outputs
                                    + torch.randn_like(fake_outputs) * noise_level
                                )

                                # åµŒå…¥å‡è¾“å‡ºåˆ°å…¨å±€
                                fake_global_embedded, fake_global_mask_embedded, _ = (
                                    merge_local_to_global(
                                        fake_outputs_noisy,
                                        batch_global_inputs,
                                        batch_global_masks,
                                        metadata,
                                    )
                                )

                                # åµŒå…¥çœŸå®è¾“å…¥åˆ°å…¨å±€
                                real_global_embedded, real_global_mask_embedded, _ = (
                                    merge_local_to_global(
                                        batch_local_targets_noisy,
                                        batch_global_inputs,
                                        batch_global_masks,
                                        metadata,
                                    )
                                )

                                # åˆ›å»ºå¹³æ»‘æ ‡ç­¾
                                batch_size = batch_local_targets.size(0)
                                real_labels = (
                                    torch.ones(batch_size, 1).to(device) * 0.95
                                )  # æ ‡ç­¾å¹³æ»‘
                                fake_labels = (
                                    torch.zeros(batch_size, 1).to(device) + 0.05
                                )  # æ ‡ç­¾å¹³æ»‘

                                # è®­ç»ƒåˆ¤åˆ«å™¨å¤„ç†å‡æ ·æœ¬
                                fake_predictions, fake_lf, fake_gf = (
                                    safe_tensor_operation(
                                        model_cd,
                                        fake_outputs_noisy,
                                        batch_local_masks,
                                        fake_global_embedded,
                                        fake_global_mask_embedded,
                                    )
                                )

                                # è®­ç»ƒåˆ¤åˆ«å™¨å¤„ç†çœŸå®æ ·æœ¬
                                real_predictions, real_lf, real_gf = (
                                    safe_tensor_operation(
                                        model_cd,
                                        batch_local_targets_noisy,
                                        batch_local_masks,
                                        real_global_embedded,
                                        real_global_mask_embedded,
                                    )
                                )

                                # ä½¿ç”¨BCEWithLogitsLossä»£æ›¿BCEæˆ–è‡ªå®šä¹‰æŸå¤±
                                loss_real = F.binary_cross_entropy_with_logits(
                                    real_predictions, real_labels
                                )
                                loss_fake = F.binary_cross_entropy_with_logits(
                                    fake_predictions, fake_labels
                                )

                                # è®¡ç®—ç‰¹å¾åŒ¹é…æŸå¤±
                                fm_weight = 4
                                fm_loss = (
                                    feature_contrastive_loss(
                                        real_lf,
                                        real_gf,
                                        fake_lf,
                                        fake_gf,
                                    )
                                    * fm_weight
                                )
                                if (
                                    fm_loss.mean() > loss_real.mean()
                                    and fm_loss.mean() > loss_fake.mean()
                                ):
                                    fm_loss *= 1
                                    # print("fm_loss weight reduced")
                                lsb_loss = log_scale_balance_loss(loss_real, loss_fake)
                                r1_loss = r1_regularization(
                                    model_cd,
                                    batch_local_targets_noisy,
                                    batch_local_masks,
                                    real_global_embedded,
                                    real_global_mask_embedded,
                                )

                                # ä½¿ç”¨sigmoidè®¡ç®—é¢„æµ‹æ¦‚ç‡ï¼Œç”¨äºå‡†ç¡®ç‡è®¡ç®—
                                with torch.no_grad():
                                    real_probs = torch.sigmoid(real_predictions)
                                    fake_probs = torch.sigmoid(fake_predictions)

                                    real_acc = (real_probs >= 0.5).float().mean().item()
                                    fake_acc = (fake_probs < 0.5).float().mean().item()
                                    avg_acc = (real_acc + fake_acc) / 2

                                # åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡ï¼Œå¹³è¡¡è®­ç»ƒ
                                if real_acc < 0.1 and fake_acc > 0.9:
                                    loss_real *= 5.0
                                    loss_fake *= 0.5
                                    print("loss real weight strengthened")
                                elif fake_acc < 0.1 and real_acc > 0.9:
                                    loss_real *= 0.5
                                    loss_fake *= 5.0
                                    print("loss fake weight strengthened")
                                loss = (
                                    loss_real + loss_fake + fm_loss + lsb_loss + r1_loss
                                )

                            # ä½¿ç”¨æ¢¯åº¦ç¼©æ”¾å™¨è¿›è¡Œåå‘ä¼ æ’­
                            scaler2.scale(loss).backward()
                            # loss.backward()

                            # æ¢¯åº¦è£å‰ª
                            scaler2.unscale_(opt_cd_p2)
                            torch.nn.utils.clip_grad_norm_(
                                model_cd.parameters(), max_norm=1.0
                            )

                            # æ›´æ–°å‚æ•°
                            scaler2.step(opt_cd_p2)
                            # opt_cd_p2.step()
                            scaler2.update()
                            opt_cd_p2.zero_grad(set_to_none=True)

                            # æ›´æ–°å­¦ä¹ ç‡ - åªä½¿ç”¨è°ƒåº¦å™¨
                            scheduler.step()

                            # æ›´æ–°è¿è¡ŒæŒ‡æ ‡
                            running_loss += loss.item()
                            running_real_acc += real_acc
                            running_fake_acc += fake_acc

                            # å®šæœŸæ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
                            if step % 100 == 0:
                                avg_loss = running_loss / min(
                                    100, step - step_phase2 + 1
                                )
                                avg_real_acc = running_real_acc / min(
                                    100, step - step_phase2 + 1
                                )
                                avg_fake_acc = running_fake_acc / min(
                                    100, step - step_phase2 + 1
                                )

                                print(f"\nç»Ÿè®¡ä¿¡æ¯(æœ€è¿‘100æ­¥):")
                                print(f"å¹³å‡æŸå¤±: {avg_loss:.4f}")
                                print(f"çœŸå®æ ·æœ¬å¹³å‡å‡†ç¡®ç‡: {avg_real_acc:.4f}")
                                print(f"è™šå‡æ ·æœ¬å¹³å‡å‡†ç¡®ç‡: {avg_fake_acc:.4f}")
                                print(f"å½“å‰å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f}")

                                # é‡ç½®è¿è¡ŒæŒ‡æ ‡
                                running_loss = 0.0
                                running_real_acc = 0.0
                                running_fake_acc = 0.0

                            # æ›´æ–°è¿›åº¦æ¡
                            current_lr = scheduler.get_last_lr()[0]
                            pbar.set_description(
                                f"Phase 2 | loss: {loss.item():.4f}, acc: {(real_acc+fake_acc)/2:.3f}, loss_rf:{(loss_real+loss_fake)/2:.3f}, lr: {current_lr:.1e}"
                            )
                            pbar.update(1)

                            # Visdomå¯è§†åŒ–
                            if viz is not None and step % 100 == 0:
                                try:
                                    if "phase2_disc" in loss_windows:
                                        viz.line(
                                            Y=torch.tensor([loss.item()]),
                                            X=torch.tensor([step]),
                                            win=loss_windows["phase2_disc"],
                                            update="append",
                                        )

                                    if "phase2_acc" in loss_windows:
                                        viz.line(
                                            Y=torch.tensor(
                                                [[avg_acc, real_acc, fake_acc]]
                                            ),
                                            X=torch.tensor([step]),
                                            win=loss_windows["phase2_acc"],
                                            update="append",
                                        )
                                except:
                                    pass

                            step += 1

                            # æ¸…ç†ä¸´æ—¶å˜é‡
                            del (
                                fake_outputs,
                                fake_outputs_noisy,
                                batch_local_targets_noisy,
                            )
                            del fake_global_embedded, fake_global_mask_embedded
                            del real_global_embedded, real_global_mask_embedded
                            del (
                                fake_predictions,
                                real_predictions,
                                fake_lf,
                                fake_gf,
                                real_lf,
                                real_gf,
                            )
                            del loss_real, loss_fake, fm_loss, lsb_loss, r1_loss, loss
                            del batch_data

                            # åœ¨æ¯ä¸ªsnaperiod_2æ­¥ä¿å­˜æ£€æŸ¥ç‚¹å’Œè¿›è¡ŒéªŒè¯
                            if step % snaperiod_2 == 0:
                                try:
                                    # è¯„ä¼°åˆ¤åˆ«å™¨
                                    model_cd.eval()
                                    val_correct = 0
                                    val_total = 0
                                    with torch.no_grad():
                                        for val_batch in val_subset_loader:
                                            try:
                                                # å‡†å¤‡éªŒè¯æ‰¹æ¬¡
                                                val_data = prepare_batch_data(
                                                    val_batch, device
                                                )
                                                (
                                                    val_local_inputs,
                                                    val_local_masks,
                                                    val_local_targets,
                                                    val_global_inputs,
                                                    val_global_masks,
                                                    val_global_targets,
                                                    val_metadata,
                                                ) = val_data
                                                # ç”Ÿæˆå‡æ ·æœ¬
                                                val_fake_outputs = (
                                                    safe_tensor_operation(
                                                        model_cn,
                                                        val_local_inputs,
                                                        val_local_masks,
                                                        val_global_inputs,
                                                        val_global_masks,
                                                    )
                                                )

                                                # åµŒå…¥
                                                (
                                                    val_fake_global_embedded,
                                                    val_fake_global_mask_embedded,
                                                    _,
                                                ) = merge_local_to_global(
                                                    val_fake_outputs,
                                                    val_global_inputs,
                                                    val_global_masks,
                                                    val_metadata,
                                                )

                                                (
                                                    val_real_global_embedded,
                                                    val_real_global_mask_embedded,
                                                    _,
                                                ) = merge_local_to_global(
                                                    val_local_targets,
                                                    val_global_inputs,
                                                    val_global_masks,
                                                    val_metadata,
                                                )
                                                # è·å–åˆ¤åˆ«å™¨é¢„æµ‹
                                                val_fake_preds, _, _ = (
                                                    safe_tensor_operation(
                                                        model_cd,
                                                        val_fake_outputs,
                                                        val_local_masks,
                                                        val_fake_global_embedded,
                                                        val_fake_global_mask_embedded,
                                                    )
                                                )

                                                val_real_preds, _, _ = (
                                                    safe_tensor_operation(
                                                        model_cd,
                                                        val_local_targets,
                                                        val_local_masks,
                                                        val_real_global_embedded,
                                                        val_real_global_mask_embedded,
                                                    )
                                                )
                                                val_fake_preds = torch.sigmoid(
                                                    val_fake_preds
                                                )
                                                val_real_preds = torch.sigmoid(
                                                    val_real_preds
                                                )
                                                # è®¡ç®—å‡†ç¡®ç‡
                                                val_total += val_fake_preds.size(0) * 2
                                                val_correct += (
                                                    (val_fake_preds < 0.5).sum()
                                                    + (val_real_preds >= 0.5).sum()
                                                ).item()
                                                print(
                                                    f"validation: fake_preds: {(val_fake_preds<0.5).sum()}/{val_fake_preds.size(0)}, real_preds: {(val_real_preds>0.5).sum()}/{val_real_preds.size(0)}"
                                                )

                                                # æ¸…ç†éªŒè¯å˜é‡
                                                del (
                                                    val_fake_outputs,
                                                    val_fake_global_embedded,
                                                    val_fake_global_mask_embedded,
                                                )
                                                del (
                                                    val_real_global_embedded,
                                                    val_real_global_mask_embedded,
                                                )
                                                del val_fake_preds, val_real_preds
                                                del val_data

                                            except Exception as e:
                                                print(f"éªŒè¯æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                                                cleanup_memory()
                                                continue

                                    val_accuracy = val_correct / max(val_total, 1)
                                    print(f"\néªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")

                                    # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿› - ç§»é™¤æ‰‹åŠ¨å­¦ä¹ ç‡è°ƒæ•´
                                    if val_accuracy > best_acc:
                                        best_acc = val_accuracy
                                        best_model_path = os.path.join(
                                            result_dir, "phase_2", "model_cd_best"
                                        )
                                        torch.save(
                                            model_cd.state_dict(), best_model_path
                                        )
                                        print(
                                            f"å‘ç°æ–°çš„æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {val_accuracy:.4f}"
                                        )
                                        patience_counter = 0
                                    else:
                                        patience_counter += 1
                                        print(
                                            f"æœªè§æ”¹å–„ã€‚è€å¿ƒè®¡æ•°: {patience_counter}/{max_patience}"
                                        )
                                        """# å¦‚æœé•¿æ—¶é—´æ²¡æœ‰æ”¹å–„ï¼Œé™ä½å­¦ä¹ ç‡
                                        if patience_counter >= max_patience:
                                            for param_group in opt_cd.param_groups:
                                                param_group['lr'] *= 0.5
                                            print(f"æ€§èƒ½åœæ»ï¼Œå°†å­¦ä¹ ç‡é™ä½åˆ°: {opt_cd.param_groups[0]['lr']:.6f}")
                                            patience_counter = 0"""

                                    # ä¿å­˜æ¨¡å‹

                                    # ä¿å­˜æ¨¡å‹

                                    # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„æ£€æŸ¥ç‚¹æ–‡ä»¶

                                    # åˆ é™¤stepå°äºå½“å‰step-1000çš„æ–‡ä»¶

                                    folder = os.path.join(result_dir, "phase_2")
                                    pattern = os.path.join(folder, "model_cd_step*")
                                    old_files = glob.glob(pattern)
                                    for file in old_files:
                                        try:
                                            # ä»æ–‡ä»¶åä¸­æå–stepæ•°å­—
                                            basename = os.path.basename(file)
                                            match = re.search(
                                                r"model_cd_step(\d+)(?:\.pth)?$",
                                                basename,
                                            )
                                            if match:
                                                file_step = int(match.group(1))
                                                # åªåˆ é™¤stepå°äºå½“å‰step-1000çš„æ–‡ä»¶
                                                if file_step < step - 1000:
                                                    os.remove(file)
                                                    print(
                                                        f"åˆ é™¤æ—§æ–‡ä»¶: {basename} (step={file_step})"
                                                    )
                                            else:
                                                print(f"æ— æ³•è§£ææ–‡ä»¶å: {basename}")
                                        except Exception as e:
                                            print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
                                    model_path = os.path.join(
                                        result_dir, "phase_2", f"model_cd_step{step}"
                                    )
                                    torch.save(model_cd.state_dict(), model_path)

                                    # å›åˆ°è®­ç»ƒæ¨¡å¼
                                    model_cd.train()

                                    # å‡†å¤‡Phase 2ä¸“ç”¨çš„çŠ¶æ€
                                    phase1_scalers = {}
                                    if "scaler" in locals() and scaler is not None:
                                        phase1_scalers["cn"] = scaler
                                    if (
                                        "scaler_bqd" in locals()
                                        and scaler_bqd is not None
                                    ):
                                        phase1_scalers["bqd"] = scaler_bqd

                                    """save_checkpoint(
                                        model_cn,
                                        None,              # Phase 1ä¸éœ€è¦CD
                                        model_bqd,
                                        opt_cn_p1,
                                        None,              # Phase 1ä¸éœ€è¦CDä¼˜åŒ–å™¨  
                                        opt_bqd,
                                        step,
                                        1,
                                        result_dir,
                                        best_val_loss=best_val_loss,
                                        best_acc=None,
                                        cn_lr=opt_cn_p1.param_groups[0]["lr"],
                                        cd_lr=None,
                                        bqd_lr=opt_bqd.param_groups[0]["lr"],
                                        schedulers=None,   # Phase 1ä¸ä½¿ç”¨è°ƒåº¦å™¨
                                        scalers=phase1_scalers if phase1_scalers else None,
                                        stability_tracker=None,
                                    )"""
                                    """save_checkpoint(
                                        model_cn,
                                        model_cd,
                                        model_bqd,
                                        opt_cn_p1,
                                        opt_cd_p2,
                                        opt_bqd,
                                        step,
                                        2,
                                        result_dir,
                                        best_acc=best_acc,
                                        cd_lr=opt_cd_p2.param_groups[0]["lr"],
                                    )"""

                                except Exception as e:
                                    print(f"éªŒè¯æ­¥éª¤å¤±è´¥: {e}")
                                    cleanup_memory()

                            # åˆ¤æ–­æ˜¯å¦å®Œæˆè®­ç»ƒ
                            if step >= steps_2:
                                break

                        except Exception as e:
                            print(f"è®­ç»ƒæ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                            traceback.print_exc()  # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
                            cleanup_memory()
                            continue

                pbar.close()
                print(f"Phase 2 è®­ç»ƒå®Œæˆ! æœ€ä½³åˆ¤åˆ«å™¨å‡†ç¡®ç‡: {best_acc:.4f}")
                phase = 3

            except Exception as e:
                print(f"Phase 2è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                cleanup_memory()
                raise

        # =================================================
        # Training Phase 3: ä¼˜åŒ–åçš„è”åˆè®­ç»ƒ
        # =================================================

        if step_phase3 < steps_3 and phase == 3:
            print("å¼€å§‹/ç»§ç»­ç¬¬3é˜¶æ®µè®­ç»ƒ...")
            try:
                cleanup_memory()
                pbar = tqdm(total=steps_3, initial=step_phase3)
                step = step_phase3

                # ==================== ä¼˜åŒ–é…ç½® ====================
                # æ¢¯åº¦ç´¯ç§¯è®¾ç½®
                target_batch_size = 32
                actual_batch_size = batch_size  # å½“å‰çš„batch_size (8)
                accumulation_steps = target_batch_size // actual_batch_size  # 2
                print(
                    f"ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯: {accumulation_steps} steps, æœ‰æ•ˆbatch size: {target_batch_size}"
                )

                # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ - ä½¿ç”¨æ›´ä¿å®ˆçš„å­¦ä¹ ç‡

                if cn_lr is not None:
                    opt_cn_p3.param_groups[0]["lr"] = cn_lr  # è®¾ç½®åˆå§‹å­¦ä¹ ç‡
                if cd_lr is not None:
                    opt_cd_p3.param_groups[0]["lr"] = cd_lr  # è®¾ç½®åˆå§‹å­¦ä¹ ç‡
                if bqd_lr is not None:
                    opt_bqd.param_groups[0]["lr"] = bqd_lr

                # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨
                scaler_cn = GradScaler(
                    enabled=True, init_scale=2**10, growth_interval=2000
                )
                scaler_cd = GradScaler(
                    enabled=True, init_scale=2**10, growth_interval=2000
                )
                scaler_bqd_p3 = GradScaler()

                # å­¦ä¹ ç‡è°ƒåº¦å™¨
                scheduler_cn = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    opt_cn_p3, mode="min", factor=0.7, patience=8, min_lr=1e-6
                )
                scheduler_cd = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    opt_cd_p3, mode="max", factor=0.7, patience=5, min_lr=1e-7
                )
                stability_tracker = StabilityTracker()

                # æŸå¤±æƒé‡
                alpha_d = 1.0
                alpha_g = 0.1  # é™ä½å¯¹æŠ—æŸå¤±æƒé‡
                fm_weight = 2.0  # é™ä½ç‰¹å¾åŒ¹é…æƒé‡

                # å™ªå£°è®¾ç½®
                start_noise = 0.005  # é™ä½åˆå§‹å™ªå£°
                min_noise = 0.001

                # ç´¯ç§¯å˜é‡
                accumulated_step = 0
                running_metrics = {
                    "recon_loss": 0.0,
                    "adv_loss": 0.0,
                    "disc_loss": 0.0,
                    "real_acc": 0.0,
                    "fake_acc": 0.0,
                    "count": 0,
                }

                # ==================== ä¸»è®­ç»ƒå¾ªç¯ ====================
                while step < steps_3:
                    for batch in train_loader:
                        try:
                            # å‡å°‘å†…å­˜æ£€æŸ¥é¢‘ç‡
                            if step % 50 == 0:
                                check_memory_and_cleanup(threshold_gb=12.0)

                            # å‡†å¤‡æ•°æ®
                            batch_data = prepare_batch_data(batch, device)
                            (
                                batch_local_inputs,
                                batch_local_masks,
                                batch_local_targets,
                                batch_global_inputs,
                                batch_global_masks,
                                batch_global_targets,
                                metadata,
                            ) = batch_data

                            batch_size = batch_local_targets.size(0)
                            noise_level = max(
                                min_noise, start_noise * (1.0 - step / steps_3)
                            )

                            # ==================== è®­ç»ƒåˆ¤åˆ«å™¨ ====================
                            for param in model_cn.parameters():
                                param.requires_grad = False
                            for param in model_bqd.parameters():
                                param.requires_grad = False
                            for param in model_cd.parameters():
                                param.requires_grad = True

                            """with autocast(
                                device_type=device.type,
                                dtype=torch.float32,
                                enabled=True,
                            ):"""
                            # ç”Ÿæˆå‡æ ·æœ¬
                            with torch.no_grad():
                                fake_outputs = safe_tensor_operation(
                                    model_cn,
                                    batch_local_inputs,
                                    batch_local_masks,
                                    batch_global_inputs,
                                    batch_global_masks,
                                )

                            # åº”ç”¨é€‚åº¦å™ªå£°
                            batch_local_targets_noisy = (
                                batch_local_targets
                                + torch.randn_like(batch_local_targets) * noise_level
                            )
                            fake_outputs_noisy = (
                                fake_outputs
                                + torch.randn_like(fake_outputs) * noise_level
                            )

                            # åµŒå…¥åˆ°å…¨å±€
                            fake_global_embedded, fake_global_mask_embedded, _ = (
                                merge_local_to_global(
                                    fake_outputs_noisy,
                                    batch_global_inputs,
                                    batch_global_masks,
                                    metadata,
                                )
                            )
                            real_global_embedded, real_global_mask_embedded, _ = (
                                merge_local_to_global(
                                    batch_local_targets_noisy,
                                    batch_global_inputs,
                                    batch_global_masks,
                                    metadata,
                                )
                            )

                            # æ ‡ç­¾å¹³æ»‘
                            real_labels = torch.ones(batch_size, 1, device=device) * 0.9
                            fake_labels = (
                                torch.zeros(batch_size, 1, device=device) + 0.1
                            )

                            # åˆ¤åˆ«å™¨å‰å‘ä¼ æ’­
                            fake_predictions, fake_lf, fake_gf = safe_tensor_operation(
                                model_cd,
                                fake_outputs_noisy,
                                batch_local_masks,
                                fake_global_embedded,
                                fake_global_mask_embedded,
                            )
                            real_predictions, real_lf, real_gf = safe_tensor_operation(
                                model_cd,
                                batch_local_targets_noisy,
                                batch_local_masks,
                                real_global_embedded,
                                real_global_mask_embedded,
                            )

                            # è®¡ç®—å‡†ç¡®ç‡
                            with torch.no_grad():
                                real_probs = torch.sigmoid(real_predictions)
                                fake_probs = torch.sigmoid(fake_predictions)
                                real_acc = (real_probs >= 0.5).float().mean().item()
                                fake_acc = (fake_probs < 0.5).float().mean().item()

                            # è·å–åŠ¨æ€æƒé‡

                            real_weight, fake_weight = (
                                stability_tracker.get_loss_weights()
                            )

                            # è®¡ç®—æŸå¤±
                            loss_cd_real = (
                                F.binary_cross_entropy_with_logits(
                                    real_predictions, real_labels
                                )
                                * real_weight
                            )
                            loss_cd_fake = (
                                F.binary_cross_entropy_with_logits(
                                    fake_predictions, fake_labels
                                )
                                * fake_weight
                            )

                            # ç‰¹å¾å¯¹æ¯”æŸå¤±ï¼ˆå‡å°‘æƒé‡ï¼‰
                            fm_loss_d = (
                                feature_contrastive_loss(
                                    real_lf, real_gf, fake_lf, fake_gf
                                )
                                * fm_weight
                            )

                            # ç»„åˆåˆ¤åˆ«å™¨æŸå¤±å¹¶æ ‡å‡†åŒ–
                            loss_cd = (
                                (loss_cd_real + loss_cd_fake + fm_loss_d)
                                * alpha_d
                                / accumulation_steps
                            )

                            # åˆ¤åˆ«å™¨åå‘ä¼ æ’­
                            scaler_cd.scale(loss_cd).backward()
                            # loss_cd.backward()

                            # =================== è®­ç»ƒBQD =======================
                            for param in model_bqd.parameters():
                                param.requires_grad = True
                            for param in model_cn.parameters():
                                param.requires_grad = False
                            for param in model_cd.parameters():
                                param.requires_grad = False

                            # BQDå­¦ä¹ è¯†åˆ«ç”Ÿæˆè¾“å‡ºçš„è¾¹ç•Œ
                            with torch.no_grad():
                                fake_outputs_for_bqd = safe_tensor_operation(
                                    model_cn,
                                    batch_local_inputs,
                                    batch_local_masks,
                                    batch_global_inputs,
                                    batch_global_masks,
                                )

                            # ç¬¬ä¸€æ¬¡backward
                            nld_fo = normalization(
                                fake_outputs_for_bqd, batch_global_inputs
                            )

                            _, bqd_loss_out, _ = model_bqd(
                                nld_fo,
                                batch_local_masks,  # detaché˜»æ­¢æ¢¯åº¦ä¼ æ’­åˆ°è¡¥å…¨ç½‘ç»œ
                            )
                            # ç¡®ä¿lossæ˜¯æ ‡é‡
                            bqd_loss_out = ensure_scalar_loss(bqd_loss_out)
                            bqd_loss_out_scaled = bqd_loss_out / accumulation_steps * 2
                            scaler_bqd_p3.scale(bqd_loss_out_scaled).backward(
                                retain_graph=True
                            )

                            # ç¬¬äºŒæ¬¡backward
                            nld_li = normalization(
                                batch_local_inputs, batch_global_inputs
                            )
                            bqd_outputs_in, bqd_loss_in, _ = model_bqd(
                                nld_li, batch_local_masks
                            )
                            bqd_loss_in = ensure_scalar_loss(bqd_loss_in)
                            bqd_loss_in_scaled = bqd_loss_in / accumulation_steps * 2

                            bqd_loss_ta_scaled = (
                                bqd_loss_out_scaled + bqd_loss_in_scaled
                            )
                            scaler_bqd_p3.scale(bqd_loss_ta_scaled).backward()
                            # bqd_loss_ta_scaled.backward()

                            """# ç¬¬3æ¬¡backward
                            bqd_outputs_ta, bqd_loss_ta, _ = model_bqd(
                                batch_local_targets, torch.zeros_like(batch_local_targets)
                            )
                            bqd_loss_ta = ensure_scalar_loss(bqd_loss_ta)
                            bqd_loss_ta_scaled = (
                                bqd_loss_ta / accumulation_steps * 2
                            )
                            scaler_bqd_p3.scale(bqd_loss_ta_scaled).backward() """

                            av_bqd_loss = (
                                bqd_loss_in_scaled
                                + bqd_loss_out_scaled
                                + bqd_loss_ta_scaled
                            ) / 3

                            # - è¡¥å…¨ç½‘ç»œçš„å¯¹æŠ—æŸå¤±ï¼ˆè®©ç”Ÿæˆç»“æœæ¬ºéª—BQDï¼‰ ####

                            """ # å†»ç»“BQDå‚æ•°ï¼Œé¿å…å¯¹æŠ—æŸå¤±å½±å“BQDè®­ç»ƒ
                            for param in model_bqd.parameters():
                                param.requires_grad = False
                            for param in model_cn.parameters():
                                param.requires_grad = True

                            # é‡æ–°è®¡ç®—BQDè¾“å‡ºï¼Œä½†ä¸detachï¼Œè®©æ¢¯åº¦æµå‘è¡¥å…¨ç½‘ç»œ
                            # !!!!æ³¨æ„ï¼Œæ­¤å¤„åªè¿”å›maskéƒ¨åˆ†çš„lossï¼Œå¦åˆ™ä¼šä½¿maskå¤–çš„å¼‚å¸¸è¾¹ç¼˜æå–è¢«åˆ¤ä¸ºä¼˜
                            _, _, bqd_loss_mask_adversarial = model_bqd(
                                fake_outputs, batch_local_masks
                            )
                            # å¯¹æŠ—æŸå¤±ï¼šè®©è¡¥å…¨ç½‘ç»œç”Ÿæˆçš„ç»“æœèƒ½æ¬ºéª—BQDç½‘ç»œ
                            adversarial_loss = bqd_loss_mask_adversarial/ accumulation_steps * 2  # é™ä½æƒé‡é¿å…è¿‡å¼º
                            scaler_cn.scale(adversarial_loss).backward()  """

                            # ==================== è®­ç»ƒç”Ÿæˆå™¨ ====================
                            for param in model_cd.parameters():
                                param.requires_grad = False
                            for param in model_bqd.parameters():
                                param.requires_grad = False
                            for param in model_cn.parameters():
                                param.requires_grad = True

                            """with autocast(
                                device_type=device.type,
                                dtype=torch.float32,
                                enabled=True,
                            ):"""
                            # é‡æ–°ç”Ÿæˆï¼ˆéœ€è¦æ¢¯åº¦ï¼‰
                            fake_outputs = safe_tensor_operation(
                                model_cn,
                                batch_local_inputs,
                                batch_local_masks,
                                batch_global_inputs,
                                batch_global_masks,
                            )

                            # é‡å»ºæŸå¤±
                            fake_completed, fake_completed_mask, pos = (
                                merge_local_to_global(
                                    fake_outputs,
                                    batch_global_inputs,
                                    batch_global_masks,
                                    metadata,
                                )
                            )
                            loss_cn_recon = completion_network_loss(
                                fake_outputs,
                                batch_local_targets,
                                batch_local_masks,
                                batch_global_targets,
                                fake_completed,
                                fake_completed_mask,
                                pos,
                            )

                            # å¯¹æŠ—æŸå¤±
                            fake_global_embedded, fake_global_mask_embedded, _ = (
                                merge_local_to_global(
                                    fake_outputs,
                                    batch_global_inputs,
                                    batch_global_masks,
                                    metadata,
                                )
                            )
                            fake_predictions, fake_lf, fake_gf = safe_tensor_operation(
                                model_cd,
                                fake_outputs,
                                batch_local_masks,
                                fake_global_embedded,
                                fake_global_mask_embedded,
                            )
                            loss_cn_adv = F.binary_cross_entropy_with_logits(
                                fake_predictions, real_labels
                            )
                            # BQDæŸå¤±
                            nld_fo = normalization(fake_outputs, batch_global_inputs)
                            _, _, bloss = model_bqd(nld_fo, batch_local_masks)
                            # ç¡®ä¿lossæ˜¯æ ‡é‡ - æ–°å¢è¿™è¡Œ
                            bloss = ensure_scalar_loss(bloss)
                            """if step > steps_3 * 3 / 5:
                                bloss *= 1.5
                                loss_cn_adv *= 2"""

                            # ç»„åˆç”Ÿæˆå™¨æŸå¤±å¹¶æ ‡å‡†åŒ–
                            loss_cn = (
                                loss_cn_recon + alpha_g * loss_cn_adv + bloss * 0.1
                            ) / accumulation_steps

                            # ç”Ÿæˆå™¨åå‘ä¼ æ’­
                            # scaler_cn.scale(loss_cn).backward(retain_graph=False)
                            loss_cn.backward(retain_graph=False)

                            # ==================== ç´¯ç§¯æ¢¯åº¦æ›´æ–° ====================
                            accumulated_step += 1

                            # æ›´æ–°è¿è¡ŒæŒ‡æ ‡
                            running_metrics["recon_loss"] += loss_cn_recon.item()
                            running_metrics["adv_loss"] += loss_cn_adv.item()
                            running_metrics["disc_loss"] += (
                                loss_cd_real + loss_cd_fake
                            ).item()
                            running_metrics["real_acc"] += real_acc
                            running_metrics["fake_acc"] += fake_acc
                            running_metrics["count"] += 1

                            # æ¯accumulation_stepsæˆ–åˆ°è¾¾epochæœ«å°¾æ—¶æ›´æ–°å‚æ•°
                            if (
                                accumulated_step % accumulation_steps == 0
                                or accumulated_step == len(train_loader)
                            ):
                                # æ›´æ–°åˆ¤åˆ«å™¨
                                scaler_cd.unscale_(opt_cd_p3)
                                torch.nn.utils.clip_grad_norm_(
                                    model_cd.parameters(), max_norm=1.0
                                )
                                scaler_cd.step(opt_cd_p3)
                                # opt_cd_p3.step()
                                scaler_cd.update()
                                opt_cd_p3.zero_grad(set_to_none=True)

                                # æ›´æ–°BQD
                                scaler_bqd_p3.unscale_(opt_bqd)
                                torch.nn.utils.clip_grad_norm_(
                                    model_bqd.parameters(), max_norm=1.0
                                )
                                scaler_bqd_p3.step(opt_bqd)
                                scaler_bqd_p3.update()
                                opt_bqd.zero_grad(set_to_none=True)

                                # æ›´æ–°ç”Ÿæˆå™¨
                                # scaler_cn.unscale_(opt_cn_p3)
                                torch.nn.utils.clip_grad_norm_(
                                    model_cn.parameters(), max_norm=1.0
                                )
                                # scaler_cn.step(opt_cn_p3)
                                opt_cn_p3.step()
                                # scaler_cn.update()
                                opt_cn_p3.zero_grad(set_to_none=True)

                                # æ›´æ–°ç¨³å®šæ€§è¿½è¸ªå™¨
                                avg_recon = (
                                    running_metrics["recon_loss"]
                                    / running_metrics["count"]
                                )
                                avg_real_acc = (
                                    running_metrics["real_acc"]
                                    / running_metrics["count"]
                                )
                                avg_fake_acc = (
                                    running_metrics["fake_acc"]
                                    / running_metrics["count"]
                                )
                                stability_tracker.update(
                                    avg_real_acc, avg_fake_acc, avg_recon
                                )

                                # é‡ç½®è¿è¡ŒæŒ‡æ ‡
                                for key in running_metrics:
                                    running_metrics[key] = 0.0

                            # è§£é™¤å‚æ•°å†»ç»“
                            for param in model_cd.parameters():
                                param.requires_grad = True

                            # ==================== å¯è§†åŒ–å’Œè¿›åº¦æ›´æ–° ====================
                            if viz is not None and step % 100 == 0:
                                try:
                                    if "phase3_disc" in loss_windows:
                                        viz.line(
                                            Y=torch.tensor(
                                                [loss_cd.item() * accumulation_steps]
                                            ),
                                            X=torch.tensor([step]),
                                            win=loss_windows["phase3_disc"],
                                            update="append",
                                        )
                                    if "phase3_gen" in loss_windows:
                                        viz.line(
                                            Y=torch.tensor(
                                                [
                                                    [
                                                        loss_cn.item()
                                                        * accumulation_steps,
                                                        av_bqd_loss.item()
                                                        * accumulation_steps,
                                                        bloss.item()
                                                        * accumulation_steps,
                                                    ]
                                                ]
                                            ),
                                            X=torch.tensor([step]),
                                            win=loss_windows["phase3_gen"],
                                            update="append",
                                        )
                                    if "phase3_fake_acc" in loss_windows:
                                        viz.line(
                                            Y=torch.tensor([[fake_acc, real_acc]]),
                                            X=torch.tensor([step]),
                                            win=loss_windows["phase3_fake_acc"],
                                            update="append",
                                        )
                                except:
                                    pass

                            step += 1
                            pbar.set_description(
                                f"Phase 3 | D: {(loss_cd.item() * accumulation_steps):.4f}, "
                                f"G: {(loss_cn.item() * accumulation_steps):.4f}, "
                                f"R_acc: {real_acc:.3f}, F_acc: {fake_acc:.3f}, "
                                f"Smooth E: {bloss.item() * accumulation_steps:.4f}, "
                            )
                            pbar.update(1)

                            # æ¸…ç†ä¸´æ—¶å˜é‡
                            del fake_outputs, fake_completed, fake_completed_mask, pos
                            del (
                                loss_cn_recon,
                                loss_cn_adv,
                                loss_cn,
                                loss_cd,
                                loss_cd_real,
                                loss_cd_fake,
                            )
                            del fake_global_embedded, fake_global_mask_embedded
                            del real_global_embedded, real_global_mask_embedded
                            del (
                                fake_predictions,
                                fake_lf,
                                fake_gf,
                                real_predictions,
                                real_lf,
                                real_gf,
                            )
                            del batch_local_targets_noisy, fake_outputs_noisy
                            del batch_data

                            # ==================== éªŒè¯å’Œä¿å­˜ ====================
                            if step % snaperiod_3 == 0:
                                try:
                                    val_results = validateAll(
                                        model_cn,
                                        model_cd,
                                        model_bqd,
                                        val_subset_loader,
                                        device,
                                    )

                                    print(f"\n=== éªŒè¯ç»“æœ (Step {step}) ===")
                                    print(f"é‡å»ºæŸå¤±: {val_results['recon_loss']:.4f}")
                                    print(f"å¯¹æŠ—æŸå¤±: {val_results['adv_loss']:.4f}")
                                    print(f"çœŸå®å‡†ç¡®ç‡: {val_results['real_acc']:.4f}")
                                    print(
                                        f"å‡æ ·æœ¬å‡†ç¡®ç‡: {val_results['fake_acc']:.4f}"
                                    )
                                    print(
                                        f"åˆ¤åˆ«å™¨å¹³å‡å‡†ç¡®ç‡: {val_results['disc_acc']:.4f}"
                                    )
                                    print(
                                        f"ç¨³å®šæ€§EMA - çœŸå®: {stability_tracker.real_acc_ema:.3f}, å‡æ ·æœ¬: {stability_tracker.fake_acc_ema:.3f}"
                                    )

                                    # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                                    scheduler_cn.step(val_results["recon_loss"])
                                    scheduler_cd.step(val_results["disc_acc"])

                                    # æ£€æµ‹åˆ¤åˆ«å™¨å´©æºƒ
                                    if stability_tracker.is_collapsed():
                                        print("æ£€æµ‹åˆ°åˆ¤åˆ«å™¨ä¸ç¨³å®šï¼Œè°ƒæ•´è®­ç»ƒå‚æ•°...")
                                        # é™ä½å­¦ä¹ ç‡
                                        for param_group in opt_cd_p3.param_groups:
                                            param_group["lr"] *= 0.5
                                            print(
                                                f"åˆ¤åˆ«å™¨å­¦ä¹ ç‡é™ä½åˆ°: {param_group['lr']:.2e}"
                                            )

                                    # å¯è§†åŒ–éªŒè¯ç»“æœ
                                    if viz is not None and "phase3_val" in loss_windows:
                                        try:
                                            viz.line(
                                                Y=torch.tensor(
                                                    [
                                                        [
                                                            val_results["recon_loss"],
                                                            val_results["adv_loss"],
                                                            val_results["real_acc"],
                                                            val_results["fake_acc"],
                                                            val_results["disc_acc"],
                                                            val_results["bqd_loss"],
                                                        ]
                                                    ]
                                                ),
                                                X=torch.tensor([step]),
                                                win=loss_windows["phase3_val"],
                                                update="append",
                                            )
                                        except:
                                            pass

                                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                                    if val_results["recon_loss"] < best_val_loss_joint:
                                        best_val_loss_joint = val_results["recon_loss"]
                                        cn_best_path = os.path.join(
                                            result_dir,
                                            "phase_3",
                                            "model_cn_best" + interrupt,
                                        )
                                        cd_best_path = os.path.join(
                                            result_dir,
                                            "phase_3",
                                            "model_cd_best" + interrupt,
                                        )
                                        torch.save(model_cn.state_dict(), cn_best_path)
                                        torch.save(model_cd.state_dict(), cd_best_path)
                                        print(
                                            f"ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (é‡å»ºæŸå¤±: {val_results['recon_loss']:.4f})"
                                        )

                                    # å¯è§†åŒ–æµ‹è¯•ç»“æœ
                                    with torch.no_grad():
                                        try:
                                            test_batch = next(iter(val_subset_loader))
                                            test_data = prepare_batch_data(
                                                test_batch, device
                                            )
                                            (
                                                test_local_inputs,
                                                test_local_masks,
                                                test_local_targets,
                                                test_global_inputs,
                                                test_global_masks,
                                                test_global_targets,
                                                metadata,
                                            ) = test_data

                                            test_output = safe_tensor_operation(
                                                model_cn,
                                                test_local_inputs,
                                                test_local_masks,
                                                test_global_inputs,
                                                test_global_masks,
                                            )
                                            completed, completed_mask, _ = (
                                                merge_local_to_global(
                                                    test_output,
                                                    test_global_inputs,
                                                    test_global_masks,
                                                    metadata,
                                                )
                                            )
                                            nld_tli = normalization(
                                                test_local_inputs, test_global_inputs
                                            )
                                            nld_to = normalization(
                                                test_output, test_global_inputs
                                            )
                                            nld_tlt = normalization(
                                                test_local_targets, test_global_inputs
                                            )
                                            bqd_in, _, _ = model_bqd(
                                                nld_tli, test_local_masks
                                            )
                                            bqd_out, _, _ = model_bqd(
                                                nld_to, test_local_masks
                                            )
                                            nld_tlt = nld_tlt.unsqueeze(1)
                                            bqd_target, _, _ = model_bqd(
                                                nld_tlt, test_local_masks
                                            )

                                            visualize_results(
                                                test_local_targets,
                                                test_local_inputs,
                                                test_output,
                                                bqd_in,
                                                bqd_out,
                                                bqd_target,
                                                completed,
                                                step,
                                                "phase_3",
                                            )

                                            del (
                                                test_output,
                                                completed,
                                                completed_mask,
                                                test_data,
                                                bqd_in,
                                                bqd_out,
                                            )
                                        except Exception as e:
                                            print(f"å¯è§†åŒ–å¤±è´¥: {e}")

                                    folder = os.path.join(result_dir, "phase_3")

                                    # å¤„ç†ä¸¤ç±»æ¨¡å‹æ–‡ä»¶
                                    for model_type in ["cn", "cd"]:
                                        pattern = os.path.join(
                                            folder, f"model_{model_type}_step*"
                                        )
                                        old_files = glob.glob(pattern)

                                        # åˆ é™¤stepå°äºå½“å‰step-1000çš„æ–‡ä»¶
                                        for file in old_files:
                                            try:
                                                # ä»æ–‡ä»¶åä¸­æå–stepæ•°å­—
                                                basename = os.path.basename(file)
                                                match = re.search(
                                                    rf"model_{model_type}_step(\d+)(?:\.pth)?$",
                                                    basename,
                                                )
                                                if match:
                                                    file_step = int(match.group(1))
                                                    # åªåˆ é™¤stepå°äºå½“å‰step-1000çš„æ–‡ä»¶
                                                    if file_step < step - 1000:
                                                        os.remove(file)
                                                        print(
                                                            f"åˆ é™¤æ—§æ–‡ä»¶: {basename} (step={file_step})"
                                                        )
                                                else:
                                                    print(f"æ— æ³•è§£ææ–‡ä»¶å: {basename}")
                                            except Exception as e:
                                                print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")

                                    # ä¿å­˜å½“å‰æ¨¡å‹
                                    cn_path = os.path.join(
                                        result_dir, "phase_3", f"model_cn_step{step}"
                                    )
                                    cd_path = os.path.join(
                                        result_dir, "phase_3", f"model_cd_step{step}"
                                    )
                                    torch.save(model_cn.state_dict(), cn_path)
                                    torch.save(model_cd.state_dict(), cd_path)
                                    print(
                                        f"ä¿å­˜æ¨¡å‹: model_cn_step{step} å’Œ model_cd_step{step}"
                                    )

                                    """save_checkpoint(
                                        model_cn,
                                        model_cd,
                                        model_bqd,
                                        opt_cn_p3,
                                        opt_cd_p3,
                                        opt_bqd,
                                        step,
                                        3,
                                        result_dir,
                                        best_val_loss=best_val_loss_joint,
                                        cn_lr=opt_cn_p3.param_groups[0]["lr"],
                                        cd_lr=opt_cd_p3.param_groups[0]["lr"],
                                        bqd_lr=opt_bqd.param_groups[0]["lr"],
                                        schedulers=(
                                            {"cn": scheduler_cn, "cd": scheduler_cd}
                                            if "scheduler_cn" in locals()
                                            else None
                                        ),
                                        scalers=(
                                            {
                                                "cn": scaler_cn,
                                                "cd": scaler_cd,
                                                "bqd": scaler_bqd_p3,
                                            }
                                            if "scaler_cn" in locals()
                                            else None
                                        ),
                                        stability_tracker=(
                                            stability_tracker
                                            if "stability_tracker" in locals()
                                            else None
                                        ),
                                    )"""

                                except Exception as e:
                                    print(f"éªŒè¯æ­¥éª¤å¤±è´¥: {e}")
                                    cleanup_memory()

                            if step >= steps_3:
                                break

                        except Exception as e:
                            print(f"è®­ç»ƒæ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                            traceback.print_exc()  # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
                            cleanup_memory()
                            continue

                pbar.close()
                print(f"ğŸ‰ Phase 3 è®­ç»ƒå®Œæˆ! æœ€ä½³é‡å»ºæŸå¤±: {best_val_loss_joint:.4f}")

            except Exception as e:
                print(f"Phase 3è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                cleanup_memory()
                raise

        # Final visualization: Compare best models from each phase
        try:
            test_batch = next(iter(val_subset_loader))
            test_data = prepare_batch_data(test_batch, device)
            (
                test_local_inputs,
                test_local_masks,
                test_local_targets,
                test_global_inputs,
                test_global_masks,
                test_global_targets,
                metadata,
            ) = test_data
            # Load best models from each phase
            best_models = {}
            # Best model from phase 1
            best_model_p1 = CompletionNetwork(input_channels=1).to(device)
            p1_path = os.path.join(result_dir, "phase_1", "model_cn_best")
            if os.path.exists(p1_path):
                best_model_p1.load_state_dict(
                    torch.load(p1_path, weights_only=True, map_location=device)
                )
                best_model_p1.eval()
            # Best model from phase 3 (joint training)
            best_model_p3 = CompletionNetwork(input_channels=1).to(device)
            p3_path = os.path.join(result_dir, "phase_3", "model_cn_best")
            if os.path.exists(p3_path):
                best_model_p3.load_state_dict(
                    torch.load(p3_path, weights_only=True, map_location=device)
                )
                best_model_p3.eval()
            with torch.no_grad():
                # Generate completions with best models
                if os.path.exists(p1_path):
                    output_p1 = safe_tensor_operation(
                        best_model_p1,
                        test_local_inputs,
                        test_local_masks,
                        test_global_inputs,
                        test_global_masks,
                    )
                    # Merge outputs to global for visualization
                    completed_p1, _, _ = merge_local_to_global(
                        output_p1, test_global_inputs, test_global_masks, metadata
                    )
                else:
                    output_p1 = None
                    completed_p1 = None
                if os.path.exists(p3_path):
                    output_p3 = safe_tensor_operation(
                        best_model_p3,
                        test_local_inputs,
                        test_local_masks,
                        test_global_inputs,
                        test_global_masks,
                    )
                    completed_p3, _, _ = merge_local_to_global(
                        output_p3, test_global_inputs, test_global_masks, metadata
                    )
                else:
                    output_p3 = None
                    completed_p3 = None
                # æ¸…ç†æ¯”è¾ƒå˜é‡
                if output_p1 is not None:
                    del output_p1, completed_p1
                if output_p3 is not None:
                    del output_p3, completed_p3
                del test_data

            # æ¸…ç†æœ€ç»ˆæµ‹è¯•æ¨¡å‹
            if "best_model_p1" in locals():
                del best_model_p1
            if "best_model_p3" in locals():
                del best_model_p3
        except Exception as e:
            print(f"æœ€ç»ˆå¯è§†åŒ–å¤±è´¥: {e}")
            cleanup_memory()
        print("Training complete!")

        # æœ€ç»ˆæ¸…ç†
        cleanup_memory()

    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        cleanup_memory()
        raise
    finally:
        # ç¡®ä¿æœ€ç»ˆæ¸…ç†
        cleanup_memory()
        # å…³é—­æ•°æ®åŠ è½½å™¨
        if "train_loader" in locals():
            del train_loader
        if "val_loader" in locals():
            del val_loader
        if "val_subset_loader" in locals():
            del val_subset_loader
        print("æ‰€æœ‰èµ„æºå·²æ¸…ç†å®Œæ¯•")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="DEM completion network training")
    parser.add_argument(
        "--resume",
        type=str,
        default=r"",  # e:\KingCrimson Dataset\Simulate\data0\results31\latest_checkpoint.pth
        help="resume from checkpoint path",
    )
    parser.add_argument(
        "--dir", type=str, default="results46", help="directory to save results"
    )
    parser.add_argument(
        "--envi", type=str, default="DEM46", help="visdom environment name"
    )
    parser.add_argument("--cuda", type=str, default="cuda:3", help="CUDA device to use")
    parser.add_argument(
        "--test", type=bool, default=False, help="whether to run in test mode"
    )
    parser.add_argument("--batch", type=int, default=8, help="batch size for training")
    parser.add_argument(
        "--phase", type=int, default=3, help="training phase to start from (1, 2, or 3)"
    )
    args = parser.parse_args()
    with visdom_server():
        viz = visdom.Visdom()
        if viz.check_connection():
            print("æˆåŠŸè¿æ¥åˆ°VisdomæœåŠ¡å™¨")
            # è¿›è¡Œä½ çš„å¯è§†åŒ–æ“ä½œ
        else:
            print("è¿æ¥å¤±è´¥")
    try:
        train(
            dir=args.dir,
            envi=args.envi,
            cuda=args.cuda,
            test=args.test,
            batch=args.batch,
            resume_from=args.resume,
            Phase=args.phase,
        )
    except KeyboardInterrupt:
        print("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        cleanup_memory()
    except Exception as e:
        print(f"è®­ç»ƒå¤±è´¥: {e}")
        cleanup_memory()
        raise
